{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSC413_Project_fit_FMNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "gQ4JgbCP1pru"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup PyTorch and Ray Tune\n",
        "\n"
      ],
      "metadata": {
        "id": "jd1fAdsdXbeG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision\n",
        "!pip install ray"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1UUrCMJVIFe",
        "outputId": "1c230393-bea6-4906-853c-808bc0320ff5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.11.1+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: ray in /usr/local/lib/python3.7/dist-packages (1.12.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray) (1.0.3)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray) (4.3.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray) (3.6.0)\n",
            "Requirement already satisfied: protobuf>=3.15.3 in /usr/local/lib/python3.7/dist-packages (from ray) (3.17.3)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ray) (2.23.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray) (3.13)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from ray) (1.21.6)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from ray) (21.4.0)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.7/dist-packages (from ray) (1.2.0)\n",
            "Requirement already satisfied: virtualenv in /usr/local/lib/python3.7/dist-packages (from ray) (20.14.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.7/dist-packages (from ray) (1.3.0)\n",
            "Requirement already satisfied: grpcio<=1.43.0,>=1.28.1 in /usr/local/lib/python3.7/dist-packages (from ray) (1.43.0)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio<=1.43.0,>=1.28.1->ray) (1.15.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray) (0.18.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray) (4.1.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray) (4.11.3)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray) (5.7.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema->ray) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ray) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->ray) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->ray) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ray) (2021.10.8)\n",
            "Requirement already satisfied: distlib<1,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from virtualenv->ray) (0.3.4)\n",
            "Requirement already satisfied: platformdirs<3,>=2 in /usr/local/lib/python3.7/dist-packages (from virtualenv->ray) (2.5.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import random_split\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.models as models\n",
        "from ray import tune\n",
        "from ray.tune import CLIReporter\n",
        "from ray.tune.schedulers import ASHAScheduler"
      ],
      "metadata": {
        "id": "Nbb9tvnHT_PV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper code"
      ],
      "metadata": {
        "id": "oW_tE8n4XAIf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data loader"
      ],
      "metadata": {
        "id": "Hah6W0v5Xuak"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "IqVsM1DqB1Go"
      },
      "outputs": [],
      "source": [
        "def data_loader(batch_size=4):\n",
        "  normalize = transforms.Normalize(mean=0.2859,\n",
        "                                      std=0.3530)\n",
        "\n",
        "  fmnist_training_data = torchvision.datasets.FashionMNIST(\"/content\", \n",
        "                                              train=True,\n",
        "                                              transform = transforms.Compose([\n",
        "                                                                              transforms.ToTensor(),\n",
        "                                                                              normalize,]), \n",
        "                                              download=True)\n",
        "  \n",
        "  fmnist_val_data = torchvision.datasets.FashionMNIST(\"/content\", \n",
        "                                              train=True,\n",
        "                                              transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                                                              normalize,]), \n",
        "                                              download=True)\n",
        "\n",
        "  fmnist_testing_data = torchvision.datasets.FashionMNIST(\"/content\", \n",
        "                                              train=False,\n",
        "                                              transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                                                              normalize,]), \n",
        "                                              download=True)\n",
        "  # data = [torch.Size([10, 3, 32, 32]), torch.Size([10])]\n",
        "  num_train = len(fmnist_training_data)\n",
        "  indices = list(range(num_train))\n",
        "  split = 5000 #45k/5k train/val split\n",
        "  train_idx, valid_idx = indices[split:], indices[:split]\n",
        "  train_sampler = SubsetRandomSampler(train_idx)\n",
        "  valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "  fmnist_training_data_loader = torch.utils.data.DataLoader(fmnist_training_data, batch_size, sampler=train_sampler, shuffle=False)\n",
        "  fmnist_val_data_loader = torch.utils.data.DataLoader(fmnist_val_data, batch_size, sampler=valid_sampler, shuffle=False)\n",
        "  fmnist_testing_data_loader = torch.utils.data.DataLoader(fmnist_testing_data, batch_size, shuffle=True)\n",
        "  return fmnist_training_data_loader, fmnist_val_data_loader, fmnist_testing_data_loader\n",
        "\n",
        "  # for data in cifar10_training_data_loader:\n",
        "  #   # print(\"data: \", data)\n",
        "  #   images, labels = data[0], data[1]\n",
        "  #   print(\"images.shape: {}, labels.shape: {}\".format(images.shape, labels.shape))\n",
        "  #   break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_, _, _ = data_loader()"
      ],
      "metadata": {
        "id": "p8zRRwoSqpd5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "bBcDeGA2QmmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A 2-conv-layer block of ResNet \n",
        "class block(nn.Module):\n",
        "  def __init__(self, num_filters, enable_subsample):\n",
        "    super().__init__()\n",
        "    if enable_subsample:\n",
        "      self.conv1 = nn.Conv2d(num_filters // 2, num_filters, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "    else:\n",
        "      self.conv1 = nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1, bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(num_filters)\n",
        "    self.relu1 = nn.ReLU()\n",
        "\n",
        "    self.conv2 = nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1, bias=False)\n",
        "    self.bn2 = nn.BatchNorm2d(num_filters)\n",
        "    self.relu2 = nn.ReLU()\n",
        "\n",
        "    # Weight initialization as in https://github.com/a-martyn/resnet/blob/master/resnet.py\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Conv2d):\n",
        "          nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "      elif isinstance(m, (nn.BatchNorm2d)):\n",
        "          nn.init.constant_(m.weight,1)\n",
        "          nn.init.constant_(m.bias, 0) \n",
        "\n",
        "  def forward(self, x, enable_skip_connections=False):\n",
        "    out = self.conv1(x)\n",
        "    out = self.bn1(out)\n",
        "    out = self.relu1(out)\n",
        "    out = self.conv2(out)\n",
        "    out = self.bn2(out)\n",
        "    if enable_skip_connections:\n",
        "      # print(out.shape, x.shape)\n",
        "      if out.shape != x.shape:\n",
        "        W_s = nn.Conv2d(x.shape[1], out.shape[1], kernel_size=1, stride=2).to(device='cuda')\n",
        "        x = W_s(x)\n",
        "        # print(\"after\", out.shape, x.shape)\n",
        "      else:\n",
        "        out = x + out\n",
        "    out = self.relu2(out)\n",
        "    return out\n",
        "\n",
        "# ResNet for CIFAR-10 as in paper\n",
        "class ResNet(nn.Module):\n",
        "  def __init__(self, n, enable_skip_connections=False):\n",
        "    super().__init__()\n",
        "    self.skip_connection = enable_skip_connections\n",
        "\n",
        "    self.num_layer1_filters = 16\n",
        "    self.num_layer2_filters = 32\n",
        "    self.num_layer3_filters = 64\n",
        "\n",
        "    self.layer0 = nn.Sequential(\n",
        "        nn.Conv2d(1, self.num_layer1_filters, kernel_size=3, padding=1, bias=False),\n",
        "        nn.BatchNorm2d(self.num_layer1_filters),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    \n",
        "    self.layer1 = nn.ModuleList([block(self.num_layer1_filters, enable_subsample=False) for i in range(n)])\n",
        "    self.layer2_subsample = block(self.num_layer2_filters, enable_subsample=True)\n",
        "    self.layer2 = nn.ModuleList([block(self.num_layer2_filters, enable_subsample=False) for i in range(n-1)])\n",
        "    self.layer3_subsample = block(self.num_layer3_filters, enable_subsample=True)\n",
        "    self.layer3 = nn.ModuleList([block(self.num_layer3_filters, enable_subsample=False) for i in range(n-1)])\n",
        "\n",
        "    self.avgpooling = nn.AdaptiveAvgPool2d(1)\n",
        "    self.fc_layer = nn.Linear(self.num_layer3_filters, 10)\n",
        "    self.softmax = nn.Softmax(dim=1)    \n",
        "  \n",
        "  def forward(self, x):\n",
        "    out = self.layer0(x)\n",
        "    for block in self.layer1:\n",
        "      out = block(out, self.skip_connection)\n",
        "    out = self.layer2_subsample(out, self.skip_connection)\n",
        "    for block in self.layer2:\n",
        "      out = block(out, self.skip_connection)\n",
        "    out = self.layer3_subsample(out, self.skip_connection)\n",
        "    for block in self.layer3:\n",
        "      out = block(out, self.skip_connection)\n",
        "    out = self.avgpooling(out)\n",
        "    out = out.reshape((-1, self.num_layer3_filters))\n",
        "    out = self.fc_layer(out)\n",
        "    out = self.softmax(out)\n",
        "\n",
        "    return out\n",
        "\n",
        "# curr_model = block(32, enable_subsample=False)\n",
        "# print(curr_model)\n",
        "\n",
        "# resnet20_plain = ResNet(3)\n",
        "# resnet20 = ResNet(3, enable_skip_connections=True)\n",
        "\n",
        "# resnet18 = models.resnet18()\n",
        "# print(resnet18)\n",
        "# print(sum(p.numel() for p in resnet20_plain.parameters()))\n",
        "# print(sum(p.numel() for p in resnet20.parameters()))"
      ],
      "metadata": {
        "id": "uUdRy-anQ3Vo"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Training"
      ],
      "metadata": {
        "id": "N3sCLtBu1f0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def resnet_training():\n",
        "\n",
        "  batch_size = 128\n",
        "  net = ResNet(3, True)\n",
        "  #net = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
        "\n",
        "  trainloader, valloader, testloader = data_loader(batch_size)\n",
        "  print(len(trainloader), len(valloader), len(testloader))\n",
        "  classes = ('plane', 'car', 'bird', 'cat',\n",
        "            'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "  device = \"cpu\"\n",
        "  if torch.cuda.is_available():\n",
        "      device = \"cuda:0\"\n",
        "      if torch.cuda.device_count() > 1:\n",
        "          net = nn.DataParallel(net)\n",
        "  net = net.to(device)\n",
        "  # https://discuss.pytorch.org/t/how-to-increase-the-learning-rate-without-using-cyclical-learning-rates/140208/4\n",
        "  def _lr_lambda(current_step):\n",
        "        \"\"\"\n",
        "        _lr_lambda returns a multiplicative factor given an interger parameter epochs.\n",
        "        \"\"\"\n",
        "        if current_step < 400:\n",
        "            _lr =.1\n",
        "        elif current_step < 32000:\n",
        "            _lr = 1\n",
        "        elif current_step < 48000:\n",
        "            _lr = .1\n",
        "        else:\n",
        "            _lr = .01\n",
        "\n",
        "        return _lr\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=0.0001)\n",
        "  scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, _lr_lambda, last_epoch=-1, verbose=False)\n",
        "  for epoch in range(182):  # loop over the dataset multiple times\n",
        "      running_loss = 0.0\n",
        "      for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 10 == 9:    # print every 10 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 10:.3f}')\n",
        "            running_loss = 0.0\n",
        "      \n",
        "      val_loss = 0.0\n",
        "      val_steps = 0\n",
        "      total = 0\n",
        "      correct = 0\n",
        "      for i, data in enumerate(valloader, 0):\n",
        "          with torch.no_grad():\n",
        "              inputs, labels = data\n",
        "              inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "              outputs = net(inputs)\n",
        "              _, predicted = torch.max(outputs.data, 1)\n",
        "              total += labels.size(0)\n",
        "              correct += (predicted == labels).sum().item()\n",
        "\n",
        "              loss = criterion(outputs, labels)\n",
        "              val_loss += loss.cpu().numpy()\n",
        "              val_steps += 1\n",
        "      print(\"epoch {} val_loss {} val_steps {} val_acc {}\".format(epoch, val_loss, val_steps, correct / total))\n",
        "  print('Finished Training')\n",
        "  return net\n",
        "trained_net = resnet_training()\n"
      ],
      "metadata": {
        "id": "d708Mz221-3l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44d733c9-9b89-4481-8c7d-884d1b5fb4d2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "[69,   180] loss: 1.507\n",
            "[69,   190] loss: 1.504\n",
            "[69,   200] loss: 1.510\n",
            "[69,   210] loss: 1.515\n",
            "[69,   220] loss: 1.506\n",
            "[69,   230] loss: 1.505\n",
            "[69,   240] loss: 1.507\n",
            "[69,   250] loss: 1.523\n",
            "[69,   260] loss: 1.511\n",
            "[69,   270] loss: 1.516\n",
            "[69,   280] loss: 1.510\n",
            "[69,   290] loss: 1.511\n",
            "[69,   300] loss: 1.494\n",
            "[69,   310] loss: 1.515\n",
            "[69,   320] loss: 1.505\n",
            "[69,   330] loss: 1.503\n",
            "[69,   340] loss: 1.493\n",
            "[69,   350] loss: 1.505\n",
            "[69,   360] loss: 1.510\n",
            "[69,   370] loss: 1.501\n",
            "[69,   380] loss: 1.515\n",
            "[69,   390] loss: 1.505\n",
            "[69,   400] loss: 1.513\n",
            "[69,   410] loss: 1.506\n",
            "[69,   420] loss: 1.502\n",
            "[69,   430] loss: 1.509\n",
            "epoch 68 val_loss 61.42504286766052 val_steps 40 val_acc 0.9288\n",
            "[70,    10] loss: 1.506\n",
            "[70,    20] loss: 1.507\n",
            "[70,    30] loss: 1.501\n",
            "[70,    40] loss: 1.507\n",
            "[70,    50] loss: 1.506\n",
            "[70,    60] loss: 1.500\n",
            "[70,    70] loss: 1.500\n",
            "[70,    80] loss: 1.503\n",
            "[70,    90] loss: 1.498\n",
            "[70,   100] loss: 1.508\n",
            "[70,   110] loss: 1.503\n",
            "[70,   120] loss: 1.501\n",
            "[70,   130] loss: 1.506\n",
            "[70,   140] loss: 1.500\n",
            "[70,   150] loss: 1.509\n",
            "[70,   160] loss: 1.509\n",
            "[70,   170] loss: 1.508\n",
            "[70,   180] loss: 1.502\n",
            "[70,   190] loss: 1.500\n",
            "[70,   200] loss: 1.503\n",
            "[70,   210] loss: 1.506\n",
            "[70,   220] loss: 1.510\n",
            "[70,   230] loss: 1.506\n",
            "[70,   240] loss: 1.511\n",
            "[70,   250] loss: 1.506\n",
            "[70,   260] loss: 1.511\n",
            "[70,   270] loss: 1.509\n",
            "[70,   280] loss: 1.510\n",
            "[70,   290] loss: 1.499\n",
            "[70,   300] loss: 1.503\n",
            "[70,   310] loss: 1.498\n",
            "[70,   320] loss: 1.497\n",
            "[70,   330] loss: 1.507\n",
            "[70,   340] loss: 1.521\n",
            "[70,   350] loss: 1.508\n",
            "[70,   360] loss: 1.521\n",
            "[70,   370] loss: 1.519\n",
            "[70,   380] loss: 1.526\n",
            "[70,   390] loss: 1.504\n",
            "[70,   400] loss: 1.517\n",
            "[70,   410] loss: 1.510\n",
            "[70,   420] loss: 1.515\n",
            "[70,   430] loss: 1.512\n",
            "epoch 69 val_loss 61.424023270606995 val_steps 40 val_acc 0.9266\n",
            "[71,    10] loss: 1.507\n",
            "[71,    20] loss: 1.505\n",
            "[71,    30] loss: 1.512\n",
            "[71,    40] loss: 1.501\n",
            "[71,    50] loss: 1.506\n",
            "[71,    60] loss: 1.501\n",
            "[71,    70] loss: 1.511\n",
            "[71,    80] loss: 1.500\n",
            "[71,    90] loss: 1.501\n",
            "[71,   100] loss: 1.496\n",
            "[71,   110] loss: 1.507\n",
            "[71,   120] loss: 1.501\n",
            "[71,   130] loss: 1.507\n",
            "[71,   140] loss: 1.510\n",
            "[71,   150] loss: 1.503\n",
            "[71,   160] loss: 1.512\n",
            "[71,   170] loss: 1.504\n",
            "[71,   180] loss: 1.500\n",
            "[71,   190] loss: 1.498\n",
            "[71,   200] loss: 1.513\n",
            "[71,   210] loss: 1.509\n",
            "[71,   220] loss: 1.502\n",
            "[71,   230] loss: 1.500\n",
            "[71,   240] loss: 1.504\n",
            "[71,   250] loss: 1.518\n",
            "[71,   260] loss: 1.489\n",
            "[71,   270] loss: 1.507\n",
            "[71,   280] loss: 1.497\n",
            "[71,   290] loss: 1.500\n",
            "[71,   300] loss: 1.509\n",
            "[71,   310] loss: 1.496\n",
            "[71,   320] loss: 1.503\n",
            "[71,   330] loss: 1.498\n",
            "[71,   340] loss: 1.506\n",
            "[71,   350] loss: 1.518\n",
            "[71,   360] loss: 1.507\n",
            "[71,   370] loss: 1.513\n",
            "[71,   380] loss: 1.514\n",
            "[71,   390] loss: 1.511\n",
            "[71,   400] loss: 1.511\n",
            "[71,   410] loss: 1.505\n",
            "[71,   420] loss: 1.501\n",
            "[71,   430] loss: 1.510\n",
            "epoch 70 val_loss 61.20541775226593 val_steps 40 val_acc 0.9308\n",
            "[72,    10] loss: 1.505\n",
            "[72,    20] loss: 1.508\n",
            "[72,    30] loss: 1.505\n",
            "[72,    40] loss: 1.498\n",
            "[72,    50] loss: 1.497\n",
            "[72,    60] loss: 1.509\n",
            "[72,    70] loss: 1.496\n",
            "[72,    80] loss: 1.501\n",
            "[72,    90] loss: 1.496\n",
            "[72,   100] loss: 1.497\n",
            "[72,   110] loss: 1.504\n",
            "[72,   120] loss: 1.501\n",
            "[72,   130] loss: 1.491\n",
            "[72,   140] loss: 1.494\n",
            "[72,   150] loss: 1.500\n",
            "[72,   160] loss: 1.503\n",
            "[72,   170] loss: 1.499\n",
            "[72,   180] loss: 1.506\n",
            "[72,   190] loss: 1.496\n",
            "[72,   200] loss: 1.501\n",
            "[72,   210] loss: 1.503\n",
            "[72,   220] loss: 1.503\n",
            "[72,   230] loss: 1.509\n",
            "[72,   240] loss: 1.503\n",
            "[72,   250] loss: 1.501\n",
            "[72,   260] loss: 1.510\n",
            "[72,   270] loss: 1.497\n",
            "[72,   280] loss: 1.509\n",
            "[72,   290] loss: 1.503\n",
            "[72,   300] loss: 1.518\n",
            "[72,   310] loss: 1.516\n",
            "[72,   320] loss: 1.511\n",
            "[72,   330] loss: 1.508\n",
            "[72,   340] loss: 1.521\n",
            "[72,   350] loss: 1.506\n",
            "[72,   360] loss: 1.502\n",
            "[72,   370] loss: 1.510\n",
            "[72,   380] loss: 1.518\n",
            "[72,   390] loss: 1.511\n",
            "[72,   400] loss: 1.513\n",
            "[72,   410] loss: 1.504\n",
            "[72,   420] loss: 1.510\n",
            "[72,   430] loss: 1.508\n",
            "epoch 71 val_loss 61.647231221199036 val_steps 40 val_acc 0.9276\n",
            "[73,    10] loss: 1.504\n",
            "[73,    20] loss: 1.496\n",
            "[73,    30] loss: 1.507\n",
            "[73,    40] loss: 1.500\n",
            "[73,    50] loss: 1.507\n",
            "[73,    60] loss: 1.502\n",
            "[73,    70] loss: 1.503\n",
            "[73,    80] loss: 1.502\n",
            "[73,    90] loss: 1.501\n",
            "[73,   100] loss: 1.505\n",
            "[73,   110] loss: 1.508\n",
            "[73,   120] loss: 1.497\n",
            "[73,   130] loss: 1.504\n",
            "[73,   140] loss: 1.497\n",
            "[73,   150] loss: 1.518\n",
            "[73,   160] loss: 1.501\n",
            "[73,   170] loss: 1.509\n",
            "[73,   180] loss: 1.494\n",
            "[73,   190] loss: 1.505\n",
            "[73,   200] loss: 1.504\n",
            "[73,   210] loss: 1.497\n",
            "[73,   220] loss: 1.504\n",
            "[73,   230] loss: 1.508\n",
            "[73,   240] loss: 1.509\n",
            "[73,   250] loss: 1.502\n",
            "[73,   260] loss: 1.487\n",
            "[73,   270] loss: 1.503\n",
            "[73,   280] loss: 1.511\n",
            "[73,   290] loss: 1.504\n",
            "[73,   300] loss: 1.507\n",
            "[73,   310] loss: 1.502\n",
            "[73,   320] loss: 1.513\n",
            "[73,   330] loss: 1.505\n",
            "[73,   340] loss: 1.500\n",
            "[73,   350] loss: 1.503\n",
            "[73,   360] loss: 1.503\n",
            "[73,   370] loss: 1.499\n",
            "[73,   380] loss: 1.502\n",
            "[73,   390] loss: 1.500\n",
            "[73,   400] loss: 1.509\n",
            "[73,   410] loss: 1.502\n",
            "[73,   420] loss: 1.513\n",
            "[73,   430] loss: 1.502\n",
            "epoch 72 val_loss 61.68727171421051 val_steps 40 val_acc 0.928\n",
            "[74,    10] loss: 1.508\n",
            "[74,    20] loss: 1.506\n",
            "[74,    30] loss: 1.508\n",
            "[74,    40] loss: 1.500\n",
            "[74,    50] loss: 1.501\n",
            "[74,    60] loss: 1.499\n",
            "[74,    70] loss: 1.505\n",
            "[74,    80] loss: 1.490\n",
            "[74,    90] loss: 1.505\n",
            "[74,   100] loss: 1.503\n",
            "[74,   110] loss: 1.515\n",
            "[74,   120] loss: 1.503\n",
            "[74,   130] loss: 1.503\n",
            "[74,   140] loss: 1.509\n",
            "[74,   150] loss: 1.507\n",
            "[74,   160] loss: 1.501\n",
            "[74,   170] loss: 1.500\n",
            "[74,   180] loss: 1.507\n",
            "[74,   190] loss: 1.502\n",
            "[74,   200] loss: 1.500\n",
            "[74,   210] loss: 1.502\n",
            "[74,   220] loss: 1.500\n",
            "[74,   230] loss: 1.509\n",
            "[74,   240] loss: 1.508\n",
            "[74,   250] loss: 1.514\n",
            "[74,   260] loss: 1.506\n",
            "[74,   270] loss: 1.515\n",
            "[74,   280] loss: 1.501\n",
            "[74,   290] loss: 1.509\n",
            "[74,   300] loss: 1.511\n",
            "[74,   310] loss: 1.509\n",
            "[74,   320] loss: 1.517\n",
            "[74,   330] loss: 1.503\n",
            "[74,   340] loss: 1.508\n",
            "[74,   350] loss: 1.497\n",
            "[74,   360] loss: 1.512\n",
            "[74,   370] loss: 1.505\n",
            "[74,   380] loss: 1.505\n",
            "[74,   390] loss: 1.507\n",
            "[74,   400] loss: 1.503\n",
            "[74,   410] loss: 1.511\n",
            "[74,   420] loss: 1.499\n",
            "[74,   430] loss: 1.494\n",
            "epoch 73 val_loss 61.409632086753845 val_steps 40 val_acc 0.926\n",
            "[75,    10] loss: 1.497\n",
            "[75,    20] loss: 1.499\n",
            "[75,    30] loss: 1.503\n",
            "[75,    40] loss: 1.502\n",
            "[75,    50] loss: 1.505\n",
            "[75,    60] loss: 1.500\n",
            "[75,    70] loss: 1.504\n",
            "[75,    80] loss: 1.510\n",
            "[75,    90] loss: 1.502\n",
            "[75,   100] loss: 1.502\n",
            "[75,   110] loss: 1.505\n",
            "[75,   120] loss: 1.500\n",
            "[75,   130] loss: 1.504\n",
            "[75,   140] loss: 1.503\n",
            "[75,   150] loss: 1.502\n",
            "[75,   160] loss: 1.504\n",
            "[75,   170] loss: 1.507\n",
            "[75,   180] loss: 1.501\n",
            "[75,   190] loss: 1.499\n",
            "[75,   200] loss: 1.498\n",
            "[75,   210] loss: 1.500\n",
            "[75,   220] loss: 1.494\n",
            "[75,   230] loss: 1.495\n",
            "[75,   240] loss: 1.494\n",
            "[75,   250] loss: 1.497\n",
            "[75,   260] loss: 1.495\n",
            "[75,   270] loss: 1.482\n",
            "[75,   280] loss: 1.500\n",
            "[75,   290] loss: 1.493\n",
            "[75,   300] loss: 1.491\n",
            "[75,   310] loss: 1.496\n",
            "[75,   320] loss: 1.490\n",
            "[75,   330] loss: 1.498\n",
            "[75,   340] loss: 1.493\n",
            "[75,   350] loss: 1.486\n",
            "[75,   360] loss: 1.495\n",
            "[75,   370] loss: 1.495\n",
            "[75,   380] loss: 1.489\n",
            "[75,   390] loss: 1.486\n",
            "[75,   400] loss: 1.494\n",
            "[75,   410] loss: 1.500\n",
            "[75,   420] loss: 1.501\n",
            "[75,   430] loss: 1.490\n",
            "epoch 74 val_loss 61.24574959278107 val_steps 40 val_acc 0.9332\n",
            "[76,    10] loss: 1.488\n",
            "[76,    20] loss: 1.488\n",
            "[76,    30] loss: 1.487\n",
            "[76,    40] loss: 1.485\n",
            "[76,    50] loss: 1.482\n",
            "[76,    60] loss: 1.489\n",
            "[76,    70] loss: 1.484\n",
            "[76,    80] loss: 1.481\n",
            "[76,    90] loss: 1.488\n",
            "[76,   100] loss: 1.484\n",
            "[76,   110] loss: 1.487\n",
            "[76,   120] loss: 1.483\n",
            "[76,   130] loss: 1.484\n",
            "[76,   140] loss: 1.486\n",
            "[76,   150] loss: 1.484\n",
            "[76,   160] loss: 1.484\n",
            "[76,   170] loss: 1.491\n",
            "[76,   180] loss: 1.478\n",
            "[76,   190] loss: 1.493\n",
            "[76,   200] loss: 1.492\n",
            "[76,   210] loss: 1.491\n",
            "[76,   220] loss: 1.477\n",
            "[76,   230] loss: 1.485\n",
            "[76,   240] loss: 1.491\n",
            "[76,   250] loss: 1.483\n",
            "[76,   260] loss: 1.484\n",
            "[76,   270] loss: 1.482\n",
            "[76,   280] loss: 1.491\n",
            "[76,   290] loss: 1.485\n",
            "[76,   300] loss: 1.485\n",
            "[76,   310] loss: 1.484\n",
            "[76,   320] loss: 1.492\n",
            "[76,   330] loss: 1.487\n",
            "[76,   340] loss: 1.489\n",
            "[76,   350] loss: 1.487\n",
            "[76,   360] loss: 1.480\n",
            "[76,   370] loss: 1.480\n",
            "[76,   380] loss: 1.489\n",
            "[76,   390] loss: 1.496\n",
            "[76,   400] loss: 1.481\n",
            "[76,   410] loss: 1.477\n",
            "[76,   420] loss: 1.490\n",
            "[76,   430] loss: 1.478\n",
            "epoch 75 val_loss 61.039382219314575 val_steps 40 val_acc 0.9344\n",
            "[77,    10] loss: 1.486\n",
            "[77,    20] loss: 1.481\n",
            "[77,    30] loss: 1.490\n",
            "[77,    40] loss: 1.481\n",
            "[77,    50] loss: 1.476\n",
            "[77,    60] loss: 1.486\n",
            "[77,    70] loss: 1.483\n",
            "[77,    80] loss: 1.487\n",
            "[77,    90] loss: 1.482\n",
            "[77,   100] loss: 1.478\n",
            "[77,   110] loss: 1.480\n",
            "[77,   120] loss: 1.475\n",
            "[77,   130] loss: 1.480\n",
            "[77,   140] loss: 1.482\n",
            "[77,   150] loss: 1.482\n",
            "[77,   160] loss: 1.488\n",
            "[77,   170] loss: 1.481\n",
            "[77,   180] loss: 1.481\n",
            "[77,   190] loss: 1.488\n",
            "[77,   200] loss: 1.480\n",
            "[77,   210] loss: 1.482\n",
            "[77,   220] loss: 1.484\n",
            "[77,   230] loss: 1.486\n",
            "[77,   240] loss: 1.476\n",
            "[77,   250] loss: 1.482\n",
            "[77,   260] loss: 1.483\n",
            "[77,   270] loss: 1.487\n",
            "[77,   280] loss: 1.486\n",
            "[77,   290] loss: 1.491\n",
            "[77,   300] loss: 1.485\n",
            "[77,   310] loss: 1.480\n",
            "[77,   320] loss: 1.482\n",
            "[77,   330] loss: 1.484\n",
            "[77,   340] loss: 1.486\n",
            "[77,   350] loss: 1.483\n",
            "[77,   360] loss: 1.484\n",
            "[77,   370] loss: 1.483\n",
            "[77,   380] loss: 1.483\n",
            "[77,   390] loss: 1.482\n",
            "[77,   400] loss: 1.486\n",
            "[77,   410] loss: 1.482\n",
            "[77,   420] loss: 1.473\n",
            "[77,   430] loss: 1.482\n",
            "epoch 76 val_loss 60.946600556373596 val_steps 40 val_acc 0.9364\n",
            "[78,    10] loss: 1.483\n",
            "[78,    20] loss: 1.481\n",
            "[78,    30] loss: 1.479\n",
            "[78,    40] loss: 1.479\n",
            "[78,    50] loss: 1.480\n",
            "[78,    60] loss: 1.480\n",
            "[78,    70] loss: 1.480\n",
            "[78,    80] loss: 1.486\n",
            "[78,    90] loss: 1.480\n",
            "[78,   100] loss: 1.479\n",
            "[78,   110] loss: 1.489\n",
            "[78,   120] loss: 1.478\n",
            "[78,   130] loss: 1.487\n",
            "[78,   140] loss: 1.472\n",
            "[78,   150] loss: 1.479\n",
            "[78,   160] loss: 1.484\n",
            "[78,   170] loss: 1.485\n",
            "[78,   180] loss: 1.476\n",
            "[78,   190] loss: 1.480\n",
            "[78,   200] loss: 1.479\n",
            "[78,   210] loss: 1.482\n",
            "[78,   220] loss: 1.486\n",
            "[78,   230] loss: 1.485\n",
            "[78,   240] loss: 1.479\n",
            "[78,   250] loss: 1.484\n",
            "[78,   260] loss: 1.474\n",
            "[78,   270] loss: 1.480\n",
            "[78,   280] loss: 1.484\n",
            "[78,   290] loss: 1.481\n",
            "[78,   300] loss: 1.477\n",
            "[78,   310] loss: 1.481\n",
            "[78,   320] loss: 1.473\n",
            "[78,   330] loss: 1.476\n",
            "[78,   340] loss: 1.482\n",
            "[78,   350] loss: 1.482\n",
            "[78,   360] loss: 1.480\n",
            "[78,   370] loss: 1.477\n",
            "[78,   380] loss: 1.481\n",
            "[78,   390] loss: 1.486\n",
            "[78,   400] loss: 1.481\n",
            "[78,   410] loss: 1.478\n",
            "[78,   420] loss: 1.481\n",
            "[78,   430] loss: 1.486\n",
            "epoch 77 val_loss 61.184420466423035 val_steps 40 val_acc 0.9358\n",
            "[79,    10] loss: 1.485\n",
            "[79,    20] loss: 1.479\n",
            "[79,    30] loss: 1.474\n",
            "[79,    40] loss: 1.473\n",
            "[79,    50] loss: 1.481\n",
            "[79,    60] loss: 1.474\n",
            "[79,    70] loss: 1.475\n",
            "[79,    80] loss: 1.484\n",
            "[79,    90] loss: 1.480\n",
            "[79,   100] loss: 1.482\n",
            "[79,   110] loss: 1.484\n",
            "[79,   120] loss: 1.478\n",
            "[79,   130] loss: 1.478\n",
            "[79,   140] loss: 1.483\n",
            "[79,   150] loss: 1.482\n",
            "[79,   160] loss: 1.478\n",
            "[79,   170] loss: 1.481\n",
            "[79,   180] loss: 1.479\n",
            "[79,   190] loss: 1.478\n",
            "[79,   200] loss: 1.481\n",
            "[79,   210] loss: 1.478\n",
            "[79,   220] loss: 1.473\n",
            "[79,   230] loss: 1.473\n",
            "[79,   240] loss: 1.480\n",
            "[79,   250] loss: 1.483\n",
            "[79,   260] loss: 1.477\n",
            "[79,   270] loss: 1.478\n",
            "[79,   280] loss: 1.478\n",
            "[79,   290] loss: 1.479\n",
            "[79,   300] loss: 1.479\n",
            "[79,   310] loss: 1.483\n",
            "[79,   320] loss: 1.480\n",
            "[79,   330] loss: 1.480\n",
            "[79,   340] loss: 1.477\n",
            "[79,   350] loss: 1.480\n",
            "[79,   360] loss: 1.476\n",
            "[79,   370] loss: 1.479\n",
            "[79,   380] loss: 1.482\n",
            "[79,   390] loss: 1.488\n",
            "[79,   400] loss: 1.483\n",
            "[79,   410] loss: 1.474\n",
            "[79,   420] loss: 1.481\n",
            "[79,   430] loss: 1.481\n",
            "epoch 78 val_loss 60.93706488609314 val_steps 40 val_acc 0.938\n",
            "[80,    10] loss: 1.479\n",
            "[80,    20] loss: 1.480\n",
            "[80,    30] loss: 1.475\n",
            "[80,    40] loss: 1.478\n",
            "[80,    50] loss: 1.483\n",
            "[80,    60] loss: 1.475\n",
            "[80,    70] loss: 1.473\n",
            "[80,    80] loss: 1.474\n",
            "[80,    90] loss: 1.479\n",
            "[80,   100] loss: 1.473\n",
            "[80,   110] loss: 1.480\n",
            "[80,   120] loss: 1.478\n",
            "[80,   130] loss: 1.477\n",
            "[80,   140] loss: 1.479\n",
            "[80,   150] loss: 1.477\n",
            "[80,   160] loss: 1.480\n",
            "[80,   170] loss: 1.483\n",
            "[80,   180] loss: 1.482\n",
            "[80,   190] loss: 1.485\n",
            "[80,   200] loss: 1.480\n",
            "[80,   210] loss: 1.477\n",
            "[80,   220] loss: 1.482\n",
            "[80,   230] loss: 1.478\n",
            "[80,   240] loss: 1.484\n",
            "[80,   250] loss: 1.477\n",
            "[80,   260] loss: 1.482\n",
            "[80,   270] loss: 1.478\n",
            "[80,   280] loss: 1.482\n",
            "[80,   290] loss: 1.477\n",
            "[80,   300] loss: 1.476\n",
            "[80,   310] loss: 1.482\n",
            "[80,   320] loss: 1.482\n",
            "[80,   330] loss: 1.482\n",
            "[80,   340] loss: 1.476\n",
            "[80,   350] loss: 1.479\n",
            "[80,   360] loss: 1.479\n",
            "[80,   370] loss: 1.475\n",
            "[80,   380] loss: 1.473\n",
            "[80,   390] loss: 1.475\n",
            "[80,   400] loss: 1.475\n",
            "[80,   410] loss: 1.482\n",
            "[80,   420] loss: 1.477\n",
            "[80,   430] loss: 1.476\n",
            "epoch 79 val_loss 60.981324911117554 val_steps 40 val_acc 0.937\n",
            "[81,    10] loss: 1.480\n",
            "[81,    20] loss: 1.487\n",
            "[81,    30] loss: 1.472\n",
            "[81,    40] loss: 1.481\n",
            "[81,    50] loss: 1.475\n",
            "[81,    60] loss: 1.477\n",
            "[81,    70] loss: 1.473\n",
            "[81,    80] loss: 1.483\n",
            "[81,    90] loss: 1.479\n",
            "[81,   100] loss: 1.478\n",
            "[81,   110] loss: 1.483\n",
            "[81,   120] loss: 1.474\n",
            "[81,   130] loss: 1.479\n",
            "[81,   140] loss: 1.476\n",
            "[81,   150] loss: 1.476\n",
            "[81,   160] loss: 1.482\n",
            "[81,   170] loss: 1.472\n",
            "[81,   180] loss: 1.477\n",
            "[81,   190] loss: 1.476\n",
            "[81,   200] loss: 1.480\n",
            "[81,   210] loss: 1.471\n",
            "[81,   220] loss: 1.475\n",
            "[81,   230] loss: 1.485\n",
            "[81,   240] loss: 1.481\n",
            "[81,   250] loss: 1.479\n",
            "[81,   260] loss: 1.475\n",
            "[81,   270] loss: 1.480\n",
            "[81,   280] loss: 1.478\n",
            "[81,   290] loss: 1.476\n",
            "[81,   300] loss: 1.474\n",
            "[81,   310] loss: 1.477\n",
            "[81,   320] loss: 1.480\n",
            "[81,   330] loss: 1.479\n",
            "[81,   340] loss: 1.480\n",
            "[81,   350] loss: 1.481\n",
            "[81,   360] loss: 1.481\n",
            "[81,   370] loss: 1.477\n",
            "[81,   380] loss: 1.478\n",
            "[81,   390] loss: 1.476\n",
            "[81,   400] loss: 1.475\n",
            "[81,   410] loss: 1.477\n",
            "[81,   420] loss: 1.474\n",
            "[81,   430] loss: 1.477\n",
            "epoch 80 val_loss 61.19921374320984 val_steps 40 val_acc 0.938\n",
            "[82,    10] loss: 1.479\n",
            "[82,    20] loss: 1.478\n",
            "[82,    30] loss: 1.485\n",
            "[82,    40] loss: 1.472\n",
            "[82,    50] loss: 1.479\n",
            "[82,    60] loss: 1.479\n",
            "[82,    70] loss: 1.476\n",
            "[82,    80] loss: 1.472\n",
            "[82,    90] loss: 1.484\n",
            "[82,   100] loss: 1.476\n",
            "[82,   110] loss: 1.472\n",
            "[82,   120] loss: 1.474\n",
            "[82,   130] loss: 1.479\n",
            "[82,   140] loss: 1.477\n",
            "[82,   150] loss: 1.475\n",
            "[82,   160] loss: 1.475\n",
            "[82,   170] loss: 1.482\n",
            "[82,   180] loss: 1.479\n",
            "[82,   190] loss: 1.477\n",
            "[82,   200] loss: 1.478\n",
            "[82,   210] loss: 1.481\n",
            "[82,   220] loss: 1.471\n",
            "[82,   230] loss: 1.478\n",
            "[82,   240] loss: 1.478\n",
            "[82,   250] loss: 1.483\n",
            "[82,   260] loss: 1.480\n",
            "[82,   270] loss: 1.472\n",
            "[82,   280] loss: 1.473\n",
            "[82,   290] loss: 1.476\n",
            "[82,   300] loss: 1.480\n",
            "[82,   310] loss: 1.471\n",
            "[82,   320] loss: 1.484\n",
            "[82,   330] loss: 1.477\n",
            "[82,   340] loss: 1.481\n",
            "[82,   350] loss: 1.474\n",
            "[82,   360] loss: 1.476\n",
            "[82,   370] loss: 1.473\n",
            "[82,   380] loss: 1.474\n",
            "[82,   390] loss: 1.475\n",
            "[82,   400] loss: 1.477\n",
            "[82,   410] loss: 1.477\n",
            "[82,   420] loss: 1.477\n",
            "[82,   430] loss: 1.476\n",
            "epoch 81 val_loss 61.15876269340515 val_steps 40 val_acc 0.9332\n",
            "[83,    10] loss: 1.477\n",
            "[83,    20] loss: 1.480\n",
            "[83,    30] loss: 1.476\n",
            "[83,    40] loss: 1.475\n",
            "[83,    50] loss: 1.473\n",
            "[83,    60] loss: 1.474\n",
            "[83,    70] loss: 1.473\n",
            "[83,    80] loss: 1.477\n",
            "[83,    90] loss: 1.476\n",
            "[83,   100] loss: 1.476\n",
            "[83,   110] loss: 1.472\n",
            "[83,   120] loss: 1.480\n",
            "[83,   130] loss: 1.477\n",
            "[83,   140] loss: 1.480\n",
            "[83,   150] loss: 1.484\n",
            "[83,   160] loss: 1.474\n",
            "[83,   170] loss: 1.472\n",
            "[83,   180] loss: 1.476\n",
            "[83,   190] loss: 1.475\n",
            "[83,   200] loss: 1.481\n",
            "[83,   210] loss: 1.481\n",
            "[83,   220] loss: 1.472\n",
            "[83,   230] loss: 1.478\n",
            "[83,   240] loss: 1.478\n",
            "[83,   250] loss: 1.470\n",
            "[83,   260] loss: 1.473\n",
            "[83,   270] loss: 1.476\n",
            "[83,   280] loss: 1.472\n",
            "[83,   290] loss: 1.479\n",
            "[83,   300] loss: 1.480\n",
            "[83,   310] loss: 1.475\n",
            "[83,   320] loss: 1.472\n",
            "[83,   330] loss: 1.476\n",
            "[83,   340] loss: 1.475\n",
            "[83,   350] loss: 1.479\n",
            "[83,   360] loss: 1.476\n",
            "[83,   370] loss: 1.482\n",
            "[83,   380] loss: 1.481\n",
            "[83,   390] loss: 1.476\n",
            "[83,   400] loss: 1.471\n",
            "[83,   410] loss: 1.483\n",
            "[83,   420] loss: 1.474\n",
            "[83,   430] loss: 1.477\n",
            "epoch 82 val_loss 61.074917674064636 val_steps 40 val_acc 0.9352\n",
            "[84,    10] loss: 1.472\n",
            "[84,    20] loss: 1.477\n",
            "[84,    30] loss: 1.477\n",
            "[84,    40] loss: 1.477\n",
            "[84,    50] loss: 1.475\n",
            "[84,    60] loss: 1.472\n",
            "[84,    70] loss: 1.476\n",
            "[84,    80] loss: 1.478\n",
            "[84,    90] loss: 1.477\n",
            "[84,   100] loss: 1.477\n",
            "[84,   110] loss: 1.479\n",
            "[84,   120] loss: 1.467\n",
            "[84,   130] loss: 1.481\n",
            "[84,   140] loss: 1.483\n",
            "[84,   150] loss: 1.476\n",
            "[84,   160] loss: 1.473\n",
            "[84,   170] loss: 1.480\n",
            "[84,   180] loss: 1.478\n",
            "[84,   190] loss: 1.475\n",
            "[84,   200] loss: 1.473\n",
            "[84,   210] loss: 1.474\n",
            "[84,   220] loss: 1.473\n",
            "[84,   230] loss: 1.478\n",
            "[84,   240] loss: 1.480\n",
            "[84,   250] loss: 1.473\n",
            "[84,   260] loss: 1.476\n",
            "[84,   270] loss: 1.476\n",
            "[84,   280] loss: 1.477\n",
            "[84,   290] loss: 1.480\n",
            "[84,   300] loss: 1.476\n",
            "[84,   310] loss: 1.470\n",
            "[84,   320] loss: 1.477\n",
            "[84,   330] loss: 1.472\n",
            "[84,   340] loss: 1.481\n",
            "[84,   350] loss: 1.479\n",
            "[84,   360] loss: 1.476\n",
            "[84,   370] loss: 1.472\n",
            "[84,   380] loss: 1.476\n",
            "[84,   390] loss: 1.479\n",
            "[84,   400] loss: 1.472\n",
            "[84,   410] loss: 1.473\n",
            "[84,   420] loss: 1.478\n",
            "[84,   430] loss: 1.480\n",
            "epoch 83 val_loss 61.178232192993164 val_steps 40 val_acc 0.9366\n",
            "[85,    10] loss: 1.472\n",
            "[85,    20] loss: 1.475\n",
            "[85,    30] loss: 1.471\n",
            "[85,    40] loss: 1.475\n",
            "[85,    50] loss: 1.476\n",
            "[85,    60] loss: 1.476\n",
            "[85,    70] loss: 1.474\n",
            "[85,    80] loss: 1.474\n",
            "[85,    90] loss: 1.481\n",
            "[85,   100] loss: 1.476\n",
            "[85,   110] loss: 1.474\n",
            "[85,   120] loss: 1.477\n",
            "[85,   130] loss: 1.473\n",
            "[85,   140] loss: 1.477\n",
            "[85,   150] loss: 1.473\n",
            "[85,   160] loss: 1.478\n",
            "[85,   170] loss: 1.477\n",
            "[85,   180] loss: 1.480\n",
            "[85,   190] loss: 1.477\n",
            "[85,   200] loss: 1.473\n",
            "[85,   210] loss: 1.476\n",
            "[85,   220] loss: 1.470\n",
            "[85,   230] loss: 1.474\n",
            "[85,   240] loss: 1.475\n",
            "[85,   250] loss: 1.481\n",
            "[85,   260] loss: 1.469\n",
            "[85,   270] loss: 1.478\n",
            "[85,   280] loss: 1.477\n",
            "[85,   290] loss: 1.478\n",
            "[85,   300] loss: 1.475\n",
            "[85,   310] loss: 1.478\n",
            "[85,   320] loss: 1.473\n",
            "[85,   330] loss: 1.479\n",
            "[85,   340] loss: 1.475\n",
            "[85,   350] loss: 1.477\n",
            "[85,   360] loss: 1.473\n",
            "[85,   370] loss: 1.480\n",
            "[85,   380] loss: 1.482\n",
            "[85,   390] loss: 1.482\n",
            "[85,   400] loss: 1.475\n",
            "[85,   410] loss: 1.476\n",
            "[85,   420] loss: 1.478\n",
            "[85,   430] loss: 1.472\n",
            "epoch 84 val_loss 60.973347425460815 val_steps 40 val_acc 0.9372\n",
            "[86,    10] loss: 1.476\n",
            "[86,    20] loss: 1.477\n",
            "[86,    30] loss: 1.469\n",
            "[86,    40] loss: 1.479\n",
            "[86,    50] loss: 1.479\n",
            "[86,    60] loss: 1.479\n",
            "[86,    70] loss: 1.474\n",
            "[86,    80] loss: 1.475\n",
            "[86,    90] loss: 1.474\n",
            "[86,   100] loss: 1.476\n",
            "[86,   110] loss: 1.476\n",
            "[86,   120] loss: 1.475\n",
            "[86,   130] loss: 1.476\n",
            "[86,   140] loss: 1.476\n",
            "[86,   150] loss: 1.476\n",
            "[86,   160] loss: 1.474\n",
            "[86,   170] loss: 1.471\n",
            "[86,   180] loss: 1.480\n",
            "[86,   190] loss: 1.469\n",
            "[86,   200] loss: 1.474\n",
            "[86,   210] loss: 1.474\n",
            "[86,   220] loss: 1.474\n",
            "[86,   230] loss: 1.471\n",
            "[86,   240] loss: 1.472\n",
            "[86,   250] loss: 1.472\n",
            "[86,   260] loss: 1.480\n",
            "[86,   270] loss: 1.473\n",
            "[86,   280] loss: 1.482\n",
            "[86,   290] loss: 1.475\n",
            "[86,   300] loss: 1.475\n",
            "[86,   310] loss: 1.473\n",
            "[86,   320] loss: 1.472\n",
            "[86,   330] loss: 1.476\n",
            "[86,   340] loss: 1.478\n",
            "[86,   350] loss: 1.476\n",
            "[86,   360] loss: 1.480\n",
            "[86,   370] loss: 1.475\n",
            "[86,   380] loss: 1.478\n",
            "[86,   390] loss: 1.481\n",
            "[86,   400] loss: 1.473\n",
            "[86,   410] loss: 1.476\n",
            "[86,   420] loss: 1.479\n",
            "[86,   430] loss: 1.482\n",
            "epoch 85 val_loss 61.1516215801239 val_steps 40 val_acc 0.935\n",
            "[87,    10] loss: 1.476\n",
            "[87,    20] loss: 1.473\n",
            "[87,    30] loss: 1.474\n",
            "[87,    40] loss: 1.478\n",
            "[87,    50] loss: 1.476\n",
            "[87,    60] loss: 1.474\n",
            "[87,    70] loss: 1.480\n",
            "[87,    80] loss: 1.478\n",
            "[87,    90] loss: 1.478\n",
            "[87,   100] loss: 1.474\n",
            "[87,   110] loss: 1.474\n",
            "[87,   120] loss: 1.476\n",
            "[87,   130] loss: 1.471\n",
            "[87,   140] loss: 1.476\n",
            "[87,   150] loss: 1.475\n",
            "[87,   160] loss: 1.473\n",
            "[87,   170] loss: 1.472\n",
            "[87,   180] loss: 1.480\n",
            "[87,   190] loss: 1.471\n",
            "[87,   200] loss: 1.478\n",
            "[87,   210] loss: 1.478\n",
            "[87,   220] loss: 1.474\n",
            "[87,   230] loss: 1.471\n",
            "[87,   240] loss: 1.474\n",
            "[87,   250] loss: 1.477\n",
            "[87,   260] loss: 1.474\n",
            "[87,   270] loss: 1.477\n",
            "[87,   280] loss: 1.479\n",
            "[87,   290] loss: 1.473\n",
            "[87,   300] loss: 1.479\n",
            "[87,   310] loss: 1.474\n",
            "[87,   320] loss: 1.471\n",
            "[87,   330] loss: 1.471\n",
            "[87,   340] loss: 1.475\n",
            "[87,   350] loss: 1.473\n",
            "[87,   360] loss: 1.477\n",
            "[87,   370] loss: 1.477\n",
            "[87,   380] loss: 1.477\n",
            "[87,   390] loss: 1.472\n",
            "[87,   400] loss: 1.479\n",
            "[87,   410] loss: 1.476\n",
            "[87,   420] loss: 1.473\n",
            "[87,   430] loss: 1.476\n",
            "epoch 86 val_loss 61.1939582824707 val_steps 40 val_acc 0.9354\n",
            "[88,    10] loss: 1.480\n",
            "[88,    20] loss: 1.471\n",
            "[88,    30] loss: 1.476\n",
            "[88,    40] loss: 1.476\n",
            "[88,    50] loss: 1.477\n",
            "[88,    60] loss: 1.478\n",
            "[88,    70] loss: 1.475\n",
            "[88,    80] loss: 1.475\n",
            "[88,    90] loss: 1.472\n",
            "[88,   100] loss: 1.472\n",
            "[88,   110] loss: 1.475\n",
            "[88,   120] loss: 1.480\n",
            "[88,   130] loss: 1.473\n",
            "[88,   140] loss: 1.477\n",
            "[88,   150] loss: 1.476\n",
            "[88,   160] loss: 1.478\n",
            "[88,   170] loss: 1.474\n",
            "[88,   180] loss: 1.474\n",
            "[88,   190] loss: 1.472\n",
            "[88,   200] loss: 1.477\n",
            "[88,   210] loss: 1.468\n",
            "[88,   220] loss: 1.478\n",
            "[88,   230] loss: 1.477\n",
            "[88,   240] loss: 1.475\n",
            "[88,   250] loss: 1.483\n",
            "[88,   260] loss: 1.477\n",
            "[88,   270] loss: 1.471\n",
            "[88,   280] loss: 1.478\n",
            "[88,   290] loss: 1.477\n",
            "[88,   300] loss: 1.471\n",
            "[88,   310] loss: 1.474\n",
            "[88,   320] loss: 1.474\n",
            "[88,   330] loss: 1.476\n",
            "[88,   340] loss: 1.471\n",
            "[88,   350] loss: 1.472\n",
            "[88,   360] loss: 1.477\n",
            "[88,   370] loss: 1.474\n",
            "[88,   380] loss: 1.476\n",
            "[88,   390] loss: 1.474\n",
            "[88,   400] loss: 1.479\n",
            "[88,   410] loss: 1.475\n",
            "[88,   420] loss: 1.475\n",
            "[88,   430] loss: 1.475\n",
            "epoch 87 val_loss 61.134806990623474 val_steps 40 val_acc 0.9354\n",
            "[89,    10] loss: 1.473\n",
            "[89,    20] loss: 1.474\n",
            "[89,    30] loss: 1.476\n",
            "[89,    40] loss: 1.473\n",
            "[89,    50] loss: 1.475\n",
            "[89,    60] loss: 1.476\n",
            "[89,    70] loss: 1.476\n",
            "[89,    80] loss: 1.476\n",
            "[89,    90] loss: 1.477\n",
            "[89,   100] loss: 1.477\n",
            "[89,   110] loss: 1.475\n",
            "[89,   120] loss: 1.469\n",
            "[89,   130] loss: 1.479\n",
            "[89,   140] loss: 1.473\n",
            "[89,   150] loss: 1.468\n",
            "[89,   160] loss: 1.473\n",
            "[89,   170] loss: 1.471\n",
            "[89,   180] loss: 1.476\n",
            "[89,   190] loss: 1.477\n",
            "[89,   200] loss: 1.478\n",
            "[89,   210] loss: 1.476\n",
            "[89,   220] loss: 1.475\n",
            "[89,   230] loss: 1.471\n",
            "[89,   240] loss: 1.480\n",
            "[89,   250] loss: 1.476\n",
            "[89,   260] loss: 1.475\n",
            "[89,   270] loss: 1.477\n",
            "[89,   280] loss: 1.475\n",
            "[89,   290] loss: 1.476\n",
            "[89,   300] loss: 1.478\n",
            "[89,   310] loss: 1.475\n",
            "[89,   320] loss: 1.473\n",
            "[89,   330] loss: 1.474\n",
            "[89,   340] loss: 1.476\n",
            "[89,   350] loss: 1.479\n",
            "[89,   360] loss: 1.472\n",
            "[89,   370] loss: 1.473\n",
            "[89,   380] loss: 1.474\n",
            "[89,   390] loss: 1.472\n",
            "[89,   400] loss: 1.476\n",
            "[89,   410] loss: 1.474\n",
            "[89,   420] loss: 1.477\n",
            "[89,   430] loss: 1.474\n",
            "epoch 88 val_loss 60.970093965530396 val_steps 40 val_acc 0.9366\n",
            "[90,    10] loss: 1.475\n",
            "[90,    20] loss: 1.469\n",
            "[90,    30] loss: 1.475\n",
            "[90,    40] loss: 1.473\n",
            "[90,    50] loss: 1.474\n",
            "[90,    60] loss: 1.479\n",
            "[90,    70] loss: 1.478\n",
            "[90,    80] loss: 1.471\n",
            "[90,    90] loss: 1.469\n",
            "[90,   100] loss: 1.477\n",
            "[90,   110] loss: 1.472\n",
            "[90,   120] loss: 1.477\n",
            "[90,   130] loss: 1.469\n",
            "[90,   140] loss: 1.473\n",
            "[90,   150] loss: 1.478\n",
            "[90,   160] loss: 1.478\n",
            "[90,   170] loss: 1.474\n",
            "[90,   180] loss: 1.473\n",
            "[90,   190] loss: 1.477\n",
            "[90,   200] loss: 1.472\n",
            "[90,   210] loss: 1.474\n",
            "[90,   220] loss: 1.471\n",
            "[90,   230] loss: 1.478\n",
            "[90,   240] loss: 1.473\n",
            "[90,   250] loss: 1.473\n",
            "[90,   260] loss: 1.473\n",
            "[90,   270] loss: 1.477\n",
            "[90,   280] loss: 1.474\n",
            "[90,   290] loss: 1.476\n",
            "[90,   300] loss: 1.471\n",
            "[90,   310] loss: 1.479\n",
            "[90,   320] loss: 1.473\n",
            "[90,   330] loss: 1.479\n",
            "[90,   340] loss: 1.470\n",
            "[90,   350] loss: 1.468\n",
            "[90,   360] loss: 1.481\n",
            "[90,   370] loss: 1.480\n",
            "[90,   380] loss: 1.478\n",
            "[90,   390] loss: 1.475\n",
            "[90,   400] loss: 1.480\n",
            "[90,   410] loss: 1.477\n",
            "[90,   420] loss: 1.472\n",
            "[90,   430] loss: 1.479\n",
            "epoch 89 val_loss 61.00325000286102 val_steps 40 val_acc 0.936\n",
            "[91,    10] loss: 1.478\n",
            "[91,    20] loss: 1.471\n",
            "[91,    30] loss: 1.475\n",
            "[91,    40] loss: 1.474\n",
            "[91,    50] loss: 1.473\n",
            "[91,    60] loss: 1.477\n",
            "[91,    70] loss: 1.476\n",
            "[91,    80] loss: 1.473\n",
            "[91,    90] loss: 1.471\n",
            "[91,   100] loss: 1.468\n",
            "[91,   110] loss: 1.476\n",
            "[91,   120] loss: 1.474\n",
            "[91,   130] loss: 1.475\n",
            "[91,   140] loss: 1.476\n",
            "[91,   150] loss: 1.479\n",
            "[91,   160] loss: 1.474\n",
            "[91,   170] loss: 1.474\n",
            "[91,   180] loss: 1.473\n",
            "[91,   190] loss: 1.471\n",
            "[91,   200] loss: 1.475\n",
            "[91,   210] loss: 1.477\n",
            "[91,   220] loss: 1.473\n",
            "[91,   230] loss: 1.479\n",
            "[91,   240] loss: 1.479\n",
            "[91,   250] loss: 1.471\n",
            "[91,   260] loss: 1.475\n",
            "[91,   270] loss: 1.475\n",
            "[91,   280] loss: 1.472\n",
            "[91,   290] loss: 1.475\n",
            "[91,   300] loss: 1.473\n",
            "[91,   310] loss: 1.480\n",
            "[91,   320] loss: 1.473\n",
            "[91,   330] loss: 1.476\n",
            "[91,   340] loss: 1.477\n",
            "[91,   350] loss: 1.468\n",
            "[91,   360] loss: 1.475\n",
            "[91,   370] loss: 1.473\n",
            "[91,   380] loss: 1.477\n",
            "[91,   390] loss: 1.472\n",
            "[91,   400] loss: 1.477\n",
            "[91,   410] loss: 1.474\n",
            "[91,   420] loss: 1.470\n",
            "[91,   430] loss: 1.476\n",
            "epoch 90 val_loss 61.006032943725586 val_steps 40 val_acc 0.937\n",
            "[92,    10] loss: 1.474\n",
            "[92,    20] loss: 1.477\n",
            "[92,    30] loss: 1.478\n",
            "[92,    40] loss: 1.480\n",
            "[92,    50] loss: 1.479\n",
            "[92,    60] loss: 1.477\n",
            "[92,    70] loss: 1.475\n",
            "[92,    80] loss: 1.475\n",
            "[92,    90] loss: 1.476\n",
            "[92,   100] loss: 1.476\n",
            "[92,   110] loss: 1.483\n",
            "[92,   120] loss: 1.471\n",
            "[92,   130] loss: 1.472\n",
            "[92,   140] loss: 1.476\n",
            "[92,   150] loss: 1.473\n",
            "[92,   160] loss: 1.473\n",
            "[92,   170] loss: 1.468\n",
            "[92,   180] loss: 1.474\n",
            "[92,   190] loss: 1.474\n",
            "[92,   200] loss: 1.475\n",
            "[92,   210] loss: 1.471\n",
            "[92,   220] loss: 1.469\n",
            "[92,   230] loss: 1.477\n",
            "[92,   240] loss: 1.474\n",
            "[92,   250] loss: 1.474\n",
            "[92,   260] loss: 1.475\n",
            "[92,   270] loss: 1.475\n",
            "[92,   280] loss: 1.472\n",
            "[92,   290] loss: 1.475\n",
            "[92,   300] loss: 1.475\n",
            "[92,   310] loss: 1.477\n",
            "[92,   320] loss: 1.473\n",
            "[92,   330] loss: 1.474\n",
            "[92,   340] loss: 1.470\n",
            "[92,   350] loss: 1.478\n",
            "[92,   360] loss: 1.474\n",
            "[92,   370] loss: 1.476\n",
            "[92,   380] loss: 1.471\n",
            "[92,   390] loss: 1.472\n",
            "[92,   400] loss: 1.471\n",
            "[92,   410] loss: 1.468\n",
            "[92,   420] loss: 1.470\n",
            "[92,   430] loss: 1.477\n",
            "epoch 91 val_loss 61.10106348991394 val_steps 40 val_acc 0.933\n",
            "[93,    10] loss: 1.474\n",
            "[93,    20] loss: 1.474\n",
            "[93,    30] loss: 1.474\n",
            "[93,    40] loss: 1.474\n",
            "[93,    50] loss: 1.474\n",
            "[93,    60] loss: 1.475\n",
            "[93,    70] loss: 1.476\n",
            "[93,    80] loss: 1.472\n",
            "[93,    90] loss: 1.471\n",
            "[93,   100] loss: 1.469\n",
            "[93,   110] loss: 1.480\n",
            "[93,   120] loss: 1.472\n",
            "[93,   130] loss: 1.480\n",
            "[93,   140] loss: 1.478\n",
            "[93,   150] loss: 1.471\n",
            "[93,   160] loss: 1.475\n",
            "[93,   170] loss: 1.477\n",
            "[93,   180] loss: 1.475\n",
            "[93,   190] loss: 1.479\n",
            "[93,   200] loss: 1.468\n",
            "[93,   210] loss: 1.473\n",
            "[93,   220] loss: 1.475\n",
            "[93,   230] loss: 1.473\n",
            "[93,   240] loss: 1.476\n",
            "[93,   250] loss: 1.478\n",
            "[93,   260] loss: 1.473\n",
            "[93,   270] loss: 1.473\n",
            "[93,   280] loss: 1.474\n",
            "[93,   290] loss: 1.475\n",
            "[93,   300] loss: 1.477\n",
            "[93,   310] loss: 1.472\n",
            "[93,   320] loss: 1.473\n",
            "[93,   330] loss: 1.472\n",
            "[93,   340] loss: 1.468\n",
            "[93,   350] loss: 1.479\n",
            "[93,   360] loss: 1.472\n",
            "[93,   370] loss: 1.473\n",
            "[93,   380] loss: 1.476\n",
            "[93,   390] loss: 1.466\n",
            "[93,   400] loss: 1.475\n",
            "[93,   410] loss: 1.476\n",
            "[93,   420] loss: 1.479\n",
            "[93,   430] loss: 1.473\n",
            "epoch 92 val_loss 61.06628775596619 val_steps 40 val_acc 0.9346\n",
            "[94,    10] loss: 1.468\n",
            "[94,    20] loss: 1.480\n",
            "[94,    30] loss: 1.480\n",
            "[94,    40] loss: 1.472\n",
            "[94,    50] loss: 1.480\n",
            "[94,    60] loss: 1.476\n",
            "[94,    70] loss: 1.480\n",
            "[94,    80] loss: 1.468\n",
            "[94,    90] loss: 1.480\n",
            "[94,   100] loss: 1.472\n",
            "[94,   110] loss: 1.477\n",
            "[94,   120] loss: 1.473\n",
            "[94,   130] loss: 1.473\n",
            "[94,   140] loss: 1.474\n",
            "[94,   150] loss: 1.478\n",
            "[94,   160] loss: 1.466\n",
            "[94,   170] loss: 1.474\n",
            "[94,   180] loss: 1.475\n",
            "[94,   190] loss: 1.473\n",
            "[94,   200] loss: 1.473\n",
            "[94,   210] loss: 1.476\n",
            "[94,   220] loss: 1.471\n",
            "[94,   230] loss: 1.477\n",
            "[94,   240] loss: 1.473\n",
            "[94,   250] loss: 1.475\n",
            "[94,   260] loss: 1.471\n",
            "[94,   270] loss: 1.473\n",
            "[94,   280] loss: 1.471\n",
            "[94,   290] loss: 1.479\n",
            "[94,   300] loss: 1.476\n",
            "[94,   310] loss: 1.474\n",
            "[94,   320] loss: 1.466\n",
            "[94,   330] loss: 1.476\n",
            "[94,   340] loss: 1.475\n",
            "[94,   350] loss: 1.473\n",
            "[94,   360] loss: 1.473\n",
            "[94,   370] loss: 1.475\n",
            "[94,   380] loss: 1.472\n",
            "[94,   390] loss: 1.474\n",
            "[94,   400] loss: 1.468\n",
            "[94,   410] loss: 1.477\n",
            "[94,   420] loss: 1.472\n",
            "[94,   430] loss: 1.473\n",
            "epoch 93 val_loss 61.2046844959259 val_steps 40 val_acc 0.9322\n",
            "[95,    10] loss: 1.476\n",
            "[95,    20] loss: 1.473\n",
            "[95,    30] loss: 1.473\n",
            "[95,    40] loss: 1.476\n",
            "[95,    50] loss: 1.471\n",
            "[95,    60] loss: 1.472\n",
            "[95,    70] loss: 1.473\n",
            "[95,    80] loss: 1.474\n",
            "[95,    90] loss: 1.480\n",
            "[95,   100] loss: 1.477\n",
            "[95,   110] loss: 1.472\n",
            "[95,   120] loss: 1.476\n",
            "[95,   130] loss: 1.473\n",
            "[95,   140] loss: 1.473\n",
            "[95,   150] loss: 1.473\n",
            "[95,   160] loss: 1.471\n",
            "[95,   170] loss: 1.473\n",
            "[95,   180] loss: 1.475\n",
            "[95,   190] loss: 1.477\n",
            "[95,   200] loss: 1.474\n",
            "[95,   210] loss: 1.477\n",
            "[95,   220] loss: 1.477\n",
            "[95,   230] loss: 1.476\n",
            "[95,   240] loss: 1.473\n",
            "[95,   250] loss: 1.474\n",
            "[95,   260] loss: 1.476\n",
            "[95,   270] loss: 1.477\n",
            "[95,   280] loss: 1.476\n",
            "[95,   290] loss: 1.473\n",
            "[95,   300] loss: 1.469\n",
            "[95,   310] loss: 1.470\n",
            "[95,   320] loss: 1.468\n",
            "[95,   330] loss: 1.478\n",
            "[95,   340] loss: 1.472\n",
            "[95,   350] loss: 1.471\n",
            "[95,   360] loss: 1.482\n",
            "[95,   370] loss: 1.478\n",
            "[95,   380] loss: 1.475\n",
            "[95,   390] loss: 1.475\n",
            "[95,   400] loss: 1.466\n",
            "[95,   410] loss: 1.470\n",
            "[95,   420] loss: 1.473\n",
            "[95,   430] loss: 1.474\n",
            "epoch 94 val_loss 61.07389461994171 val_steps 40 val_acc 0.9348\n",
            "[96,    10] loss: 1.475\n",
            "[96,    20] loss: 1.473\n",
            "[96,    30] loss: 1.470\n",
            "[96,    40] loss: 1.476\n",
            "[96,    50] loss: 1.479\n",
            "[96,    60] loss: 1.475\n",
            "[96,    70] loss: 1.469\n",
            "[96,    80] loss: 1.476\n",
            "[96,    90] loss: 1.471\n",
            "[96,   100] loss: 1.471\n",
            "[96,   110] loss: 1.476\n",
            "[96,   120] loss: 1.477\n",
            "[96,   130] loss: 1.471\n",
            "[96,   140] loss: 1.469\n",
            "[96,   150] loss: 1.475\n",
            "[96,   160] loss: 1.472\n",
            "[96,   170] loss: 1.473\n",
            "[96,   180] loss: 1.475\n",
            "[96,   190] loss: 1.473\n",
            "[96,   200] loss: 1.472\n",
            "[96,   210] loss: 1.469\n",
            "[96,   220] loss: 1.470\n",
            "[96,   230] loss: 1.477\n",
            "[96,   240] loss: 1.478\n",
            "[96,   250] loss: 1.470\n",
            "[96,   260] loss: 1.478\n",
            "[96,   270] loss: 1.479\n",
            "[96,   280] loss: 1.474\n",
            "[96,   290] loss: 1.474\n",
            "[96,   300] loss: 1.473\n",
            "[96,   310] loss: 1.473\n",
            "[96,   320] loss: 1.471\n",
            "[96,   330] loss: 1.481\n",
            "[96,   340] loss: 1.476\n",
            "[96,   350] loss: 1.472\n",
            "[96,   360] loss: 1.471\n",
            "[96,   370] loss: 1.472\n",
            "[96,   380] loss: 1.480\n",
            "[96,   390] loss: 1.472\n",
            "[96,   400] loss: 1.469\n",
            "[96,   410] loss: 1.472\n",
            "[96,   420] loss: 1.474\n",
            "[96,   430] loss: 1.477\n",
            "epoch 95 val_loss 61.13694751262665 val_steps 40 val_acc 0.9348\n",
            "[97,    10] loss: 1.475\n",
            "[97,    20] loss: 1.474\n",
            "[97,    30] loss: 1.478\n",
            "[97,    40] loss: 1.473\n",
            "[97,    50] loss: 1.474\n",
            "[97,    60] loss: 1.470\n",
            "[97,    70] loss: 1.475\n",
            "[97,    80] loss: 1.477\n",
            "[97,    90] loss: 1.469\n",
            "[97,   100] loss: 1.473\n",
            "[97,   110] loss: 1.474\n",
            "[97,   120] loss: 1.475\n",
            "[97,   130] loss: 1.474\n",
            "[97,   140] loss: 1.473\n",
            "[97,   150] loss: 1.472\n",
            "[97,   160] loss: 1.469\n",
            "[97,   170] loss: 1.473\n",
            "[97,   180] loss: 1.478\n",
            "[97,   190] loss: 1.473\n",
            "[97,   200] loss: 1.474\n",
            "[97,   210] loss: 1.479\n",
            "[97,   220] loss: 1.471\n",
            "[97,   230] loss: 1.470\n",
            "[97,   240] loss: 1.473\n",
            "[97,   250] loss: 1.476\n",
            "[97,   260] loss: 1.475\n",
            "[97,   270] loss: 1.470\n",
            "[97,   280] loss: 1.478\n",
            "[97,   290] loss: 1.472\n",
            "[97,   300] loss: 1.475\n",
            "[97,   310] loss: 1.477\n",
            "[97,   320] loss: 1.478\n",
            "[97,   330] loss: 1.469\n",
            "[97,   340] loss: 1.473\n",
            "[97,   350] loss: 1.472\n",
            "[97,   360] loss: 1.476\n",
            "[97,   370] loss: 1.476\n",
            "[97,   380] loss: 1.470\n",
            "[97,   390] loss: 1.473\n",
            "[97,   400] loss: 1.476\n",
            "[97,   410] loss: 1.469\n",
            "[97,   420] loss: 1.472\n",
            "[97,   430] loss: 1.472\n",
            "epoch 96 val_loss 61.03120815753937 val_steps 40 val_acc 0.9354\n",
            "[98,    10] loss: 1.475\n",
            "[98,    20] loss: 1.474\n",
            "[98,    30] loss: 1.469\n",
            "[98,    40] loss: 1.465\n",
            "[98,    50] loss: 1.470\n",
            "[98,    60] loss: 1.476\n",
            "[98,    70] loss: 1.475\n",
            "[98,    80] loss: 1.477\n",
            "[98,    90] loss: 1.469\n",
            "[98,   100] loss: 1.476\n",
            "[98,   110] loss: 1.470\n",
            "[98,   120] loss: 1.474\n",
            "[98,   130] loss: 1.471\n",
            "[98,   140] loss: 1.470\n",
            "[98,   150] loss: 1.470\n",
            "[98,   160] loss: 1.469\n",
            "[98,   170] loss: 1.475\n",
            "[98,   180] loss: 1.476\n",
            "[98,   190] loss: 1.479\n",
            "[98,   200] loss: 1.472\n",
            "[98,   210] loss: 1.473\n",
            "[98,   220] loss: 1.480\n",
            "[98,   230] loss: 1.472\n",
            "[98,   240] loss: 1.477\n",
            "[98,   250] loss: 1.471\n",
            "[98,   260] loss: 1.476\n",
            "[98,   270] loss: 1.474\n",
            "[98,   280] loss: 1.478\n",
            "[98,   290] loss: 1.470\n",
            "[98,   300] loss: 1.470\n",
            "[98,   310] loss: 1.469\n",
            "[98,   320] loss: 1.476\n",
            "[98,   330] loss: 1.473\n",
            "[98,   340] loss: 1.473\n",
            "[98,   350] loss: 1.481\n",
            "[98,   360] loss: 1.473\n",
            "[98,   370] loss: 1.474\n",
            "[98,   380] loss: 1.474\n",
            "[98,   390] loss: 1.473\n",
            "[98,   400] loss: 1.470\n",
            "[98,   410] loss: 1.475\n",
            "[98,   420] loss: 1.473\n",
            "[98,   430] loss: 1.477\n",
            "epoch 97 val_loss 61.12239873409271 val_steps 40 val_acc 0.9352\n",
            "[99,    10] loss: 1.474\n",
            "[99,    20] loss: 1.477\n",
            "[99,    30] loss: 1.468\n",
            "[99,    40] loss: 1.475\n",
            "[99,    50] loss: 1.472\n",
            "[99,    60] loss: 1.476\n",
            "[99,    70] loss: 1.470\n",
            "[99,    80] loss: 1.473\n",
            "[99,    90] loss: 1.471\n",
            "[99,   100] loss: 1.474\n",
            "[99,   110] loss: 1.479\n",
            "[99,   120] loss: 1.472\n",
            "[99,   130] loss: 1.469\n",
            "[99,   140] loss: 1.477\n",
            "[99,   150] loss: 1.472\n",
            "[99,   160] loss: 1.470\n",
            "[99,   170] loss: 1.474\n",
            "[99,   180] loss: 1.472\n",
            "[99,   190] loss: 1.476\n",
            "[99,   200] loss: 1.471\n",
            "[99,   210] loss: 1.468\n",
            "[99,   220] loss: 1.475\n",
            "[99,   230] loss: 1.474\n",
            "[99,   240] loss: 1.473\n",
            "[99,   250] loss: 1.475\n",
            "[99,   260] loss: 1.478\n",
            "[99,   270] loss: 1.473\n",
            "[99,   280] loss: 1.472\n",
            "[99,   290] loss: 1.478\n",
            "[99,   300] loss: 1.471\n",
            "[99,   310] loss: 1.474\n",
            "[99,   320] loss: 1.470\n",
            "[99,   330] loss: 1.472\n",
            "[99,   340] loss: 1.477\n",
            "[99,   350] loss: 1.473\n",
            "[99,   360] loss: 1.471\n",
            "[99,   370] loss: 1.478\n",
            "[99,   380] loss: 1.469\n",
            "[99,   390] loss: 1.473\n",
            "[99,   400] loss: 1.474\n",
            "[99,   410] loss: 1.467\n",
            "[99,   420] loss: 1.473\n",
            "[99,   430] loss: 1.472\n",
            "epoch 98 val_loss 61.179433822631836 val_steps 40 val_acc 0.935\n",
            "[100,    10] loss: 1.483\n",
            "[100,    20] loss: 1.474\n",
            "[100,    30] loss: 1.473\n",
            "[100,    40] loss: 1.472\n",
            "[100,    50] loss: 1.472\n",
            "[100,    60] loss: 1.475\n",
            "[100,    70] loss: 1.474\n",
            "[100,    80] loss: 1.474\n",
            "[100,    90] loss: 1.470\n",
            "[100,   100] loss: 1.468\n",
            "[100,   110] loss: 1.477\n",
            "[100,   120] loss: 1.471\n",
            "[100,   130] loss: 1.478\n",
            "[100,   140] loss: 1.469\n",
            "[100,   150] loss: 1.480\n",
            "[100,   160] loss: 1.476\n",
            "[100,   170] loss: 1.478\n",
            "[100,   180] loss: 1.477\n",
            "[100,   190] loss: 1.471\n",
            "[100,   200] loss: 1.473\n",
            "[100,   210] loss: 1.473\n",
            "[100,   220] loss: 1.471\n",
            "[100,   230] loss: 1.477\n",
            "[100,   240] loss: 1.473\n",
            "[100,   250] loss: 1.475\n",
            "[100,   260] loss: 1.471\n",
            "[100,   270] loss: 1.471\n",
            "[100,   280] loss: 1.470\n",
            "[100,   290] loss: 1.473\n",
            "[100,   300] loss: 1.477\n",
            "[100,   310] loss: 1.475\n",
            "[100,   320] loss: 1.472\n",
            "[100,   330] loss: 1.466\n",
            "[100,   340] loss: 1.476\n",
            "[100,   350] loss: 1.473\n",
            "[100,   360] loss: 1.466\n",
            "[100,   370] loss: 1.468\n",
            "[100,   380] loss: 1.473\n",
            "[100,   390] loss: 1.472\n",
            "[100,   400] loss: 1.466\n",
            "[100,   410] loss: 1.470\n",
            "[100,   420] loss: 1.478\n",
            "[100,   430] loss: 1.471\n",
            "epoch 99 val_loss 61.3364075422287 val_steps 40 val_acc 0.9318\n",
            "[101,    10] loss: 1.473\n",
            "[101,    20] loss: 1.474\n",
            "[101,    30] loss: 1.475\n",
            "[101,    40] loss: 1.474\n",
            "[101,    50] loss: 1.469\n",
            "[101,    60] loss: 1.473\n",
            "[101,    70] loss: 1.469\n",
            "[101,    80] loss: 1.468\n",
            "[101,    90] loss: 1.478\n",
            "[101,   100] loss: 1.476\n",
            "[101,   110] loss: 1.472\n",
            "[101,   120] loss: 1.482\n",
            "[101,   130] loss: 1.476\n",
            "[101,   140] loss: 1.473\n",
            "[101,   150] loss: 1.479\n",
            "[101,   160] loss: 1.475\n",
            "[101,   170] loss: 1.476\n",
            "[101,   180] loss: 1.472\n",
            "[101,   190] loss: 1.474\n",
            "[101,   200] loss: 1.468\n",
            "[101,   210] loss: 1.473\n",
            "[101,   220] loss: 1.471\n",
            "[101,   230] loss: 1.473\n",
            "[101,   240] loss: 1.471\n",
            "[101,   250] loss: 1.476\n",
            "[101,   260] loss: 1.472\n",
            "[101,   270] loss: 1.468\n",
            "[101,   280] loss: 1.473\n",
            "[101,   290] loss: 1.476\n",
            "[101,   300] loss: 1.476\n",
            "[101,   310] loss: 1.469\n",
            "[101,   320] loss: 1.472\n",
            "[101,   330] loss: 1.472\n",
            "[101,   340] loss: 1.473\n",
            "[101,   350] loss: 1.479\n",
            "[101,   360] loss: 1.472\n",
            "[101,   370] loss: 1.472\n",
            "[101,   380] loss: 1.474\n",
            "[101,   390] loss: 1.469\n",
            "[101,   400] loss: 1.468\n",
            "[101,   410] loss: 1.469\n",
            "[101,   420] loss: 1.471\n",
            "[101,   430] loss: 1.473\n",
            "epoch 100 val_loss 61.18383979797363 val_steps 40 val_acc 0.9334\n",
            "[102,    10] loss: 1.472\n",
            "[102,    20] loss: 1.471\n",
            "[102,    30] loss: 1.478\n",
            "[102,    40] loss: 1.472\n",
            "[102,    50] loss: 1.472\n",
            "[102,    60] loss: 1.477\n",
            "[102,    70] loss: 1.471\n",
            "[102,    80] loss: 1.472\n",
            "[102,    90] loss: 1.474\n",
            "[102,   100] loss: 1.472\n",
            "[102,   110] loss: 1.470\n",
            "[102,   120] loss: 1.475\n",
            "[102,   130] loss: 1.477\n",
            "[102,   140] loss: 1.477\n",
            "[102,   150] loss: 1.470\n",
            "[102,   160] loss: 1.475\n",
            "[102,   170] loss: 1.475\n",
            "[102,   180] loss: 1.473\n",
            "[102,   190] loss: 1.477\n",
            "[102,   200] loss: 1.467\n",
            "[102,   210] loss: 1.473\n",
            "[102,   220] loss: 1.470\n",
            "[102,   230] loss: 1.472\n",
            "[102,   240] loss: 1.473\n",
            "[102,   250] loss: 1.469\n",
            "[102,   260] loss: 1.473\n",
            "[102,   270] loss: 1.479\n",
            "[102,   280] loss: 1.472\n",
            "[102,   290] loss: 1.472\n",
            "[102,   300] loss: 1.471\n",
            "[102,   310] loss: 1.470\n",
            "[102,   320] loss: 1.474\n",
            "[102,   330] loss: 1.479\n",
            "[102,   340] loss: 1.470\n",
            "[102,   350] loss: 1.472\n",
            "[102,   360] loss: 1.469\n",
            "[102,   370] loss: 1.469\n",
            "[102,   380] loss: 1.475\n",
            "[102,   390] loss: 1.473\n",
            "[102,   400] loss: 1.477\n",
            "[102,   410] loss: 1.470\n",
            "[102,   420] loss: 1.474\n",
            "[102,   430] loss: 1.469\n",
            "epoch 101 val_loss 61.08880150318146 val_steps 40 val_acc 0.9336\n",
            "[103,    10] loss: 1.471\n",
            "[103,    20] loss: 1.473\n",
            "[103,    30] loss: 1.474\n",
            "[103,    40] loss: 1.477\n",
            "[103,    50] loss: 1.476\n",
            "[103,    60] loss: 1.477\n",
            "[103,    70] loss: 1.472\n",
            "[103,    80] loss: 1.468\n",
            "[103,    90] loss: 1.471\n",
            "[103,   100] loss: 1.475\n",
            "[103,   110] loss: 1.469\n",
            "[103,   120] loss: 1.467\n",
            "[103,   130] loss: 1.472\n",
            "[103,   140] loss: 1.474\n",
            "[103,   150] loss: 1.473\n",
            "[103,   160] loss: 1.475\n",
            "[103,   170] loss: 1.476\n",
            "[103,   180] loss: 1.473\n",
            "[103,   190] loss: 1.472\n",
            "[103,   200] loss: 1.468\n",
            "[103,   210] loss: 1.469\n",
            "[103,   220] loss: 1.475\n",
            "[103,   230] loss: 1.475\n",
            "[103,   240] loss: 1.471\n",
            "[103,   250] loss: 1.469\n",
            "[103,   260] loss: 1.469\n",
            "[103,   270] loss: 1.471\n",
            "[103,   280] loss: 1.476\n",
            "[103,   290] loss: 1.476\n",
            "[103,   300] loss: 1.472\n",
            "[103,   310] loss: 1.478\n",
            "[103,   320] loss: 1.471\n",
            "[103,   330] loss: 1.473\n",
            "[103,   340] loss: 1.473\n",
            "[103,   350] loss: 1.473\n",
            "[103,   360] loss: 1.472\n",
            "[103,   370] loss: 1.474\n",
            "[103,   380] loss: 1.468\n",
            "[103,   390] loss: 1.477\n",
            "[103,   400] loss: 1.471\n",
            "[103,   410] loss: 1.476\n",
            "[103,   420] loss: 1.475\n",
            "[103,   430] loss: 1.469\n",
            "epoch 102 val_loss 61.19205188751221 val_steps 40 val_acc 0.9332\n",
            "[104,    10] loss: 1.471\n",
            "[104,    20] loss: 1.476\n",
            "[104,    30] loss: 1.473\n",
            "[104,    40] loss: 1.471\n",
            "[104,    50] loss: 1.474\n",
            "[104,    60] loss: 1.473\n",
            "[104,    70] loss: 1.469\n",
            "[104,    80] loss: 1.474\n",
            "[104,    90] loss: 1.472\n",
            "[104,   100] loss: 1.473\n",
            "[104,   110] loss: 1.472\n",
            "[104,   120] loss: 1.467\n",
            "[104,   130] loss: 1.476\n",
            "[104,   140] loss: 1.474\n",
            "[104,   150] loss: 1.474\n",
            "[104,   160] loss: 1.473\n",
            "[104,   170] loss: 1.474\n",
            "[104,   180] loss: 1.471\n",
            "[104,   190] loss: 1.468\n",
            "[104,   200] loss: 1.482\n",
            "[104,   210] loss: 1.476\n",
            "[104,   220] loss: 1.473\n",
            "[104,   230] loss: 1.476\n",
            "[104,   240] loss: 1.473\n",
            "[104,   250] loss: 1.469\n",
            "[104,   260] loss: 1.468\n",
            "[104,   270] loss: 1.476\n",
            "[104,   280] loss: 1.467\n",
            "[104,   290] loss: 1.474\n",
            "[104,   300] loss: 1.473\n",
            "[104,   310] loss: 1.468\n",
            "[104,   320] loss: 1.466\n",
            "[104,   330] loss: 1.469\n",
            "[104,   340] loss: 1.472\n",
            "[104,   350] loss: 1.477\n",
            "[104,   360] loss: 1.478\n",
            "[104,   370] loss: 1.475\n",
            "[104,   380] loss: 1.469\n",
            "[104,   390] loss: 1.476\n",
            "[104,   400] loss: 1.474\n",
            "[104,   410] loss: 1.470\n",
            "[104,   420] loss: 1.473\n",
            "[104,   430] loss: 1.473\n",
            "epoch 103 val_loss 61.30351209640503 val_steps 40 val_acc 0.9338\n",
            "[105,    10] loss: 1.474\n",
            "[105,    20] loss: 1.470\n",
            "[105,    30] loss: 1.473\n",
            "[105,    40] loss: 1.472\n",
            "[105,    50] loss: 1.469\n",
            "[105,    60] loss: 1.470\n",
            "[105,    70] loss: 1.470\n",
            "[105,    80] loss: 1.477\n",
            "[105,    90] loss: 1.468\n",
            "[105,   100] loss: 1.470\n",
            "[105,   110] loss: 1.478\n",
            "[105,   120] loss: 1.475\n",
            "[105,   130] loss: 1.472\n",
            "[105,   140] loss: 1.468\n",
            "[105,   150] loss: 1.481\n",
            "[105,   160] loss: 1.471\n",
            "[105,   170] loss: 1.476\n",
            "[105,   180] loss: 1.475\n",
            "[105,   190] loss: 1.477\n",
            "[105,   200] loss: 1.473\n",
            "[105,   210] loss: 1.471\n",
            "[105,   220] loss: 1.468\n",
            "[105,   230] loss: 1.475\n",
            "[105,   240] loss: 1.469\n",
            "[105,   250] loss: 1.470\n",
            "[105,   260] loss: 1.480\n",
            "[105,   270] loss: 1.472\n",
            "[105,   280] loss: 1.469\n",
            "[105,   290] loss: 1.472\n",
            "[105,   300] loss: 1.472\n",
            "[105,   310] loss: 1.471\n",
            "[105,   320] loss: 1.473\n",
            "[105,   330] loss: 1.472\n",
            "[105,   340] loss: 1.476\n",
            "[105,   350] loss: 1.474\n",
            "[105,   360] loss: 1.475\n",
            "[105,   370] loss: 1.474\n",
            "[105,   380] loss: 1.471\n",
            "[105,   390] loss: 1.467\n",
            "[105,   400] loss: 1.466\n",
            "[105,   410] loss: 1.472\n",
            "[105,   420] loss: 1.472\n",
            "[105,   430] loss: 1.472\n",
            "epoch 104 val_loss 61.29147207736969 val_steps 40 val_acc 0.935\n",
            "[106,    10] loss: 1.475\n",
            "[106,    20] loss: 1.469\n",
            "[106,    30] loss: 1.471\n",
            "[106,    40] loss: 1.473\n",
            "[106,    50] loss: 1.472\n",
            "[106,    60] loss: 1.476\n",
            "[106,    70] loss: 1.476\n",
            "[106,    80] loss: 1.471\n",
            "[106,    90] loss: 1.469\n",
            "[106,   100] loss: 1.475\n",
            "[106,   110] loss: 1.473\n",
            "[106,   120] loss: 1.471\n",
            "[106,   130] loss: 1.469\n",
            "[106,   140] loss: 1.473\n",
            "[106,   150] loss: 1.473\n",
            "[106,   160] loss: 1.471\n",
            "[106,   170] loss: 1.470\n",
            "[106,   180] loss: 1.473\n",
            "[106,   190] loss: 1.472\n",
            "[106,   200] loss: 1.474\n",
            "[106,   210] loss: 1.474\n",
            "[106,   220] loss: 1.468\n",
            "[106,   230] loss: 1.470\n",
            "[106,   240] loss: 1.473\n",
            "[106,   250] loss: 1.465\n",
            "[106,   260] loss: 1.472\n",
            "[106,   270] loss: 1.474\n",
            "[106,   280] loss: 1.480\n",
            "[106,   290] loss: 1.476\n",
            "[106,   300] loss: 1.474\n",
            "[106,   310] loss: 1.470\n",
            "[106,   320] loss: 1.469\n",
            "[106,   330] loss: 1.471\n",
            "[106,   340] loss: 1.472\n",
            "[106,   350] loss: 1.475\n",
            "[106,   360] loss: 1.473\n",
            "[106,   370] loss: 1.473\n",
            "[106,   380] loss: 1.472\n",
            "[106,   390] loss: 1.476\n",
            "[106,   400] loss: 1.474\n",
            "[106,   410] loss: 1.467\n",
            "[106,   420] loss: 1.475\n",
            "[106,   430] loss: 1.472\n",
            "epoch 105 val_loss 61.09832000732422 val_steps 40 val_acc 0.933\n",
            "[107,    10] loss: 1.472\n",
            "[107,    20] loss: 1.474\n",
            "[107,    30] loss: 1.477\n",
            "[107,    40] loss: 1.472\n",
            "[107,    50] loss: 1.470\n",
            "[107,    60] loss: 1.474\n",
            "[107,    70] loss: 1.474\n",
            "[107,    80] loss: 1.471\n",
            "[107,    90] loss: 1.472\n",
            "[107,   100] loss: 1.472\n",
            "[107,   110] loss: 1.474\n",
            "[107,   120] loss: 1.469\n",
            "[107,   130] loss: 1.473\n",
            "[107,   140] loss: 1.469\n",
            "[107,   150] loss: 1.470\n",
            "[107,   160] loss: 1.472\n",
            "[107,   170] loss: 1.470\n",
            "[107,   180] loss: 1.476\n",
            "[107,   190] loss: 1.470\n",
            "[107,   200] loss: 1.477\n",
            "[107,   210] loss: 1.470\n",
            "[107,   220] loss: 1.472\n",
            "[107,   230] loss: 1.474\n",
            "[107,   240] loss: 1.471\n",
            "[107,   250] loss: 1.471\n",
            "[107,   260] loss: 1.481\n",
            "[107,   270] loss: 1.471\n",
            "[107,   280] loss: 1.470\n",
            "[107,   290] loss: 1.477\n",
            "[107,   300] loss: 1.469\n",
            "[107,   310] loss: 1.473\n",
            "[107,   320] loss: 1.470\n",
            "[107,   330] loss: 1.475\n",
            "[107,   340] loss: 1.470\n",
            "[107,   350] loss: 1.468\n",
            "[107,   360] loss: 1.475\n",
            "[107,   370] loss: 1.474\n",
            "[107,   380] loss: 1.474\n",
            "[107,   390] loss: 1.471\n",
            "[107,   400] loss: 1.469\n",
            "[107,   410] loss: 1.470\n",
            "[107,   420] loss: 1.476\n",
            "[107,   430] loss: 1.470\n",
            "epoch 106 val_loss 61.393038392066956 val_steps 40 val_acc 0.9332\n",
            "[108,    10] loss: 1.471\n",
            "[108,    20] loss: 1.481\n",
            "[108,    30] loss: 1.471\n",
            "[108,    40] loss: 1.467\n",
            "[108,    50] loss: 1.480\n",
            "[108,    60] loss: 1.468\n",
            "[108,    70] loss: 1.472\n",
            "[108,    80] loss: 1.471\n",
            "[108,    90] loss: 1.472\n",
            "[108,   100] loss: 1.478\n",
            "[108,   110] loss: 1.468\n",
            "[108,   120] loss: 1.473\n",
            "[108,   130] loss: 1.473\n",
            "[108,   140] loss: 1.472\n",
            "[108,   150] loss: 1.472\n",
            "[108,   160] loss: 1.468\n",
            "[108,   170] loss: 1.468\n",
            "[108,   180] loss: 1.472\n",
            "[108,   190] loss: 1.474\n",
            "[108,   200] loss: 1.473\n",
            "[108,   210] loss: 1.473\n",
            "[108,   220] loss: 1.472\n",
            "[108,   230] loss: 1.472\n",
            "[108,   240] loss: 1.472\n",
            "[108,   250] loss: 1.471\n",
            "[108,   260] loss: 1.475\n",
            "[108,   270] loss: 1.474\n",
            "[108,   280] loss: 1.472\n",
            "[108,   290] loss: 1.471\n",
            "[108,   300] loss: 1.473\n",
            "[108,   310] loss: 1.476\n",
            "[108,   320] loss: 1.470\n",
            "[108,   330] loss: 1.470\n",
            "[108,   340] loss: 1.471\n",
            "[108,   350] loss: 1.471\n",
            "[108,   360] loss: 1.472\n",
            "[108,   370] loss: 1.473\n",
            "[108,   380] loss: 1.473\n",
            "[108,   390] loss: 1.474\n",
            "[108,   400] loss: 1.469\n",
            "[108,   410] loss: 1.472\n",
            "[108,   420] loss: 1.473\n",
            "[108,   430] loss: 1.472\n",
            "epoch 107 val_loss 61.33846437931061 val_steps 40 val_acc 0.9328\n",
            "[109,    10] loss: 1.470\n",
            "[109,    20] loss: 1.468\n",
            "[109,    30] loss: 1.470\n",
            "[109,    40] loss: 1.472\n",
            "[109,    50] loss: 1.472\n",
            "[109,    60] loss: 1.469\n",
            "[109,    70] loss: 1.471\n",
            "[109,    80] loss: 1.478\n",
            "[109,    90] loss: 1.477\n",
            "[109,   100] loss: 1.473\n",
            "[109,   110] loss: 1.473\n",
            "[109,   120] loss: 1.472\n",
            "[109,   130] loss: 1.469\n",
            "[109,   140] loss: 1.474\n",
            "[109,   150] loss: 1.476\n",
            "[109,   160] loss: 1.469\n",
            "[109,   170] loss: 1.471\n",
            "[109,   180] loss: 1.472\n",
            "[109,   190] loss: 1.470\n",
            "[109,   200] loss: 1.475\n",
            "[109,   210] loss: 1.471\n",
            "[109,   220] loss: 1.466\n",
            "[109,   230] loss: 1.469\n",
            "[109,   240] loss: 1.468\n",
            "[109,   250] loss: 1.476\n",
            "[109,   260] loss: 1.469\n",
            "[109,   270] loss: 1.470\n",
            "[109,   280] loss: 1.469\n",
            "[109,   290] loss: 1.468\n",
            "[109,   300] loss: 1.476\n",
            "[109,   310] loss: 1.475\n",
            "[109,   320] loss: 1.470\n",
            "[109,   330] loss: 1.477\n",
            "[109,   340] loss: 1.472\n",
            "[109,   350] loss: 1.475\n",
            "[109,   360] loss: 1.471\n",
            "[109,   370] loss: 1.475\n",
            "[109,   380] loss: 1.476\n",
            "[109,   390] loss: 1.473\n",
            "[109,   400] loss: 1.469\n",
            "[109,   410] loss: 1.475\n",
            "[109,   420] loss: 1.475\n",
            "[109,   430] loss: 1.470\n",
            "epoch 108 val_loss 61.07836353778839 val_steps 40 val_acc 0.9338\n",
            "[110,    10] loss: 1.474\n",
            "[110,    20] loss: 1.477\n",
            "[110,    30] loss: 1.469\n",
            "[110,    40] loss: 1.469\n",
            "[110,    50] loss: 1.472\n",
            "[110,    60] loss: 1.469\n",
            "[110,    70] loss: 1.478\n",
            "[110,    80] loss: 1.471\n",
            "[110,    90] loss: 1.474\n",
            "[110,   100] loss: 1.470\n",
            "[110,   110] loss: 1.473\n",
            "[110,   120] loss: 1.470\n",
            "[110,   130] loss: 1.475\n",
            "[110,   140] loss: 1.468\n",
            "[110,   150] loss: 1.474\n",
            "[110,   160] loss: 1.467\n",
            "[110,   170] loss: 1.471\n",
            "[110,   180] loss: 1.476\n",
            "[110,   190] loss: 1.471\n",
            "[110,   200] loss: 1.478\n",
            "[110,   210] loss: 1.472\n",
            "[110,   220] loss: 1.471\n",
            "[110,   230] loss: 1.474\n",
            "[110,   240] loss: 1.475\n",
            "[110,   250] loss: 1.474\n",
            "[110,   260] loss: 1.474\n",
            "[110,   270] loss: 1.469\n",
            "[110,   280] loss: 1.470\n",
            "[110,   290] loss: 1.471\n",
            "[110,   300] loss: 1.473\n",
            "[110,   310] loss: 1.472\n",
            "[110,   320] loss: 1.468\n",
            "[110,   330] loss: 1.465\n",
            "[110,   340] loss: 1.476\n",
            "[110,   350] loss: 1.469\n",
            "[110,   360] loss: 1.470\n",
            "[110,   370] loss: 1.471\n",
            "[110,   380] loss: 1.476\n",
            "[110,   390] loss: 1.474\n",
            "[110,   400] loss: 1.473\n",
            "[110,   410] loss: 1.466\n",
            "[110,   420] loss: 1.468\n",
            "[110,   430] loss: 1.478\n",
            "epoch 109 val_loss 61.09064841270447 val_steps 40 val_acc 0.9334\n",
            "[111,    10] loss: 1.473\n",
            "[111,    20] loss: 1.473\n",
            "[111,    30] loss: 1.473\n",
            "[111,    40] loss: 1.473\n",
            "[111,    50] loss: 1.476\n",
            "[111,    60] loss: 1.471\n",
            "[111,    70] loss: 1.472\n",
            "[111,    80] loss: 1.468\n",
            "[111,    90] loss: 1.476\n",
            "[111,   100] loss: 1.473\n",
            "[111,   110] loss: 1.475\n",
            "[111,   120] loss: 1.474\n",
            "[111,   130] loss: 1.474\n",
            "[111,   140] loss: 1.472\n",
            "[111,   150] loss: 1.471\n",
            "[111,   160] loss: 1.473\n",
            "[111,   170] loss: 1.477\n",
            "[111,   180] loss: 1.471\n",
            "[111,   190] loss: 1.468\n",
            "[111,   200] loss: 1.474\n",
            "[111,   210] loss: 1.476\n",
            "[111,   220] loss: 1.474\n",
            "[111,   230] loss: 1.474\n",
            "[111,   240] loss: 1.470\n",
            "[111,   250] loss: 1.467\n",
            "[111,   260] loss: 1.471\n",
            "[111,   270] loss: 1.471\n",
            "[111,   280] loss: 1.470\n",
            "[111,   290] loss: 1.472\n",
            "[111,   300] loss: 1.469\n",
            "[111,   310] loss: 1.475\n",
            "[111,   320] loss: 1.468\n",
            "[111,   330] loss: 1.471\n",
            "[111,   340] loss: 1.474\n",
            "[111,   350] loss: 1.470\n",
            "[111,   360] loss: 1.470\n",
            "[111,   370] loss: 1.473\n",
            "[111,   380] loss: 1.473\n",
            "[111,   390] loss: 1.467\n",
            "[111,   400] loss: 1.468\n",
            "[111,   410] loss: 1.470\n",
            "[111,   420] loss: 1.473\n",
            "[111,   430] loss: 1.469\n",
            "epoch 110 val_loss 61.123172760009766 val_steps 40 val_acc 0.9338\n",
            "[112,    10] loss: 1.476\n",
            "[112,    20] loss: 1.470\n",
            "[112,    30] loss: 1.469\n",
            "[112,    40] loss: 1.473\n",
            "[112,    50] loss: 1.472\n",
            "[112,    60] loss: 1.472\n",
            "[112,    70] loss: 1.475\n",
            "[112,    80] loss: 1.479\n",
            "[112,    90] loss: 1.472\n",
            "[112,   100] loss: 1.476\n",
            "[112,   110] loss: 1.473\n",
            "[112,   120] loss: 1.468\n",
            "[112,   130] loss: 1.469\n",
            "[112,   140] loss: 1.471\n",
            "[112,   150] loss: 1.468\n",
            "[112,   160] loss: 1.469\n",
            "[112,   170] loss: 1.477\n",
            "[112,   180] loss: 1.474\n",
            "[112,   190] loss: 1.470\n",
            "[112,   200] loss: 1.474\n",
            "[112,   210] loss: 1.471\n",
            "[112,   220] loss: 1.471\n",
            "[112,   230] loss: 1.471\n",
            "[112,   240] loss: 1.472\n",
            "[112,   250] loss: 1.470\n",
            "[112,   260] loss: 1.467\n",
            "[112,   270] loss: 1.473\n",
            "[112,   280] loss: 1.471\n",
            "[112,   290] loss: 1.473\n",
            "[112,   300] loss: 1.468\n",
            "[112,   310] loss: 1.476\n",
            "[112,   320] loss: 1.471\n",
            "[112,   330] loss: 1.478\n",
            "[112,   340] loss: 1.467\n",
            "[112,   350] loss: 1.469\n",
            "[112,   360] loss: 1.476\n",
            "[112,   370] loss: 1.469\n",
            "[112,   380] loss: 1.471\n",
            "[112,   390] loss: 1.471\n",
            "[112,   400] loss: 1.475\n",
            "[112,   410] loss: 1.472\n",
            "[112,   420] loss: 1.470\n",
            "[112,   430] loss: 1.469\n",
            "epoch 111 val_loss 61.08417534828186 val_steps 40 val_acc 0.9346\n",
            "[113,    10] loss: 1.473\n",
            "[113,    20] loss: 1.472\n",
            "[113,    30] loss: 1.473\n",
            "[113,    40] loss: 1.474\n",
            "[113,    50] loss: 1.476\n",
            "[113,    60] loss: 1.474\n",
            "[113,    70] loss: 1.472\n",
            "[113,    80] loss: 1.474\n",
            "[113,    90] loss: 1.467\n",
            "[113,   100] loss: 1.471\n",
            "[113,   110] loss: 1.473\n",
            "[113,   120] loss: 1.470\n",
            "[113,   130] loss: 1.474\n",
            "[113,   140] loss: 1.474\n",
            "[113,   150] loss: 1.474\n",
            "[113,   160] loss: 1.471\n",
            "[113,   170] loss: 1.467\n",
            "[113,   180] loss: 1.472\n",
            "[113,   190] loss: 1.470\n",
            "[113,   200] loss: 1.471\n",
            "[113,   210] loss: 1.472\n",
            "[113,   220] loss: 1.473\n",
            "[113,   230] loss: 1.473\n",
            "[113,   240] loss: 1.470\n",
            "[113,   250] loss: 1.474\n",
            "[113,   260] loss: 1.469\n",
            "[113,   270] loss: 1.471\n",
            "[113,   280] loss: 1.470\n",
            "[113,   290] loss: 1.472\n",
            "[113,   300] loss: 1.469\n",
            "[113,   310] loss: 1.469\n",
            "[113,   320] loss: 1.472\n",
            "[113,   330] loss: 1.473\n",
            "[113,   340] loss: 1.476\n",
            "[113,   350] loss: 1.472\n",
            "[113,   360] loss: 1.470\n",
            "[113,   370] loss: 1.473\n",
            "[113,   380] loss: 1.470\n",
            "[113,   390] loss: 1.473\n",
            "[113,   400] loss: 1.472\n",
            "[113,   410] loss: 1.473\n",
            "[113,   420] loss: 1.467\n",
            "[113,   430] loss: 1.468\n",
            "epoch 112 val_loss 61.2117133140564 val_steps 40 val_acc 0.934\n",
            "[114,    10] loss: 1.468\n",
            "[114,    20] loss: 1.469\n",
            "[114,    30] loss: 1.473\n",
            "[114,    40] loss: 1.470\n",
            "[114,    50] loss: 1.468\n",
            "[114,    60] loss: 1.475\n",
            "[114,    70] loss: 1.472\n",
            "[114,    80] loss: 1.474\n",
            "[114,    90] loss: 1.478\n",
            "[114,   100] loss: 1.472\n",
            "[114,   110] loss: 1.471\n",
            "[114,   120] loss: 1.472\n",
            "[114,   130] loss: 1.474\n",
            "[114,   140] loss: 1.471\n",
            "[114,   150] loss: 1.471\n",
            "[114,   160] loss: 1.473\n",
            "[114,   170] loss: 1.469\n",
            "[114,   180] loss: 1.469\n",
            "[114,   190] loss: 1.472\n",
            "[114,   200] loss: 1.478\n",
            "[114,   210] loss: 1.476\n",
            "[114,   220] loss: 1.473\n",
            "[114,   230] loss: 1.471\n",
            "[114,   240] loss: 1.471\n",
            "[114,   250] loss: 1.468\n",
            "[114,   260] loss: 1.467\n",
            "[114,   270] loss: 1.466\n",
            "[114,   280] loss: 1.472\n",
            "[114,   290] loss: 1.470\n",
            "[114,   300] loss: 1.476\n",
            "[114,   310] loss: 1.468\n",
            "[114,   320] loss: 1.476\n",
            "[114,   330] loss: 1.472\n",
            "[114,   340] loss: 1.472\n",
            "[114,   350] loss: 1.474\n",
            "[114,   360] loss: 1.475\n",
            "[114,   370] loss: 1.478\n",
            "[114,   380] loss: 1.471\n",
            "[114,   390] loss: 1.470\n",
            "[114,   400] loss: 1.472\n",
            "[114,   410] loss: 1.470\n",
            "[114,   420] loss: 1.468\n",
            "[114,   430] loss: 1.470\n",
            "epoch 113 val_loss 61.02314043045044 val_steps 40 val_acc 0.9352\n",
            "[115,    10] loss: 1.471\n",
            "[115,    20] loss: 1.468\n",
            "[115,    30] loss: 1.472\n",
            "[115,    40] loss: 1.476\n",
            "[115,    50] loss: 1.470\n",
            "[115,    60] loss: 1.472\n",
            "[115,    70] loss: 1.473\n",
            "[115,    80] loss: 1.471\n",
            "[115,    90] loss: 1.472\n",
            "[115,   100] loss: 1.469\n",
            "[115,   110] loss: 1.471\n",
            "[115,   120] loss: 1.470\n",
            "[115,   130] loss: 1.469\n",
            "[115,   140] loss: 1.473\n",
            "[115,   150] loss: 1.469\n",
            "[115,   160] loss: 1.475\n",
            "[115,   170] loss: 1.472\n",
            "[115,   180] loss: 1.474\n",
            "[115,   190] loss: 1.469\n",
            "[115,   200] loss: 1.473\n",
            "[115,   210] loss: 1.472\n",
            "[115,   220] loss: 1.478\n",
            "[115,   230] loss: 1.476\n",
            "[115,   240] loss: 1.470\n",
            "[115,   250] loss: 1.472\n",
            "[115,   260] loss: 1.473\n",
            "[115,   270] loss: 1.469\n",
            "[115,   280] loss: 1.474\n",
            "[115,   290] loss: 1.472\n",
            "[115,   300] loss: 1.471\n",
            "[115,   310] loss: 1.471\n",
            "[115,   320] loss: 1.470\n",
            "[115,   330] loss: 1.474\n",
            "[115,   340] loss: 1.477\n",
            "[115,   350] loss: 1.474\n",
            "[115,   360] loss: 1.469\n",
            "[115,   370] loss: 1.472\n",
            "[115,   380] loss: 1.471\n",
            "[115,   390] loss: 1.469\n",
            "[115,   400] loss: 1.471\n",
            "[115,   410] loss: 1.473\n",
            "[115,   420] loss: 1.467\n",
            "[115,   430] loss: 1.472\n",
            "epoch 114 val_loss 61.17537486553192 val_steps 40 val_acc 0.9324\n",
            "[116,    10] loss: 1.475\n",
            "[116,    20] loss: 1.471\n",
            "[116,    30] loss: 1.476\n",
            "[116,    40] loss: 1.472\n",
            "[116,    50] loss: 1.468\n",
            "[116,    60] loss: 1.473\n",
            "[116,    70] loss: 1.478\n",
            "[116,    80] loss: 1.474\n",
            "[116,    90] loss: 1.468\n",
            "[116,   100] loss: 1.472\n",
            "[116,   110] loss: 1.473\n",
            "[116,   120] loss: 1.471\n",
            "[116,   130] loss: 1.473\n",
            "[116,   140] loss: 1.474\n",
            "[116,   150] loss: 1.475\n",
            "[116,   160] loss: 1.471\n",
            "[116,   170] loss: 1.476\n",
            "[116,   180] loss: 1.468\n",
            "[116,   190] loss: 1.474\n",
            "[116,   200] loss: 1.472\n",
            "[116,   210] loss: 1.467\n",
            "[116,   220] loss: 1.465\n",
            "[116,   230] loss: 1.470\n",
            "[116,   240] loss: 1.474\n",
            "[116,   250] loss: 1.468\n",
            "[116,   260] loss: 1.471\n",
            "[116,   270] loss: 1.477\n",
            "[116,   280] loss: 1.476\n",
            "[116,   290] loss: 1.469\n",
            "[116,   300] loss: 1.472\n",
            "[116,   310] loss: 1.471\n",
            "[116,   320] loss: 1.470\n",
            "[116,   330] loss: 1.471\n",
            "[116,   340] loss: 1.474\n",
            "[116,   350] loss: 1.470\n",
            "[116,   360] loss: 1.467\n",
            "[116,   370] loss: 1.473\n",
            "[116,   380] loss: 1.472\n",
            "[116,   390] loss: 1.468\n",
            "[116,   400] loss: 1.472\n",
            "[116,   410] loss: 1.471\n",
            "[116,   420] loss: 1.473\n",
            "[116,   430] loss: 1.467\n",
            "epoch 115 val_loss 61.10573422908783 val_steps 40 val_acc 0.934\n",
            "[117,    10] loss: 1.472\n",
            "[117,    20] loss: 1.473\n",
            "[117,    30] loss: 1.475\n",
            "[117,    40] loss: 1.472\n",
            "[117,    50] loss: 1.469\n",
            "[117,    60] loss: 1.472\n",
            "[117,    70] loss: 1.472\n",
            "[117,    80] loss: 1.471\n",
            "[117,    90] loss: 1.473\n",
            "[117,   100] loss: 1.471\n",
            "[117,   110] loss: 1.469\n",
            "[117,   120] loss: 1.469\n",
            "[117,   130] loss: 1.471\n",
            "[117,   140] loss: 1.467\n",
            "[117,   150] loss: 1.474\n",
            "[117,   160] loss: 1.474\n",
            "[117,   170] loss: 1.474\n",
            "[117,   180] loss: 1.475\n",
            "[117,   190] loss: 1.473\n",
            "[117,   200] loss: 1.472\n",
            "[117,   210] loss: 1.468\n",
            "[117,   220] loss: 1.467\n",
            "[117,   230] loss: 1.474\n",
            "[117,   240] loss: 1.466\n",
            "[117,   250] loss: 1.471\n",
            "[117,   260] loss: 1.474\n",
            "[117,   270] loss: 1.473\n",
            "[117,   280] loss: 1.467\n",
            "[117,   290] loss: 1.474\n",
            "[117,   300] loss: 1.475\n",
            "[117,   310] loss: 1.469\n",
            "[117,   320] loss: 1.475\n",
            "[117,   330] loss: 1.472\n",
            "[117,   340] loss: 1.470\n",
            "[117,   350] loss: 1.469\n",
            "[117,   360] loss: 1.477\n",
            "[117,   370] loss: 1.473\n",
            "[117,   380] loss: 1.470\n",
            "[117,   390] loss: 1.478\n",
            "[117,   400] loss: 1.474\n",
            "[117,   410] loss: 1.469\n",
            "[117,   420] loss: 1.473\n",
            "[117,   430] loss: 1.467\n",
            "epoch 116 val_loss 61.14282441139221 val_steps 40 val_acc 0.9308\n",
            "[118,    10] loss: 1.466\n",
            "[118,    20] loss: 1.470\n",
            "[118,    30] loss: 1.470\n",
            "[118,    40] loss: 1.477\n",
            "[118,    50] loss: 1.474\n",
            "[118,    60] loss: 1.473\n",
            "[118,    70] loss: 1.471\n",
            "[118,    80] loss: 1.473\n",
            "[118,    90] loss: 1.472\n",
            "[118,   100] loss: 1.475\n",
            "[118,   110] loss: 1.473\n",
            "[118,   120] loss: 1.468\n",
            "[118,   130] loss: 1.476\n",
            "[118,   140] loss: 1.465\n",
            "[118,   150] loss: 1.474\n",
            "[118,   160] loss: 1.473\n",
            "[118,   170] loss: 1.470\n",
            "[118,   180] loss: 1.477\n",
            "[118,   190] loss: 1.466\n",
            "[118,   200] loss: 1.471\n",
            "[118,   210] loss: 1.470\n",
            "[118,   220] loss: 1.476\n",
            "[118,   230] loss: 1.472\n",
            "[118,   240] loss: 1.475\n",
            "[118,   250] loss: 1.469\n",
            "[118,   260] loss: 1.471\n",
            "[118,   270] loss: 1.471\n",
            "[118,   280] loss: 1.471\n",
            "[118,   290] loss: 1.473\n",
            "[118,   300] loss: 1.471\n",
            "[118,   310] loss: 1.470\n",
            "[118,   320] loss: 1.469\n",
            "[118,   330] loss: 1.472\n",
            "[118,   340] loss: 1.472\n",
            "[118,   350] loss: 1.471\n",
            "[118,   360] loss: 1.472\n",
            "[118,   370] loss: 1.473\n",
            "[118,   380] loss: 1.473\n",
            "[118,   390] loss: 1.469\n",
            "[118,   400] loss: 1.472\n",
            "[118,   410] loss: 1.474\n",
            "[118,   420] loss: 1.470\n",
            "[118,   430] loss: 1.468\n",
            "epoch 117 val_loss 61.11333107948303 val_steps 40 val_acc 0.9334\n",
            "[119,    10] loss: 1.471\n",
            "[119,    20] loss: 1.467\n",
            "[119,    30] loss: 1.476\n",
            "[119,    40] loss: 1.472\n",
            "[119,    50] loss: 1.472\n",
            "[119,    60] loss: 1.475\n",
            "[119,    70] loss: 1.476\n",
            "[119,    80] loss: 1.472\n",
            "[119,    90] loss: 1.475\n",
            "[119,   100] loss: 1.472\n",
            "[119,   110] loss: 1.473\n",
            "[119,   120] loss: 1.469\n",
            "[119,   130] loss: 1.472\n",
            "[119,   140] loss: 1.473\n",
            "[119,   150] loss: 1.476\n",
            "[119,   160] loss: 1.471\n",
            "[119,   170] loss: 1.471\n",
            "[119,   180] loss: 1.466\n",
            "[119,   190] loss: 1.474\n",
            "[119,   200] loss: 1.473\n",
            "[119,   210] loss: 1.471\n",
            "[119,   220] loss: 1.472\n",
            "[119,   230] loss: 1.469\n",
            "[119,   240] loss: 1.471\n",
            "[119,   250] loss: 1.469\n",
            "[119,   260] loss: 1.465\n",
            "[119,   270] loss: 1.474\n",
            "[119,   280] loss: 1.472\n",
            "[119,   290] loss: 1.471\n",
            "[119,   300] loss: 1.474\n",
            "[119,   310] loss: 1.474\n",
            "[119,   320] loss: 1.469\n",
            "[119,   330] loss: 1.471\n",
            "[119,   340] loss: 1.470\n",
            "[119,   350] loss: 1.472\n",
            "[119,   360] loss: 1.475\n",
            "[119,   370] loss: 1.469\n",
            "[119,   380] loss: 1.470\n",
            "[119,   390] loss: 1.471\n",
            "[119,   400] loss: 1.466\n",
            "[119,   410] loss: 1.473\n",
            "[119,   420] loss: 1.470\n",
            "[119,   430] loss: 1.477\n",
            "epoch 118 val_loss 61.3040429353714 val_steps 40 val_acc 0.9334\n",
            "[120,    10] loss: 1.472\n",
            "[120,    20] loss: 1.472\n",
            "[120,    30] loss: 1.469\n",
            "[120,    40] loss: 1.471\n",
            "[120,    50] loss: 1.472\n",
            "[120,    60] loss: 1.471\n",
            "[120,    70] loss: 1.471\n",
            "[120,    80] loss: 1.470\n",
            "[120,    90] loss: 1.472\n",
            "[120,   100] loss: 1.471\n",
            "[120,   110] loss: 1.473\n",
            "[120,   120] loss: 1.468\n",
            "[120,   130] loss: 1.472\n",
            "[120,   140] loss: 1.472\n",
            "[120,   150] loss: 1.469\n",
            "[120,   160] loss: 1.473\n",
            "[120,   170] loss: 1.468\n",
            "[120,   180] loss: 1.472\n",
            "[120,   190] loss: 1.474\n",
            "[120,   200] loss: 1.470\n",
            "[120,   210] loss: 1.472\n",
            "[120,   220] loss: 1.477\n",
            "[120,   230] loss: 1.472\n",
            "[120,   240] loss: 1.472\n",
            "[120,   250] loss: 1.472\n",
            "[120,   260] loss: 1.475\n",
            "[120,   270] loss: 1.469\n",
            "[120,   280] loss: 1.474\n",
            "[120,   290] loss: 1.468\n",
            "[120,   300] loss: 1.467\n",
            "[120,   310] loss: 1.474\n",
            "[120,   320] loss: 1.474\n",
            "[120,   330] loss: 1.475\n",
            "[120,   340] loss: 1.476\n",
            "[120,   350] loss: 1.473\n",
            "[120,   360] loss: 1.466\n",
            "[120,   370] loss: 1.465\n",
            "[120,   380] loss: 1.472\n",
            "[120,   390] loss: 1.474\n",
            "[120,   400] loss: 1.474\n",
            "[120,   410] loss: 1.473\n",
            "[120,   420] loss: 1.469\n",
            "[120,   430] loss: 1.474\n",
            "epoch 119 val_loss 61.20758295059204 val_steps 40 val_acc 0.9358\n",
            "[121,    10] loss: 1.476\n",
            "[121,    20] loss: 1.468\n",
            "[121,    30] loss: 1.472\n",
            "[121,    40] loss: 1.467\n",
            "[121,    50] loss: 1.476\n",
            "[121,    60] loss: 1.468\n",
            "[121,    70] loss: 1.469\n",
            "[121,    80] loss: 1.479\n",
            "[121,    90] loss: 1.469\n",
            "[121,   100] loss: 1.471\n",
            "[121,   110] loss: 1.467\n",
            "[121,   120] loss: 1.466\n",
            "[121,   130] loss: 1.468\n",
            "[121,   140] loss: 1.477\n",
            "[121,   150] loss: 1.471\n",
            "[121,   160] loss: 1.474\n",
            "[121,   170] loss: 1.470\n",
            "[121,   180] loss: 1.472\n",
            "[121,   190] loss: 1.473\n",
            "[121,   200] loss: 1.472\n",
            "[121,   210] loss: 1.470\n",
            "[121,   220] loss: 1.470\n",
            "[121,   230] loss: 1.473\n",
            "[121,   240] loss: 1.476\n",
            "[121,   250] loss: 1.477\n",
            "[121,   260] loss: 1.473\n",
            "[121,   270] loss: 1.476\n",
            "[121,   280] loss: 1.470\n",
            "[121,   290] loss: 1.471\n",
            "[121,   300] loss: 1.471\n",
            "[121,   310] loss: 1.475\n",
            "[121,   320] loss: 1.473\n",
            "[121,   330] loss: 1.470\n",
            "[121,   340] loss: 1.463\n",
            "[121,   350] loss: 1.473\n",
            "[121,   360] loss: 1.476\n",
            "[121,   370] loss: 1.475\n",
            "[121,   380] loss: 1.472\n",
            "[121,   390] loss: 1.470\n",
            "[121,   400] loss: 1.468\n",
            "[121,   410] loss: 1.475\n",
            "[121,   420] loss: 1.469\n",
            "[121,   430] loss: 1.470\n",
            "epoch 120 val_loss 61.234543681144714 val_steps 40 val_acc 0.9346\n",
            "[122,    10] loss: 1.472\n",
            "[122,    20] loss: 1.468\n",
            "[122,    30] loss: 1.470\n",
            "[122,    40] loss: 1.466\n",
            "[122,    50] loss: 1.470\n",
            "[122,    60] loss: 1.476\n",
            "[122,    70] loss: 1.472\n",
            "[122,    80] loss: 1.472\n",
            "[122,    90] loss: 1.476\n",
            "[122,   100] loss: 1.475\n",
            "[122,   110] loss: 1.470\n",
            "[122,   120] loss: 1.470\n",
            "[122,   130] loss: 1.475\n",
            "[122,   140] loss: 1.473\n",
            "[122,   150] loss: 1.471\n",
            "[122,   160] loss: 1.472\n",
            "[122,   170] loss: 1.473\n",
            "[122,   180] loss: 1.470\n",
            "[122,   190] loss: 1.472\n",
            "[122,   200] loss: 1.470\n",
            "[122,   210] loss: 1.474\n",
            "[122,   220] loss: 1.470\n",
            "[122,   230] loss: 1.467\n",
            "[122,   240] loss: 1.472\n",
            "[122,   250] loss: 1.472\n",
            "[122,   260] loss: 1.471\n",
            "[122,   270] loss: 1.470\n",
            "[122,   280] loss: 1.469\n",
            "[122,   290] loss: 1.472\n",
            "[122,   300] loss: 1.472\n",
            "[122,   310] loss: 1.472\n",
            "[122,   320] loss: 1.474\n",
            "[122,   330] loss: 1.473\n",
            "[122,   340] loss: 1.467\n",
            "[122,   350] loss: 1.472\n",
            "[122,   360] loss: 1.471\n",
            "[122,   370] loss: 1.470\n",
            "[122,   380] loss: 1.475\n",
            "[122,   390] loss: 1.476\n",
            "[122,   400] loss: 1.472\n",
            "[122,   410] loss: 1.470\n",
            "[122,   420] loss: 1.474\n",
            "[122,   430] loss: 1.474\n",
            "epoch 121 val_loss 61.24777126312256 val_steps 40 val_acc 0.9332\n",
            "[123,    10] loss: 1.474\n",
            "[123,    20] loss: 1.472\n",
            "[123,    30] loss: 1.474\n",
            "[123,    40] loss: 1.472\n",
            "[123,    50] loss: 1.472\n",
            "[123,    60] loss: 1.471\n",
            "[123,    70] loss: 1.470\n",
            "[123,    80] loss: 1.470\n",
            "[123,    90] loss: 1.469\n",
            "[123,   100] loss: 1.471\n",
            "[123,   110] loss: 1.468\n",
            "[123,   120] loss: 1.468\n",
            "[123,   130] loss: 1.474\n",
            "[123,   140] loss: 1.470\n",
            "[123,   150] loss: 1.475\n",
            "[123,   160] loss: 1.475\n",
            "[123,   170] loss: 1.472\n",
            "[123,   180] loss: 1.469\n",
            "[123,   190] loss: 1.471\n",
            "[123,   200] loss: 1.468\n",
            "[123,   210] loss: 1.467\n",
            "[123,   220] loss: 1.471\n",
            "[123,   230] loss: 1.474\n",
            "[123,   240] loss: 1.476\n",
            "[123,   250] loss: 1.473\n",
            "[123,   260] loss: 1.476\n",
            "[123,   270] loss: 1.470\n",
            "[123,   280] loss: 1.474\n",
            "[123,   290] loss: 1.471\n",
            "[123,   300] loss: 1.473\n",
            "[123,   310] loss: 1.469\n",
            "[123,   320] loss: 1.476\n",
            "[123,   330] loss: 1.471\n",
            "[123,   340] loss: 1.472\n",
            "[123,   350] loss: 1.480\n",
            "[123,   360] loss: 1.467\n",
            "[123,   370] loss: 1.469\n",
            "[123,   380] loss: 1.467\n",
            "[123,   390] loss: 1.469\n",
            "[123,   400] loss: 1.473\n",
            "[123,   410] loss: 1.474\n",
            "[123,   420] loss: 1.470\n",
            "[123,   430] loss: 1.472\n",
            "epoch 122 val_loss 61.24945950508118 val_steps 40 val_acc 0.9318\n",
            "[124,    10] loss: 1.469\n",
            "[124,    20] loss: 1.471\n",
            "[124,    30] loss: 1.471\n",
            "[124,    40] loss: 1.474\n",
            "[124,    50] loss: 1.466\n",
            "[124,    60] loss: 1.471\n",
            "[124,    70] loss: 1.471\n",
            "[124,    80] loss: 1.471\n",
            "[124,    90] loss: 1.473\n",
            "[124,   100] loss: 1.473\n",
            "[124,   110] loss: 1.479\n",
            "[124,   120] loss: 1.472\n",
            "[124,   130] loss: 1.471\n",
            "[124,   140] loss: 1.473\n",
            "[124,   150] loss: 1.471\n",
            "[124,   160] loss: 1.475\n",
            "[124,   170] loss: 1.475\n",
            "[124,   180] loss: 1.472\n",
            "[124,   190] loss: 1.476\n",
            "[124,   200] loss: 1.475\n",
            "[124,   210] loss: 1.471\n",
            "[124,   220] loss: 1.474\n",
            "[124,   230] loss: 1.475\n",
            "[124,   240] loss: 1.473\n",
            "[124,   250] loss: 1.467\n",
            "[124,   260] loss: 1.470\n",
            "[124,   270] loss: 1.472\n",
            "[124,   280] loss: 1.470\n",
            "[124,   290] loss: 1.470\n",
            "[124,   300] loss: 1.472\n",
            "[124,   310] loss: 1.470\n",
            "[124,   320] loss: 1.469\n",
            "[124,   330] loss: 1.472\n",
            "[124,   340] loss: 1.469\n",
            "[124,   350] loss: 1.472\n",
            "[124,   360] loss: 1.468\n",
            "[124,   370] loss: 1.471\n",
            "[124,   380] loss: 1.471\n",
            "[124,   390] loss: 1.475\n",
            "[124,   400] loss: 1.470\n",
            "[124,   410] loss: 1.470\n",
            "[124,   420] loss: 1.472\n",
            "[124,   430] loss: 1.468\n",
            "epoch 123 val_loss 61.330652832984924 val_steps 40 val_acc 0.9318\n",
            "[125,    10] loss: 1.473\n",
            "[125,    20] loss: 1.473\n",
            "[125,    30] loss: 1.475\n",
            "[125,    40] loss: 1.470\n",
            "[125,    50] loss: 1.471\n",
            "[125,    60] loss: 1.472\n",
            "[125,    70] loss: 1.475\n",
            "[125,    80] loss: 1.471\n",
            "[125,    90] loss: 1.473\n",
            "[125,   100] loss: 1.473\n",
            "[125,   110] loss: 1.472\n",
            "[125,   120] loss: 1.475\n",
            "[125,   130] loss: 1.473\n",
            "[125,   140] loss: 1.470\n",
            "[125,   150] loss: 1.474\n",
            "[125,   160] loss: 1.472\n",
            "[125,   170] loss: 1.472\n",
            "[125,   180] loss: 1.472\n",
            "[125,   190] loss: 1.471\n",
            "[125,   200] loss: 1.465\n",
            "[125,   210] loss: 1.473\n",
            "[125,   220] loss: 1.469\n",
            "[125,   230] loss: 1.474\n",
            "[125,   240] loss: 1.471\n",
            "[125,   250] loss: 1.475\n",
            "[125,   260] loss: 1.471\n",
            "[125,   270] loss: 1.469\n",
            "[125,   280] loss: 1.472\n",
            "[125,   290] loss: 1.475\n",
            "[125,   300] loss: 1.469\n",
            "[125,   310] loss: 1.470\n",
            "[125,   320] loss: 1.472\n",
            "[125,   330] loss: 1.467\n",
            "[125,   340] loss: 1.471\n",
            "[125,   350] loss: 1.471\n",
            "[125,   360] loss: 1.476\n",
            "[125,   370] loss: 1.474\n",
            "[125,   380] loss: 1.468\n",
            "[125,   390] loss: 1.472\n",
            "[125,   400] loss: 1.472\n",
            "[125,   410] loss: 1.472\n",
            "[125,   420] loss: 1.465\n",
            "[125,   430] loss: 1.470\n",
            "epoch 124 val_loss 61.1068856716156 val_steps 40 val_acc 0.9334\n",
            "[126,    10] loss: 1.473\n",
            "[126,    20] loss: 1.473\n",
            "[126,    30] loss: 1.475\n",
            "[126,    40] loss: 1.474\n",
            "[126,    50] loss: 1.475\n",
            "[126,    60] loss: 1.470\n",
            "[126,    70] loss: 1.473\n",
            "[126,    80] loss: 1.476\n",
            "[126,    90] loss: 1.467\n",
            "[126,   100] loss: 1.471\n",
            "[126,   110] loss: 1.473\n",
            "[126,   120] loss: 1.469\n",
            "[126,   130] loss: 1.472\n",
            "[126,   140] loss: 1.477\n",
            "[126,   150] loss: 1.473\n",
            "[126,   160] loss: 1.472\n",
            "[126,   170] loss: 1.471\n",
            "[126,   180] loss: 1.469\n",
            "[126,   190] loss: 1.474\n",
            "[126,   200] loss: 1.470\n",
            "[126,   210] loss: 1.472\n",
            "[126,   220] loss: 1.472\n",
            "[126,   230] loss: 1.475\n",
            "[126,   240] loss: 1.473\n",
            "[126,   250] loss: 1.468\n",
            "[126,   260] loss: 1.470\n",
            "[126,   270] loss: 1.467\n",
            "[126,   280] loss: 1.466\n",
            "[126,   290] loss: 1.468\n",
            "[126,   300] loss: 1.471\n",
            "[126,   310] loss: 1.474\n",
            "[126,   320] loss: 1.469\n",
            "[126,   330] loss: 1.467\n",
            "[126,   340] loss: 1.471\n",
            "[126,   350] loss: 1.475\n",
            "[126,   360] loss: 1.470\n",
            "[126,   370] loss: 1.470\n",
            "[126,   380] loss: 1.476\n",
            "[126,   390] loss: 1.470\n",
            "[126,   400] loss: 1.470\n",
            "[126,   410] loss: 1.471\n",
            "[126,   420] loss: 1.475\n",
            "[126,   430] loss: 1.469\n",
            "epoch 125 val_loss 61.24692785739899 val_steps 40 val_acc 0.9322\n",
            "[127,    10] loss: 1.474\n",
            "[127,    20] loss: 1.471\n",
            "[127,    30] loss: 1.476\n",
            "[127,    40] loss: 1.469\n",
            "[127,    50] loss: 1.475\n",
            "[127,    60] loss: 1.469\n",
            "[127,    70] loss: 1.470\n",
            "[127,    80] loss: 1.468\n",
            "[127,    90] loss: 1.471\n",
            "[127,   100] loss: 1.472\n",
            "[127,   110] loss: 1.474\n",
            "[127,   120] loss: 1.474\n",
            "[127,   130] loss: 1.466\n",
            "[127,   140] loss: 1.474\n",
            "[127,   150] loss: 1.469\n",
            "[127,   160] loss: 1.469\n",
            "[127,   170] loss: 1.470\n",
            "[127,   180] loss: 1.475\n",
            "[127,   190] loss: 1.472\n",
            "[127,   200] loss: 1.469\n",
            "[127,   210] loss: 1.476\n",
            "[127,   220] loss: 1.473\n",
            "[127,   230] loss: 1.476\n",
            "[127,   240] loss: 1.472\n",
            "[127,   250] loss: 1.472\n",
            "[127,   260] loss: 1.469\n",
            "[127,   270] loss: 1.469\n",
            "[127,   280] loss: 1.470\n",
            "[127,   290] loss: 1.472\n",
            "[127,   300] loss: 1.469\n",
            "[127,   310] loss: 1.468\n",
            "[127,   320] loss: 1.469\n",
            "[127,   330] loss: 1.473\n",
            "[127,   340] loss: 1.469\n",
            "[127,   350] loss: 1.473\n",
            "[127,   360] loss: 1.471\n",
            "[127,   370] loss: 1.476\n",
            "[127,   380] loss: 1.472\n",
            "[127,   390] loss: 1.468\n",
            "[127,   400] loss: 1.475\n",
            "[127,   410] loss: 1.474\n",
            "[127,   420] loss: 1.475\n",
            "[127,   430] loss: 1.472\n",
            "epoch 126 val_loss 61.135894536972046 val_steps 40 val_acc 0.9348\n",
            "[128,    10] loss: 1.467\n",
            "[128,    20] loss: 1.471\n",
            "[128,    30] loss: 1.473\n",
            "[128,    40] loss: 1.470\n",
            "[128,    50] loss: 1.471\n",
            "[128,    60] loss: 1.468\n",
            "[128,    70] loss: 1.468\n",
            "[128,    80] loss: 1.475\n",
            "[128,    90] loss: 1.472\n",
            "[128,   100] loss: 1.471\n",
            "[128,   110] loss: 1.469\n",
            "[128,   120] loss: 1.470\n",
            "[128,   130] loss: 1.475\n",
            "[128,   140] loss: 1.474\n",
            "[128,   150] loss: 1.469\n",
            "[128,   160] loss: 1.472\n",
            "[128,   170] loss: 1.471\n",
            "[128,   180] loss: 1.467\n",
            "[128,   190] loss: 1.474\n",
            "[128,   200] loss: 1.474\n",
            "[128,   210] loss: 1.476\n",
            "[128,   220] loss: 1.471\n",
            "[128,   230] loss: 1.466\n",
            "[128,   240] loss: 1.473\n",
            "[128,   250] loss: 1.468\n",
            "[128,   260] loss: 1.471\n",
            "[128,   270] loss: 1.476\n",
            "[128,   280] loss: 1.468\n",
            "[128,   290] loss: 1.470\n",
            "[128,   300] loss: 1.469\n",
            "[128,   310] loss: 1.473\n",
            "[128,   320] loss: 1.472\n",
            "[128,   330] loss: 1.475\n",
            "[128,   340] loss: 1.469\n",
            "[128,   350] loss: 1.472\n",
            "[128,   360] loss: 1.471\n",
            "[128,   370] loss: 1.472\n",
            "[128,   380] loss: 1.472\n",
            "[128,   390] loss: 1.476\n",
            "[128,   400] loss: 1.476\n",
            "[128,   410] loss: 1.471\n",
            "[128,   420] loss: 1.472\n",
            "[128,   430] loss: 1.472\n",
            "epoch 127 val_loss 61.26336944103241 val_steps 40 val_acc 0.9344\n",
            "[129,    10] loss: 1.471\n",
            "[129,    20] loss: 1.471\n",
            "[129,    30] loss: 1.472\n",
            "[129,    40] loss: 1.472\n",
            "[129,    50] loss: 1.476\n",
            "[129,    60] loss: 1.476\n",
            "[129,    70] loss: 1.470\n",
            "[129,    80] loss: 1.470\n",
            "[129,    90] loss: 1.473\n",
            "[129,   100] loss: 1.468\n",
            "[129,   110] loss: 1.470\n",
            "[129,   120] loss: 1.468\n",
            "[129,   130] loss: 1.471\n",
            "[129,   140] loss: 1.474\n",
            "[129,   150] loss: 1.470\n",
            "[129,   160] loss: 1.473\n",
            "[129,   170] loss: 1.473\n",
            "[129,   180] loss: 1.475\n",
            "[129,   190] loss: 1.472\n",
            "[129,   200] loss: 1.470\n",
            "[129,   210] loss: 1.470\n",
            "[129,   220] loss: 1.474\n",
            "[129,   230] loss: 1.472\n",
            "[129,   240] loss: 1.474\n",
            "[129,   250] loss: 1.470\n",
            "[129,   260] loss: 1.472\n",
            "[129,   270] loss: 1.472\n",
            "[129,   280] loss: 1.472\n",
            "[129,   290] loss: 1.469\n",
            "[129,   300] loss: 1.474\n",
            "[129,   310] loss: 1.470\n",
            "[129,   320] loss: 1.471\n",
            "[129,   330] loss: 1.467\n",
            "[129,   340] loss: 1.468\n",
            "[129,   350] loss: 1.469\n",
            "[129,   360] loss: 1.473\n",
            "[129,   370] loss: 1.473\n",
            "[129,   380] loss: 1.470\n",
            "[129,   390] loss: 1.472\n",
            "[129,   400] loss: 1.470\n",
            "[129,   410] loss: 1.473\n",
            "[129,   420] loss: 1.475\n",
            "[129,   430] loss: 1.470\n",
            "epoch 128 val_loss 61.17674398422241 val_steps 40 val_acc 0.9334\n",
            "[130,    10] loss: 1.468\n",
            "[130,    20] loss: 1.469\n",
            "[130,    30] loss: 1.468\n",
            "[130,    40] loss: 1.476\n",
            "[130,    50] loss: 1.469\n",
            "[130,    60] loss: 1.475\n",
            "[130,    70] loss: 1.475\n",
            "[130,    80] loss: 1.471\n",
            "[130,    90] loss: 1.472\n",
            "[130,   100] loss: 1.475\n",
            "[130,   110] loss: 1.472\n",
            "[130,   120] loss: 1.473\n",
            "[130,   130] loss: 1.470\n",
            "[130,   140] loss: 1.469\n",
            "[130,   150] loss: 1.467\n",
            "[130,   160] loss: 1.474\n",
            "[130,   170] loss: 1.475\n",
            "[130,   180] loss: 1.477\n",
            "[130,   190] loss: 1.477\n",
            "[130,   200] loss: 1.473\n",
            "[130,   210] loss: 1.472\n",
            "[130,   220] loss: 1.475\n",
            "[130,   230] loss: 1.472\n",
            "[130,   240] loss: 1.466\n",
            "[130,   250] loss: 1.473\n",
            "[130,   260] loss: 1.475\n",
            "[130,   270] loss: 1.472\n",
            "[130,   280] loss: 1.473\n",
            "[130,   290] loss: 1.471\n",
            "[130,   300] loss: 1.469\n",
            "[130,   310] loss: 1.469\n",
            "[130,   320] loss: 1.475\n",
            "[130,   330] loss: 1.470\n",
            "[130,   340] loss: 1.468\n",
            "[130,   350] loss: 1.468\n",
            "[130,   360] loss: 1.469\n",
            "[130,   370] loss: 1.471\n",
            "[130,   380] loss: 1.472\n",
            "[130,   390] loss: 1.473\n",
            "[130,   400] loss: 1.476\n",
            "[130,   410] loss: 1.470\n",
            "[130,   420] loss: 1.465\n",
            "[130,   430] loss: 1.469\n",
            "epoch 129 val_loss 61.27478504180908 val_steps 40 val_acc 0.9322\n",
            "[131,    10] loss: 1.477\n",
            "[131,    20] loss: 1.471\n",
            "[131,    30] loss: 1.473\n",
            "[131,    40] loss: 1.469\n",
            "[131,    50] loss: 1.474\n",
            "[131,    60] loss: 1.474\n",
            "[131,    70] loss: 1.468\n",
            "[131,    80] loss: 1.468\n",
            "[131,    90] loss: 1.473\n",
            "[131,   100] loss: 1.471\n",
            "[131,   110] loss: 1.472\n",
            "[131,   120] loss: 1.471\n",
            "[131,   130] loss: 1.475\n",
            "[131,   140] loss: 1.475\n",
            "[131,   150] loss: 1.474\n",
            "[131,   160] loss: 1.471\n",
            "[131,   170] loss: 1.474\n",
            "[131,   180] loss: 1.473\n",
            "[131,   190] loss: 1.470\n",
            "[131,   200] loss: 1.471\n",
            "[131,   210] loss: 1.472\n",
            "[131,   220] loss: 1.472\n",
            "[131,   230] loss: 1.469\n",
            "[131,   240] loss: 1.473\n",
            "[131,   250] loss: 1.470\n",
            "[131,   260] loss: 1.468\n",
            "[131,   270] loss: 1.471\n",
            "[131,   280] loss: 1.470\n",
            "[131,   290] loss: 1.474\n",
            "[131,   300] loss: 1.473\n",
            "[131,   310] loss: 1.467\n",
            "[131,   320] loss: 1.470\n",
            "[131,   330] loss: 1.468\n",
            "[131,   340] loss: 1.475\n",
            "[131,   350] loss: 1.473\n",
            "[131,   360] loss: 1.472\n",
            "[131,   370] loss: 1.466\n",
            "[131,   380] loss: 1.473\n",
            "[131,   390] loss: 1.474\n",
            "[131,   400] loss: 1.469\n",
            "[131,   410] loss: 1.470\n",
            "[131,   420] loss: 1.473\n",
            "[131,   430] loss: 1.471\n",
            "epoch 130 val_loss 61.218743085861206 val_steps 40 val_acc 0.9306\n",
            "[132,    10] loss: 1.472\n",
            "[132,    20] loss: 1.468\n",
            "[132,    30] loss: 1.471\n",
            "[132,    40] loss: 1.472\n",
            "[132,    50] loss: 1.473\n",
            "[132,    60] loss: 1.475\n",
            "[132,    70] loss: 1.469\n",
            "[132,    80] loss: 1.472\n",
            "[132,    90] loss: 1.471\n",
            "[132,   100] loss: 1.475\n",
            "[132,   110] loss: 1.470\n",
            "[132,   120] loss: 1.475\n",
            "[132,   130] loss: 1.472\n",
            "[132,   140] loss: 1.469\n",
            "[132,   150] loss: 1.473\n",
            "[132,   160] loss: 1.471\n",
            "[132,   170] loss: 1.468\n",
            "[132,   180] loss: 1.468\n",
            "[132,   190] loss: 1.474\n",
            "[132,   200] loss: 1.473\n",
            "[132,   210] loss: 1.469\n",
            "[132,   220] loss: 1.471\n",
            "[132,   230] loss: 1.469\n",
            "[132,   240] loss: 1.473\n",
            "[132,   250] loss: 1.472\n",
            "[132,   260] loss: 1.471\n",
            "[132,   270] loss: 1.470\n",
            "[132,   280] loss: 1.470\n",
            "[132,   290] loss: 1.476\n",
            "[132,   300] loss: 1.469\n",
            "[132,   310] loss: 1.471\n",
            "[132,   320] loss: 1.475\n",
            "[132,   330] loss: 1.472\n",
            "[132,   340] loss: 1.471\n",
            "[132,   350] loss: 1.470\n",
            "[132,   360] loss: 1.474\n",
            "[132,   370] loss: 1.470\n",
            "[132,   380] loss: 1.474\n",
            "[132,   390] loss: 1.473\n",
            "[132,   400] loss: 1.469\n",
            "[132,   410] loss: 1.471\n",
            "[132,   420] loss: 1.471\n",
            "[132,   430] loss: 1.478\n",
            "epoch 131 val_loss 61.36973571777344 val_steps 40 val_acc 0.9324\n",
            "[133,    10] loss: 1.471\n",
            "[133,    20] loss: 1.472\n",
            "[133,    30] loss: 1.470\n",
            "[133,    40] loss: 1.470\n",
            "[133,    50] loss: 1.468\n",
            "[133,    60] loss: 1.469\n",
            "[133,    70] loss: 1.472\n",
            "[133,    80] loss: 1.475\n",
            "[133,    90] loss: 1.473\n",
            "[133,   100] loss: 1.473\n",
            "[133,   110] loss: 1.472\n",
            "[133,   120] loss: 1.471\n",
            "[133,   130] loss: 1.469\n",
            "[133,   140] loss: 1.476\n",
            "[133,   150] loss: 1.469\n",
            "[133,   160] loss: 1.472\n",
            "[133,   170] loss: 1.472\n",
            "[133,   180] loss: 1.472\n",
            "[133,   190] loss: 1.467\n",
            "[133,   200] loss: 1.470\n",
            "[133,   210] loss: 1.470\n",
            "[133,   220] loss: 1.471\n",
            "[133,   230] loss: 1.473\n",
            "[133,   240] loss: 1.469\n",
            "[133,   250] loss: 1.473\n",
            "[133,   260] loss: 1.472\n",
            "[133,   270] loss: 1.473\n",
            "[133,   280] loss: 1.469\n",
            "[133,   290] loss: 1.474\n",
            "[133,   300] loss: 1.467\n",
            "[133,   310] loss: 1.476\n",
            "[133,   320] loss: 1.470\n",
            "[133,   330] loss: 1.469\n",
            "[133,   340] loss: 1.470\n",
            "[133,   350] loss: 1.471\n",
            "[133,   360] loss: 1.470\n",
            "[133,   370] loss: 1.475\n",
            "[133,   380] loss: 1.474\n",
            "[133,   390] loss: 1.475\n",
            "[133,   400] loss: 1.471\n",
            "[133,   410] loss: 1.470\n",
            "[133,   420] loss: 1.473\n",
            "[133,   430] loss: 1.472\n",
            "epoch 132 val_loss 61.06306552886963 val_steps 40 val_acc 0.9344\n",
            "[134,    10] loss: 1.475\n",
            "[134,    20] loss: 1.471\n",
            "[134,    30] loss: 1.470\n",
            "[134,    40] loss: 1.469\n",
            "[134,    50] loss: 1.475\n",
            "[134,    60] loss: 1.476\n",
            "[134,    70] loss: 1.470\n",
            "[134,    80] loss: 1.472\n",
            "[134,    90] loss: 1.474\n",
            "[134,   100] loss: 1.467\n",
            "[134,   110] loss: 1.474\n",
            "[134,   120] loss: 1.469\n",
            "[134,   130] loss: 1.465\n",
            "[134,   140] loss: 1.475\n",
            "[134,   150] loss: 1.467\n",
            "[134,   160] loss: 1.472\n",
            "[134,   170] loss: 1.468\n",
            "[134,   180] loss: 1.471\n",
            "[134,   190] loss: 1.472\n",
            "[134,   200] loss: 1.471\n",
            "[134,   210] loss: 1.473\n",
            "[134,   220] loss: 1.471\n",
            "[134,   230] loss: 1.473\n",
            "[134,   240] loss: 1.475\n",
            "[134,   250] loss: 1.471\n",
            "[134,   260] loss: 1.466\n",
            "[134,   270] loss: 1.478\n",
            "[134,   280] loss: 1.472\n",
            "[134,   290] loss: 1.469\n",
            "[134,   300] loss: 1.475\n",
            "[134,   310] loss: 1.470\n",
            "[134,   320] loss: 1.472\n",
            "[134,   330] loss: 1.468\n",
            "[134,   340] loss: 1.474\n",
            "[134,   350] loss: 1.471\n",
            "[134,   360] loss: 1.469\n",
            "[134,   370] loss: 1.469\n",
            "[134,   380] loss: 1.472\n",
            "[134,   390] loss: 1.473\n",
            "[134,   400] loss: 1.472\n",
            "[134,   410] loss: 1.478\n",
            "[134,   420] loss: 1.466\n",
            "[134,   430] loss: 1.471\n",
            "epoch 133 val_loss 61.051830768585205 val_steps 40 val_acc 0.9344\n",
            "[135,    10] loss: 1.473\n",
            "[135,    20] loss: 1.471\n",
            "[135,    30] loss: 1.467\n",
            "[135,    40] loss: 1.474\n",
            "[135,    50] loss: 1.471\n",
            "[135,    60] loss: 1.471\n",
            "[135,    70] loss: 1.473\n",
            "[135,    80] loss: 1.475\n",
            "[135,    90] loss: 1.472\n",
            "[135,   100] loss: 1.473\n",
            "[135,   110] loss: 1.472\n",
            "[135,   120] loss: 1.472\n",
            "[135,   130] loss: 1.474\n",
            "[135,   140] loss: 1.471\n",
            "[135,   150] loss: 1.468\n",
            "[135,   160] loss: 1.473\n",
            "[135,   170] loss: 1.468\n",
            "[135,   180] loss: 1.472\n",
            "[135,   190] loss: 1.470\n",
            "[135,   200] loss: 1.477\n",
            "[135,   210] loss: 1.471\n",
            "[135,   220] loss: 1.474\n",
            "[135,   230] loss: 1.469\n",
            "[135,   240] loss: 1.474\n",
            "[135,   250] loss: 1.472\n",
            "[135,   260] loss: 1.470\n",
            "[135,   270] loss: 1.473\n",
            "[135,   280] loss: 1.472\n",
            "[135,   290] loss: 1.470\n",
            "[135,   300] loss: 1.473\n",
            "[135,   310] loss: 1.471\n",
            "[135,   320] loss: 1.473\n",
            "[135,   330] loss: 1.476\n",
            "[135,   340] loss: 1.475\n",
            "[135,   350] loss: 1.472\n",
            "[135,   360] loss: 1.466\n",
            "[135,   370] loss: 1.470\n",
            "[135,   380] loss: 1.468\n",
            "[135,   390] loss: 1.473\n",
            "[135,   400] loss: 1.472\n",
            "[135,   410] loss: 1.471\n",
            "[135,   420] loss: 1.470\n",
            "[135,   430] loss: 1.468\n",
            "epoch 134 val_loss 61.16791784763336 val_steps 40 val_acc 0.9352\n",
            "[136,    10] loss: 1.472\n",
            "[136,    20] loss: 1.475\n",
            "[136,    30] loss: 1.477\n",
            "[136,    40] loss: 1.472\n",
            "[136,    50] loss: 1.470\n",
            "[136,    60] loss: 1.475\n",
            "[136,    70] loss: 1.473\n",
            "[136,    80] loss: 1.469\n",
            "[136,    90] loss: 1.470\n",
            "[136,   100] loss: 1.469\n",
            "[136,   110] loss: 1.472\n",
            "[136,   120] loss: 1.470\n",
            "[136,   130] loss: 1.469\n",
            "[136,   140] loss: 1.470\n",
            "[136,   150] loss: 1.469\n",
            "[136,   160] loss: 1.468\n",
            "[136,   170] loss: 1.473\n",
            "[136,   180] loss: 1.470\n",
            "[136,   190] loss: 1.472\n",
            "[136,   200] loss: 1.469\n",
            "[136,   210] loss: 1.477\n",
            "[136,   220] loss: 1.476\n",
            "[136,   230] loss: 1.472\n",
            "[136,   240] loss: 1.468\n",
            "[136,   250] loss: 1.469\n",
            "[136,   260] loss: 1.470\n",
            "[136,   270] loss: 1.475\n",
            "[136,   280] loss: 1.471\n",
            "[136,   290] loss: 1.476\n",
            "[136,   300] loss: 1.466\n",
            "[136,   310] loss: 1.471\n",
            "[136,   320] loss: 1.467\n",
            "[136,   330] loss: 1.468\n",
            "[136,   340] loss: 1.469\n",
            "[136,   350] loss: 1.472\n",
            "[136,   360] loss: 1.471\n",
            "[136,   370] loss: 1.473\n",
            "[136,   380] loss: 1.479\n",
            "[136,   390] loss: 1.470\n",
            "[136,   400] loss: 1.472\n",
            "[136,   410] loss: 1.470\n",
            "[136,   420] loss: 1.470\n",
            "[136,   430] loss: 1.479\n",
            "epoch 135 val_loss 61.35414171218872 val_steps 40 val_acc 0.9312\n",
            "[137,    10] loss: 1.470\n",
            "[137,    20] loss: 1.471\n",
            "[137,    30] loss: 1.469\n",
            "[137,    40] loss: 1.471\n",
            "[137,    50] loss: 1.474\n",
            "[137,    60] loss: 1.471\n",
            "[137,    70] loss: 1.470\n",
            "[137,    80] loss: 1.472\n",
            "[137,    90] loss: 1.470\n",
            "[137,   100] loss: 1.474\n",
            "[137,   110] loss: 1.472\n",
            "[137,   120] loss: 1.473\n",
            "[137,   130] loss: 1.468\n",
            "[137,   140] loss: 1.470\n",
            "[137,   150] loss: 1.474\n",
            "[137,   160] loss: 1.477\n",
            "[137,   170] loss: 1.471\n",
            "[137,   180] loss: 1.472\n",
            "[137,   190] loss: 1.472\n",
            "[137,   200] loss: 1.465\n",
            "[137,   210] loss: 1.473\n",
            "[137,   220] loss: 1.467\n",
            "[137,   230] loss: 1.469\n",
            "[137,   240] loss: 1.473\n",
            "[137,   250] loss: 1.474\n",
            "[137,   260] loss: 1.474\n",
            "[137,   270] loss: 1.474\n",
            "[137,   280] loss: 1.469\n",
            "[137,   290] loss: 1.471\n",
            "[137,   300] loss: 1.471\n",
            "[137,   310] loss: 1.471\n",
            "[137,   320] loss: 1.474\n",
            "[137,   330] loss: 1.470\n",
            "[137,   340] loss: 1.472\n",
            "[137,   350] loss: 1.478\n",
            "[137,   360] loss: 1.474\n",
            "[137,   370] loss: 1.471\n",
            "[137,   380] loss: 1.470\n",
            "[137,   390] loss: 1.470\n",
            "[137,   400] loss: 1.468\n",
            "[137,   410] loss: 1.475\n",
            "[137,   420] loss: 1.470\n",
            "[137,   430] loss: 1.469\n",
            "epoch 136 val_loss 61.19871735572815 val_steps 40 val_acc 0.9328\n",
            "[138,    10] loss: 1.471\n",
            "[138,    20] loss: 1.472\n",
            "[138,    30] loss: 1.473\n",
            "[138,    40] loss: 1.469\n",
            "[138,    50] loss: 1.467\n",
            "[138,    60] loss: 1.475\n",
            "[138,    70] loss: 1.473\n",
            "[138,    80] loss: 1.473\n",
            "[138,    90] loss: 1.468\n",
            "[138,   100] loss: 1.473\n",
            "[138,   110] loss: 1.474\n",
            "[138,   120] loss: 1.474\n",
            "[138,   130] loss: 1.473\n",
            "[138,   140] loss: 1.473\n",
            "[138,   150] loss: 1.476\n",
            "[138,   160] loss: 1.477\n",
            "[138,   170] loss: 1.472\n",
            "[138,   180] loss: 1.473\n",
            "[138,   190] loss: 1.466\n",
            "[138,   200] loss: 1.469\n",
            "[138,   210] loss: 1.476\n",
            "[138,   220] loss: 1.472\n",
            "[138,   230] loss: 1.467\n",
            "[138,   240] loss: 1.471\n",
            "[138,   250] loss: 1.471\n",
            "[138,   260] loss: 1.473\n",
            "[138,   270] loss: 1.473\n",
            "[138,   280] loss: 1.468\n",
            "[138,   290] loss: 1.470\n",
            "[138,   300] loss: 1.467\n",
            "[138,   310] loss: 1.467\n",
            "[138,   320] loss: 1.470\n",
            "[138,   330] loss: 1.473\n",
            "[138,   340] loss: 1.469\n",
            "[138,   350] loss: 1.475\n",
            "[138,   360] loss: 1.472\n",
            "[138,   370] loss: 1.473\n",
            "[138,   380] loss: 1.472\n",
            "[138,   390] loss: 1.472\n",
            "[138,   400] loss: 1.468\n",
            "[138,   410] loss: 1.468\n",
            "[138,   420] loss: 1.472\n",
            "[138,   430] loss: 1.475\n",
            "epoch 137 val_loss 61.13807487487793 val_steps 40 val_acc 0.935\n",
            "[139,    10] loss: 1.475\n",
            "[139,    20] loss: 1.471\n",
            "[139,    30] loss: 1.473\n",
            "[139,    40] loss: 1.472\n",
            "[139,    50] loss: 1.472\n",
            "[139,    60] loss: 1.467\n",
            "[139,    70] loss: 1.471\n",
            "[139,    80] loss: 1.466\n",
            "[139,    90] loss: 1.471\n",
            "[139,   100] loss: 1.472\n",
            "[139,   110] loss: 1.470\n",
            "[139,   120] loss: 1.471\n",
            "[139,   130] loss: 1.472\n",
            "[139,   140] loss: 1.471\n",
            "[139,   150] loss: 1.472\n",
            "[139,   160] loss: 1.474\n",
            "[139,   170] loss: 1.476\n",
            "[139,   180] loss: 1.467\n",
            "[139,   190] loss: 1.472\n",
            "[139,   200] loss: 1.469\n",
            "[139,   210] loss: 1.472\n",
            "[139,   220] loss: 1.471\n",
            "[139,   230] loss: 1.475\n",
            "[139,   240] loss: 1.474\n",
            "[139,   250] loss: 1.475\n",
            "[139,   260] loss: 1.477\n",
            "[139,   270] loss: 1.472\n",
            "[139,   280] loss: 1.471\n",
            "[139,   290] loss: 1.468\n",
            "[139,   300] loss: 1.473\n",
            "[139,   310] loss: 1.470\n",
            "[139,   320] loss: 1.471\n",
            "[139,   330] loss: 1.478\n",
            "[139,   340] loss: 1.472\n",
            "[139,   350] loss: 1.470\n",
            "[139,   360] loss: 1.475\n",
            "[139,   370] loss: 1.468\n",
            "[139,   380] loss: 1.473\n",
            "[139,   390] loss: 1.474\n",
            "[139,   400] loss: 1.473\n",
            "[139,   410] loss: 1.468\n",
            "[139,   420] loss: 1.466\n",
            "[139,   430] loss: 1.468\n",
            "epoch 138 val_loss 61.03720140457153 val_steps 40 val_acc 0.9352\n",
            "[140,    10] loss: 1.473\n",
            "[140,    20] loss: 1.470\n",
            "[140,    30] loss: 1.474\n",
            "[140,    40] loss: 1.472\n",
            "[140,    50] loss: 1.473\n",
            "[140,    60] loss: 1.473\n",
            "[140,    70] loss: 1.474\n",
            "[140,    80] loss: 1.474\n",
            "[140,    90] loss: 1.475\n",
            "[140,   100] loss: 1.473\n",
            "[140,   110] loss: 1.470\n",
            "[140,   120] loss: 1.472\n",
            "[140,   130] loss: 1.472\n",
            "[140,   140] loss: 1.474\n",
            "[140,   150] loss: 1.465\n",
            "[140,   160] loss: 1.469\n",
            "[140,   170] loss: 1.470\n",
            "[140,   180] loss: 1.468\n",
            "[140,   190] loss: 1.470\n",
            "[140,   200] loss: 1.469\n",
            "[140,   210] loss: 1.468\n",
            "[140,   220] loss: 1.469\n",
            "[140,   230] loss: 1.470\n",
            "[140,   240] loss: 1.474\n",
            "[140,   250] loss: 1.473\n",
            "[140,   260] loss: 1.471\n",
            "[140,   270] loss: 1.467\n",
            "[140,   280] loss: 1.472\n",
            "[140,   290] loss: 1.470\n",
            "[140,   300] loss: 1.475\n",
            "[140,   310] loss: 1.475\n",
            "[140,   320] loss: 1.470\n",
            "[140,   330] loss: 1.472\n",
            "[140,   340] loss: 1.474\n",
            "[140,   350] loss: 1.470\n",
            "[140,   360] loss: 1.469\n",
            "[140,   370] loss: 1.467\n",
            "[140,   380] loss: 1.472\n",
            "[140,   390] loss: 1.473\n",
            "[140,   400] loss: 1.473\n",
            "[140,   410] loss: 1.474\n",
            "[140,   420] loss: 1.468\n",
            "[140,   430] loss: 1.476\n",
            "epoch 139 val_loss 61.04969024658203 val_steps 40 val_acc 0.9354\n",
            "[141,    10] loss: 1.474\n",
            "[141,    20] loss: 1.469\n",
            "[141,    30] loss: 1.471\n",
            "[141,    40] loss: 1.471\n",
            "[141,    50] loss: 1.468\n",
            "[141,    60] loss: 1.476\n",
            "[141,    70] loss: 1.475\n",
            "[141,    80] loss: 1.467\n",
            "[141,    90] loss: 1.475\n",
            "[141,   100] loss: 1.472\n",
            "[141,   110] loss: 1.475\n",
            "[141,   120] loss: 1.471\n",
            "[141,   130] loss: 1.472\n",
            "[141,   140] loss: 1.470\n",
            "[141,   150] loss: 1.468\n",
            "[141,   160] loss: 1.473\n",
            "[141,   170] loss: 1.473\n",
            "[141,   180] loss: 1.468\n",
            "[141,   190] loss: 1.468\n",
            "[141,   200] loss: 1.471\n",
            "[141,   210] loss: 1.470\n",
            "[141,   220] loss: 1.473\n",
            "[141,   230] loss: 1.474\n",
            "[141,   240] loss: 1.469\n",
            "[141,   250] loss: 1.470\n",
            "[141,   260] loss: 1.472\n",
            "[141,   270] loss: 1.472\n",
            "[141,   280] loss: 1.471\n",
            "[141,   290] loss: 1.471\n",
            "[141,   300] loss: 1.471\n",
            "[141,   310] loss: 1.471\n",
            "[141,   320] loss: 1.472\n",
            "[141,   330] loss: 1.473\n",
            "[141,   340] loss: 1.475\n",
            "[141,   350] loss: 1.471\n",
            "[141,   360] loss: 1.473\n",
            "[141,   370] loss: 1.469\n",
            "[141,   380] loss: 1.472\n",
            "[141,   390] loss: 1.472\n",
            "[141,   400] loss: 1.473\n",
            "[141,   410] loss: 1.468\n",
            "[141,   420] loss: 1.472\n",
            "[141,   430] loss: 1.470\n",
            "epoch 140 val_loss 61.15518927574158 val_steps 40 val_acc 0.935\n",
            "[142,    10] loss: 1.478\n",
            "[142,    20] loss: 1.472\n",
            "[142,    30] loss: 1.472\n",
            "[142,    40] loss: 1.471\n",
            "[142,    50] loss: 1.470\n",
            "[142,    60] loss: 1.471\n",
            "[142,    70] loss: 1.472\n",
            "[142,    80] loss: 1.469\n",
            "[142,    90] loss: 1.471\n",
            "[142,   100] loss: 1.475\n",
            "[142,   110] loss: 1.469\n",
            "[142,   120] loss: 1.472\n",
            "[142,   130] loss: 1.473\n",
            "[142,   140] loss: 1.476\n",
            "[142,   150] loss: 1.469\n",
            "[142,   160] loss: 1.469\n",
            "[142,   170] loss: 1.476\n",
            "[142,   180] loss: 1.470\n",
            "[142,   190] loss: 1.468\n",
            "[142,   200] loss: 1.467\n",
            "[142,   210] loss: 1.466\n",
            "[142,   220] loss: 1.469\n",
            "[142,   230] loss: 1.474\n",
            "[142,   240] loss: 1.473\n",
            "[142,   250] loss: 1.469\n",
            "[142,   260] loss: 1.475\n",
            "[142,   270] loss: 1.473\n",
            "[142,   280] loss: 1.472\n",
            "[142,   290] loss: 1.471\n",
            "[142,   300] loss: 1.474\n",
            "[142,   310] loss: 1.470\n",
            "[142,   320] loss: 1.469\n",
            "[142,   330] loss: 1.475\n",
            "[142,   340] loss: 1.471\n",
            "[142,   350] loss: 1.476\n",
            "[142,   360] loss: 1.473\n",
            "[142,   370] loss: 1.468\n",
            "[142,   380] loss: 1.475\n",
            "[142,   390] loss: 1.470\n",
            "[142,   400] loss: 1.472\n",
            "[142,   410] loss: 1.469\n",
            "[142,   420] loss: 1.466\n",
            "[142,   430] loss: 1.473\n",
            "epoch 141 val_loss 61.064106583595276 val_steps 40 val_acc 0.9342\n",
            "[143,    10] loss: 1.470\n",
            "[143,    20] loss: 1.472\n",
            "[143,    30] loss: 1.479\n",
            "[143,    40] loss: 1.469\n",
            "[143,    50] loss: 1.476\n",
            "[143,    60] loss: 1.471\n",
            "[143,    70] loss: 1.469\n",
            "[143,    80] loss: 1.469\n",
            "[143,    90] loss: 1.469\n",
            "[143,   100] loss: 1.474\n",
            "[143,   110] loss: 1.475\n",
            "[143,   120] loss: 1.476\n",
            "[143,   130] loss: 1.472\n",
            "[143,   140] loss: 1.475\n",
            "[143,   150] loss: 1.470\n",
            "[143,   160] loss: 1.470\n",
            "[143,   170] loss: 1.474\n",
            "[143,   180] loss: 1.473\n",
            "[143,   190] loss: 1.475\n",
            "[143,   200] loss: 1.468\n",
            "[143,   210] loss: 1.475\n",
            "[143,   220] loss: 1.467\n",
            "[143,   230] loss: 1.469\n",
            "[143,   240] loss: 1.471\n",
            "[143,   250] loss: 1.467\n",
            "[143,   260] loss: 1.470\n",
            "[143,   270] loss: 1.475\n",
            "[143,   280] loss: 1.470\n",
            "[143,   290] loss: 1.473\n",
            "[143,   300] loss: 1.467\n",
            "[143,   310] loss: 1.470\n",
            "[143,   320] loss: 1.469\n",
            "[143,   330] loss: 1.473\n",
            "[143,   340] loss: 1.472\n",
            "[143,   350] loss: 1.469\n",
            "[143,   360] loss: 1.469\n",
            "[143,   370] loss: 1.469\n",
            "[143,   380] loss: 1.473\n",
            "[143,   390] loss: 1.471\n",
            "[143,   400] loss: 1.473\n",
            "[143,   410] loss: 1.471\n",
            "[143,   420] loss: 1.472\n",
            "[143,   430] loss: 1.470\n",
            "epoch 142 val_loss 61.302908420562744 val_steps 40 val_acc 0.9328\n",
            "[144,    10] loss: 1.467\n",
            "[144,    20] loss: 1.472\n",
            "[144,    30] loss: 1.469\n",
            "[144,    40] loss: 1.468\n",
            "[144,    50] loss: 1.471\n",
            "[144,    60] loss: 1.471\n",
            "[144,    70] loss: 1.470\n",
            "[144,    80] loss: 1.468\n",
            "[144,    90] loss: 1.472\n",
            "[144,   100] loss: 1.473\n",
            "[144,   110] loss: 1.476\n",
            "[144,   120] loss: 1.469\n",
            "[144,   130] loss: 1.473\n",
            "[144,   140] loss: 1.478\n",
            "[144,   150] loss: 1.471\n",
            "[144,   160] loss: 1.472\n",
            "[144,   170] loss: 1.472\n",
            "[144,   180] loss: 1.472\n",
            "[144,   190] loss: 1.475\n",
            "[144,   200] loss: 1.472\n",
            "[144,   210] loss: 1.473\n",
            "[144,   220] loss: 1.473\n",
            "[144,   230] loss: 1.470\n",
            "[144,   240] loss: 1.471\n",
            "[144,   250] loss: 1.468\n",
            "[144,   260] loss: 1.471\n",
            "[144,   270] loss: 1.470\n",
            "[144,   280] loss: 1.473\n",
            "[144,   290] loss: 1.472\n",
            "[144,   300] loss: 1.477\n",
            "[144,   310] loss: 1.472\n",
            "[144,   320] loss: 1.471\n",
            "[144,   330] loss: 1.473\n",
            "[144,   340] loss: 1.478\n",
            "[144,   350] loss: 1.469\n",
            "[144,   360] loss: 1.469\n",
            "[144,   370] loss: 1.471\n",
            "[144,   380] loss: 1.474\n",
            "[144,   390] loss: 1.469\n",
            "[144,   400] loss: 1.467\n",
            "[144,   410] loss: 1.472\n",
            "[144,   420] loss: 1.469\n",
            "[144,   430] loss: 1.469\n",
            "epoch 143 val_loss 61.18912851810455 val_steps 40 val_acc 0.9336\n",
            "[145,    10] loss: 1.465\n",
            "[145,    20] loss: 1.469\n",
            "[145,    30] loss: 1.468\n",
            "[145,    40] loss: 1.476\n",
            "[145,    50] loss: 1.469\n",
            "[145,    60] loss: 1.474\n",
            "[145,    70] loss: 1.473\n",
            "[145,    80] loss: 1.476\n",
            "[145,    90] loss: 1.475\n",
            "[145,   100] loss: 1.472\n",
            "[145,   110] loss: 1.471\n",
            "[145,   120] loss: 1.475\n",
            "[145,   130] loss: 1.471\n",
            "[145,   140] loss: 1.472\n",
            "[145,   150] loss: 1.471\n",
            "[145,   160] loss: 1.476\n",
            "[145,   170] loss: 1.467\n",
            "[145,   180] loss: 1.470\n",
            "[145,   190] loss: 1.472\n",
            "[145,   200] loss: 1.477\n",
            "[145,   210] loss: 1.471\n",
            "[145,   220] loss: 1.470\n",
            "[145,   230] loss: 1.472\n",
            "[145,   240] loss: 1.475\n",
            "[145,   250] loss: 1.473\n",
            "[145,   260] loss: 1.471\n",
            "[145,   270] loss: 1.468\n",
            "[145,   280] loss: 1.470\n",
            "[145,   290] loss: 1.471\n",
            "[145,   300] loss: 1.472\n",
            "[145,   310] loss: 1.468\n",
            "[145,   320] loss: 1.469\n",
            "[145,   330] loss: 1.472\n",
            "[145,   340] loss: 1.474\n",
            "[145,   350] loss: 1.469\n",
            "[145,   360] loss: 1.469\n",
            "[145,   370] loss: 1.469\n",
            "[145,   380] loss: 1.473\n",
            "[145,   390] loss: 1.471\n",
            "[145,   400] loss: 1.470\n",
            "[145,   410] loss: 1.475\n",
            "[145,   420] loss: 1.469\n",
            "[145,   430] loss: 1.469\n",
            "epoch 144 val_loss 61.136499881744385 val_steps 40 val_acc 0.9346\n",
            "[146,    10] loss: 1.472\n",
            "[146,    20] loss: 1.471\n",
            "[146,    30] loss: 1.473\n",
            "[146,    40] loss: 1.478\n",
            "[146,    50] loss: 1.473\n",
            "[146,    60] loss: 1.472\n",
            "[146,    70] loss: 1.473\n",
            "[146,    80] loss: 1.472\n",
            "[146,    90] loss: 1.469\n",
            "[146,   100] loss: 1.471\n",
            "[146,   110] loss: 1.472\n",
            "[146,   120] loss: 1.473\n",
            "[146,   130] loss: 1.475\n",
            "[146,   140] loss: 1.474\n",
            "[146,   150] loss: 1.473\n",
            "[146,   160] loss: 1.475\n",
            "[146,   170] loss: 1.475\n",
            "[146,   180] loss: 1.473\n",
            "[146,   190] loss: 1.473\n",
            "[146,   200] loss: 1.470\n",
            "[146,   210] loss: 1.473\n",
            "[146,   220] loss: 1.468\n",
            "[146,   230] loss: 1.473\n",
            "[146,   240] loss: 1.467\n",
            "[146,   250] loss: 1.468\n",
            "[146,   260] loss: 1.469\n",
            "[146,   270] loss: 1.473\n",
            "[146,   280] loss: 1.467\n",
            "[146,   290] loss: 1.468\n",
            "[146,   300] loss: 1.469\n",
            "[146,   310] loss: 1.477\n",
            "[146,   320] loss: 1.471\n",
            "[146,   330] loss: 1.472\n",
            "[146,   340] loss: 1.471\n",
            "[146,   350] loss: 1.466\n",
            "[146,   360] loss: 1.472\n",
            "[146,   370] loss: 1.472\n",
            "[146,   380] loss: 1.467\n",
            "[146,   390] loss: 1.470\n",
            "[146,   400] loss: 1.473\n",
            "[146,   410] loss: 1.470\n",
            "[146,   420] loss: 1.465\n",
            "[146,   430] loss: 1.468\n",
            "epoch 145 val_loss 61.13510715961456 val_steps 40 val_acc 0.9336\n",
            "[147,    10] loss: 1.475\n",
            "[147,    20] loss: 1.474\n",
            "[147,    30] loss: 1.471\n",
            "[147,    40] loss: 1.472\n",
            "[147,    50] loss: 1.473\n",
            "[147,    60] loss: 1.466\n",
            "[147,    70] loss: 1.472\n",
            "[147,    80] loss: 1.473\n",
            "[147,    90] loss: 1.471\n",
            "[147,   100] loss: 1.472\n",
            "[147,   110] loss: 1.474\n",
            "[147,   120] loss: 1.472\n",
            "[147,   130] loss: 1.470\n",
            "[147,   140] loss: 1.471\n",
            "[147,   150] loss: 1.468\n",
            "[147,   160] loss: 1.479\n",
            "[147,   170] loss: 1.472\n",
            "[147,   180] loss: 1.475\n",
            "[147,   190] loss: 1.472\n",
            "[147,   200] loss: 1.470\n",
            "[147,   210] loss: 1.468\n",
            "[147,   220] loss: 1.469\n",
            "[147,   230] loss: 1.473\n",
            "[147,   240] loss: 1.470\n",
            "[147,   250] loss: 1.473\n",
            "[147,   260] loss: 1.471\n",
            "[147,   270] loss: 1.471\n",
            "[147,   280] loss: 1.470\n",
            "[147,   290] loss: 1.472\n",
            "[147,   300] loss: 1.472\n",
            "[147,   310] loss: 1.475\n",
            "[147,   320] loss: 1.466\n",
            "[147,   330] loss: 1.469\n",
            "[147,   340] loss: 1.471\n",
            "[147,   350] loss: 1.472\n",
            "[147,   360] loss: 1.476\n",
            "[147,   370] loss: 1.473\n",
            "[147,   380] loss: 1.466\n",
            "[147,   390] loss: 1.467\n",
            "[147,   400] loss: 1.471\n",
            "[147,   410] loss: 1.472\n",
            "[147,   420] loss: 1.472\n",
            "[147,   430] loss: 1.471\n",
            "epoch 146 val_loss 61.30044722557068 val_steps 40 val_acc 0.9332\n",
            "[148,    10] loss: 1.471\n",
            "[148,    20] loss: 1.475\n",
            "[148,    30] loss: 1.470\n",
            "[148,    40] loss: 1.468\n",
            "[148,    50] loss: 1.473\n",
            "[148,    60] loss: 1.472\n",
            "[148,    70] loss: 1.474\n",
            "[148,    80] loss: 1.473\n",
            "[148,    90] loss: 1.469\n",
            "[148,   100] loss: 1.475\n",
            "[148,   110] loss: 1.469\n",
            "[148,   120] loss: 1.473\n",
            "[148,   130] loss: 1.469\n",
            "[148,   140] loss: 1.470\n",
            "[148,   150] loss: 1.475\n",
            "[148,   160] loss: 1.472\n",
            "[148,   170] loss: 1.473\n",
            "[148,   180] loss: 1.475\n",
            "[148,   190] loss: 1.472\n",
            "[148,   200] loss: 1.475\n",
            "[148,   210] loss: 1.468\n",
            "[148,   220] loss: 1.471\n",
            "[148,   230] loss: 1.472\n",
            "[148,   240] loss: 1.472\n",
            "[148,   250] loss: 1.470\n",
            "[148,   260] loss: 1.476\n",
            "[148,   270] loss: 1.471\n",
            "[148,   280] loss: 1.469\n",
            "[148,   290] loss: 1.469\n",
            "[148,   300] loss: 1.469\n",
            "[148,   310] loss: 1.472\n",
            "[148,   320] loss: 1.467\n",
            "[148,   330] loss: 1.469\n",
            "[148,   340] loss: 1.468\n",
            "[148,   350] loss: 1.473\n",
            "[148,   360] loss: 1.473\n",
            "[148,   370] loss: 1.472\n",
            "[148,   380] loss: 1.474\n",
            "[148,   390] loss: 1.475\n",
            "[148,   400] loss: 1.469\n",
            "[148,   410] loss: 1.469\n",
            "[148,   420] loss: 1.470\n",
            "[148,   430] loss: 1.469\n",
            "epoch 147 val_loss 61.33476531505585 val_steps 40 val_acc 0.9342\n",
            "[149,    10] loss: 1.469\n",
            "[149,    20] loss: 1.468\n",
            "[149,    30] loss: 1.471\n",
            "[149,    40] loss: 1.473\n",
            "[149,    50] loss: 1.475\n",
            "[149,    60] loss: 1.473\n",
            "[149,    70] loss: 1.473\n",
            "[149,    80] loss: 1.469\n",
            "[149,    90] loss: 1.465\n",
            "[149,   100] loss: 1.471\n",
            "[149,   110] loss: 1.469\n",
            "[149,   120] loss: 1.474\n",
            "[149,   130] loss: 1.475\n",
            "[149,   140] loss: 1.471\n",
            "[149,   150] loss: 1.472\n",
            "[149,   160] loss: 1.471\n",
            "[149,   170] loss: 1.470\n",
            "[149,   180] loss: 1.468\n",
            "[149,   190] loss: 1.468\n",
            "[149,   200] loss: 1.473\n",
            "[149,   210] loss: 1.466\n",
            "[149,   220] loss: 1.473\n",
            "[149,   230] loss: 1.470\n",
            "[149,   240] loss: 1.473\n",
            "[149,   250] loss: 1.468\n",
            "[149,   260] loss: 1.477\n",
            "[149,   270] loss: 1.473\n",
            "[149,   280] loss: 1.474\n",
            "[149,   290] loss: 1.468\n",
            "[149,   300] loss: 1.473\n",
            "[149,   310] loss: 1.473\n",
            "[149,   320] loss: 1.479\n",
            "[149,   330] loss: 1.467\n",
            "[149,   340] loss: 1.468\n",
            "[149,   350] loss: 1.471\n",
            "[149,   360] loss: 1.471\n",
            "[149,   370] loss: 1.472\n",
            "[149,   380] loss: 1.478\n",
            "[149,   390] loss: 1.469\n",
            "[149,   400] loss: 1.475\n",
            "[149,   410] loss: 1.474\n",
            "[149,   420] loss: 1.469\n",
            "[149,   430] loss: 1.470\n",
            "epoch 148 val_loss 61.16260612010956 val_steps 40 val_acc 0.9338\n",
            "[150,    10] loss: 1.467\n",
            "[150,    20] loss: 1.471\n",
            "[150,    30] loss: 1.471\n",
            "[150,    40] loss: 1.470\n",
            "[150,    50] loss: 1.474\n",
            "[150,    60] loss: 1.477\n",
            "[150,    70] loss: 1.466\n",
            "[150,    80] loss: 1.470\n",
            "[150,    90] loss: 1.474\n",
            "[150,   100] loss: 1.477\n",
            "[150,   110] loss: 1.468\n",
            "[150,   120] loss: 1.474\n",
            "[150,   130] loss: 1.474\n",
            "[150,   140] loss: 1.472\n",
            "[150,   150] loss: 1.469\n",
            "[150,   160] loss: 1.469\n",
            "[150,   170] loss: 1.481\n",
            "[150,   180] loss: 1.471\n",
            "[150,   190] loss: 1.469\n",
            "[150,   200] loss: 1.471\n",
            "[150,   210] loss: 1.472\n",
            "[150,   220] loss: 1.469\n",
            "[150,   230] loss: 1.472\n",
            "[150,   240] loss: 1.472\n",
            "[150,   250] loss: 1.472\n",
            "[150,   260] loss: 1.474\n",
            "[150,   270] loss: 1.475\n",
            "[150,   280] loss: 1.472\n",
            "[150,   290] loss: 1.473\n",
            "[150,   300] loss: 1.472\n",
            "[150,   310] loss: 1.471\n",
            "[150,   320] loss: 1.467\n",
            "[150,   330] loss: 1.467\n",
            "[150,   340] loss: 1.467\n",
            "[150,   350] loss: 1.478\n",
            "[150,   360] loss: 1.475\n",
            "[150,   370] loss: 1.470\n",
            "[150,   380] loss: 1.469\n",
            "[150,   390] loss: 1.471\n",
            "[150,   400] loss: 1.468\n",
            "[150,   410] loss: 1.469\n",
            "[150,   420] loss: 1.473\n",
            "[150,   430] loss: 1.470\n",
            "epoch 149 val_loss 61.088698506355286 val_steps 40 val_acc 0.9342\n",
            "[151,    10] loss: 1.474\n",
            "[151,    20] loss: 1.472\n",
            "[151,    30] loss: 1.474\n",
            "[151,    40] loss: 1.474\n",
            "[151,    50] loss: 1.474\n",
            "[151,    60] loss: 1.471\n",
            "[151,    70] loss: 1.470\n",
            "[151,    80] loss: 1.469\n",
            "[151,    90] loss: 1.466\n",
            "[151,   100] loss: 1.471\n",
            "[151,   110] loss: 1.469\n",
            "[151,   120] loss: 1.472\n",
            "[151,   130] loss: 1.473\n",
            "[151,   140] loss: 1.473\n",
            "[151,   150] loss: 1.468\n",
            "[151,   160] loss: 1.472\n",
            "[151,   170] loss: 1.468\n",
            "[151,   180] loss: 1.473\n",
            "[151,   190] loss: 1.468\n",
            "[151,   200] loss: 1.473\n",
            "[151,   210] loss: 1.472\n",
            "[151,   220] loss: 1.469\n",
            "[151,   230] loss: 1.473\n",
            "[151,   240] loss: 1.473\n",
            "[151,   250] loss: 1.471\n",
            "[151,   260] loss: 1.473\n",
            "[151,   270] loss: 1.476\n",
            "[151,   280] loss: 1.475\n",
            "[151,   290] loss: 1.471\n",
            "[151,   300] loss: 1.469\n",
            "[151,   310] loss: 1.471\n",
            "[151,   320] loss: 1.467\n",
            "[151,   330] loss: 1.469\n",
            "[151,   340] loss: 1.470\n",
            "[151,   350] loss: 1.473\n",
            "[151,   360] loss: 1.469\n",
            "[151,   370] loss: 1.470\n",
            "[151,   380] loss: 1.472\n",
            "[151,   390] loss: 1.474\n",
            "[151,   400] loss: 1.470\n",
            "[151,   410] loss: 1.472\n",
            "[151,   420] loss: 1.473\n",
            "[151,   430] loss: 1.470\n",
            "epoch 150 val_loss 61.149367809295654 val_steps 40 val_acc 0.9318\n",
            "[152,    10] loss: 1.479\n",
            "[152,    20] loss: 1.472\n",
            "[152,    30] loss: 1.475\n",
            "[152,    40] loss: 1.469\n",
            "[152,    50] loss: 1.471\n",
            "[152,    60] loss: 1.477\n",
            "[152,    70] loss: 1.475\n",
            "[152,    80] loss: 1.467\n",
            "[152,    90] loss: 1.467\n",
            "[152,   100] loss: 1.467\n",
            "[152,   110] loss: 1.466\n",
            "[152,   120] loss: 1.474\n",
            "[152,   130] loss: 1.473\n",
            "[152,   140] loss: 1.471\n",
            "[152,   150] loss: 1.472\n",
            "[152,   160] loss: 1.470\n",
            "[152,   170] loss: 1.474\n",
            "[152,   180] loss: 1.471\n",
            "[152,   190] loss: 1.472\n",
            "[152,   200] loss: 1.469\n",
            "[152,   210] loss: 1.472\n",
            "[152,   220] loss: 1.472\n",
            "[152,   230] loss: 1.470\n",
            "[152,   240] loss: 1.472\n",
            "[152,   250] loss: 1.474\n",
            "[152,   260] loss: 1.476\n",
            "[152,   270] loss: 1.474\n",
            "[152,   280] loss: 1.470\n",
            "[152,   290] loss: 1.465\n",
            "[152,   300] loss: 1.477\n",
            "[152,   310] loss: 1.468\n",
            "[152,   320] loss: 1.474\n",
            "[152,   330] loss: 1.472\n",
            "[152,   340] loss: 1.469\n",
            "[152,   350] loss: 1.473\n",
            "[152,   360] loss: 1.469\n",
            "[152,   370] loss: 1.471\n",
            "[152,   380] loss: 1.472\n",
            "[152,   390] loss: 1.472\n",
            "[152,   400] loss: 1.466\n",
            "[152,   410] loss: 1.472\n",
            "[152,   420] loss: 1.472\n",
            "[152,   430] loss: 1.470\n",
            "epoch 151 val_loss 61.20392382144928 val_steps 40 val_acc 0.9334\n",
            "[153,    10] loss: 1.467\n",
            "[153,    20] loss: 1.472\n",
            "[153,    30] loss: 1.469\n",
            "[153,    40] loss: 1.469\n",
            "[153,    50] loss: 1.471\n",
            "[153,    60] loss: 1.472\n",
            "[153,    70] loss: 1.470\n",
            "[153,    80] loss: 1.469\n",
            "[153,    90] loss: 1.475\n",
            "[153,   100] loss: 1.472\n",
            "[153,   110] loss: 1.472\n",
            "[153,   120] loss: 1.467\n",
            "[153,   130] loss: 1.470\n",
            "[153,   140] loss: 1.471\n",
            "[153,   150] loss: 1.473\n",
            "[153,   160] loss: 1.472\n",
            "[153,   170] loss: 1.472\n",
            "[153,   180] loss: 1.480\n",
            "[153,   190] loss: 1.472\n",
            "[153,   200] loss: 1.469\n",
            "[153,   210] loss: 1.474\n",
            "[153,   220] loss: 1.468\n",
            "[153,   230] loss: 1.476\n",
            "[153,   240] loss: 1.474\n",
            "[153,   250] loss: 1.475\n",
            "[153,   260] loss: 1.468\n",
            "[153,   270] loss: 1.469\n",
            "[153,   280] loss: 1.468\n",
            "[153,   290] loss: 1.469\n",
            "[153,   300] loss: 1.472\n",
            "[153,   310] loss: 1.469\n",
            "[153,   320] loss: 1.475\n",
            "[153,   330] loss: 1.470\n",
            "[153,   340] loss: 1.475\n",
            "[153,   350] loss: 1.468\n",
            "[153,   360] loss: 1.472\n",
            "[153,   370] loss: 1.471\n",
            "[153,   380] loss: 1.469\n",
            "[153,   390] loss: 1.473\n",
            "[153,   400] loss: 1.467\n",
            "[153,   410] loss: 1.473\n",
            "[153,   420] loss: 1.474\n",
            "[153,   430] loss: 1.472\n",
            "epoch 152 val_loss 61.12088584899902 val_steps 40 val_acc 0.932\n",
            "[154,    10] loss: 1.478\n",
            "[154,    20] loss: 1.471\n",
            "[154,    30] loss: 1.472\n",
            "[154,    40] loss: 1.474\n",
            "[154,    50] loss: 1.473\n",
            "[154,    60] loss: 1.470\n",
            "[154,    70] loss: 1.470\n",
            "[154,    80] loss: 1.471\n",
            "[154,    90] loss: 1.471\n",
            "[154,   100] loss: 1.468\n",
            "[154,   110] loss: 1.475\n",
            "[154,   120] loss: 1.473\n",
            "[154,   130] loss: 1.470\n",
            "[154,   140] loss: 1.473\n",
            "[154,   150] loss: 1.471\n",
            "[154,   160] loss: 1.468\n",
            "[154,   170] loss: 1.468\n",
            "[154,   180] loss: 1.469\n",
            "[154,   190] loss: 1.471\n",
            "[154,   200] loss: 1.473\n",
            "[154,   210] loss: 1.476\n",
            "[154,   220] loss: 1.477\n",
            "[154,   230] loss: 1.471\n",
            "[154,   240] loss: 1.472\n",
            "[154,   250] loss: 1.470\n",
            "[154,   260] loss: 1.469\n",
            "[154,   270] loss: 1.467\n",
            "[154,   280] loss: 1.470\n",
            "[154,   290] loss: 1.468\n",
            "[154,   300] loss: 1.474\n",
            "[154,   310] loss: 1.470\n",
            "[154,   320] loss: 1.473\n",
            "[154,   330] loss: 1.469\n",
            "[154,   340] loss: 1.470\n",
            "[154,   350] loss: 1.468\n",
            "[154,   360] loss: 1.472\n",
            "[154,   370] loss: 1.471\n",
            "[154,   380] loss: 1.468\n",
            "[154,   390] loss: 1.471\n",
            "[154,   400] loss: 1.475\n",
            "[154,   410] loss: 1.471\n",
            "[154,   420] loss: 1.474\n",
            "[154,   430] loss: 1.469\n",
            "epoch 153 val_loss 61.31520926952362 val_steps 40 val_acc 0.934\n",
            "[155,    10] loss: 1.471\n",
            "[155,    20] loss: 1.469\n",
            "[155,    30] loss: 1.472\n",
            "[155,    40] loss: 1.468\n",
            "[155,    50] loss: 1.472\n",
            "[155,    60] loss: 1.467\n",
            "[155,    70] loss: 1.472\n",
            "[155,    80] loss: 1.468\n",
            "[155,    90] loss: 1.476\n",
            "[155,   100] loss: 1.472\n",
            "[155,   110] loss: 1.471\n",
            "[155,   120] loss: 1.469\n",
            "[155,   130] loss: 1.473\n",
            "[155,   140] loss: 1.470\n",
            "[155,   150] loss: 1.474\n",
            "[155,   160] loss: 1.470\n",
            "[155,   170] loss: 1.467\n",
            "[155,   180] loss: 1.471\n",
            "[155,   190] loss: 1.473\n",
            "[155,   200] loss: 1.471\n",
            "[155,   210] loss: 1.479\n",
            "[155,   220] loss: 1.471\n",
            "[155,   230] loss: 1.468\n",
            "[155,   240] loss: 1.473\n",
            "[155,   250] loss: 1.471\n",
            "[155,   260] loss: 1.474\n",
            "[155,   270] loss: 1.472\n",
            "[155,   280] loss: 1.472\n",
            "[155,   290] loss: 1.478\n",
            "[155,   300] loss: 1.475\n",
            "[155,   310] loss: 1.470\n",
            "[155,   320] loss: 1.469\n",
            "[155,   330] loss: 1.473\n",
            "[155,   340] loss: 1.469\n",
            "[155,   350] loss: 1.470\n",
            "[155,   360] loss: 1.474\n",
            "[155,   370] loss: 1.471\n",
            "[155,   380] loss: 1.472\n",
            "[155,   390] loss: 1.467\n",
            "[155,   400] loss: 1.472\n",
            "[155,   410] loss: 1.472\n",
            "[155,   420] loss: 1.470\n",
            "[155,   430] loss: 1.470\n",
            "epoch 154 val_loss 61.462775349617004 val_steps 40 val_acc 0.9314\n",
            "[156,    10] loss: 1.470\n",
            "[156,    20] loss: 1.470\n",
            "[156,    30] loss: 1.475\n",
            "[156,    40] loss: 1.467\n",
            "[156,    50] loss: 1.472\n",
            "[156,    60] loss: 1.475\n",
            "[156,    70] loss: 1.471\n",
            "[156,    80] loss: 1.472\n",
            "[156,    90] loss: 1.473\n",
            "[156,   100] loss: 1.476\n",
            "[156,   110] loss: 1.470\n",
            "[156,   120] loss: 1.472\n",
            "[156,   130] loss: 1.472\n",
            "[156,   140] loss: 1.472\n",
            "[156,   150] loss: 1.471\n",
            "[156,   160] loss: 1.475\n",
            "[156,   170] loss: 1.476\n",
            "[156,   180] loss: 1.475\n",
            "[156,   190] loss: 1.472\n",
            "[156,   200] loss: 1.471\n",
            "[156,   210] loss: 1.474\n",
            "[156,   220] loss: 1.468\n",
            "[156,   230] loss: 1.466\n",
            "[156,   240] loss: 1.469\n",
            "[156,   250] loss: 1.474\n",
            "[156,   260] loss: 1.470\n",
            "[156,   270] loss: 1.470\n",
            "[156,   280] loss: 1.469\n",
            "[156,   290] loss: 1.473\n",
            "[156,   300] loss: 1.470\n",
            "[156,   310] loss: 1.471\n",
            "[156,   320] loss: 1.472\n",
            "[156,   330] loss: 1.474\n",
            "[156,   340] loss: 1.468\n",
            "[156,   350] loss: 1.468\n",
            "[156,   360] loss: 1.474\n",
            "[156,   370] loss: 1.470\n",
            "[156,   380] loss: 1.469\n",
            "[156,   390] loss: 1.470\n",
            "[156,   400] loss: 1.470\n",
            "[156,   410] loss: 1.473\n",
            "[156,   420] loss: 1.467\n",
            "[156,   430] loss: 1.470\n",
            "epoch 155 val_loss 61.23274219036102 val_steps 40 val_acc 0.9336\n",
            "[157,    10] loss: 1.474\n",
            "[157,    20] loss: 1.472\n",
            "[157,    30] loss: 1.472\n",
            "[157,    40] loss: 1.469\n",
            "[157,    50] loss: 1.471\n",
            "[157,    60] loss: 1.473\n",
            "[157,    70] loss: 1.472\n",
            "[157,    80] loss: 1.470\n",
            "[157,    90] loss: 1.476\n",
            "[157,   100] loss: 1.472\n",
            "[157,   110] loss: 1.470\n",
            "[157,   120] loss: 1.469\n",
            "[157,   130] loss: 1.470\n",
            "[157,   140] loss: 1.468\n",
            "[157,   150] loss: 1.469\n",
            "[157,   160] loss: 1.467\n",
            "[157,   170] loss: 1.466\n",
            "[157,   180] loss: 1.472\n",
            "[157,   190] loss: 1.472\n",
            "[157,   200] loss: 1.468\n",
            "[157,   210] loss: 1.473\n",
            "[157,   220] loss: 1.468\n",
            "[157,   230] loss: 1.473\n",
            "[157,   240] loss: 1.474\n",
            "[157,   250] loss: 1.474\n",
            "[157,   260] loss: 1.476\n",
            "[157,   270] loss: 1.476\n",
            "[157,   280] loss: 1.475\n",
            "[157,   290] loss: 1.473\n",
            "[157,   300] loss: 1.473\n",
            "[157,   310] loss: 1.467\n",
            "[157,   320] loss: 1.471\n",
            "[157,   330] loss: 1.467\n",
            "[157,   340] loss: 1.475\n",
            "[157,   350] loss: 1.466\n",
            "[157,   360] loss: 1.472\n",
            "[157,   370] loss: 1.472\n",
            "[157,   380] loss: 1.472\n",
            "[157,   390] loss: 1.468\n",
            "[157,   400] loss: 1.471\n",
            "[157,   410] loss: 1.470\n",
            "[157,   420] loss: 1.475\n",
            "[157,   430] loss: 1.472\n",
            "epoch 156 val_loss 61.16034197807312 val_steps 40 val_acc 0.9358\n",
            "[158,    10] loss: 1.470\n",
            "[158,    20] loss: 1.472\n",
            "[158,    30] loss: 1.469\n",
            "[158,    40] loss: 1.472\n",
            "[158,    50] loss: 1.469\n",
            "[158,    60] loss: 1.472\n",
            "[158,    70] loss: 1.468\n",
            "[158,    80] loss: 1.471\n",
            "[158,    90] loss: 1.473\n",
            "[158,   100] loss: 1.472\n",
            "[158,   110] loss: 1.468\n",
            "[158,   120] loss: 1.470\n",
            "[158,   130] loss: 1.471\n",
            "[158,   140] loss: 1.468\n",
            "[158,   150] loss: 1.468\n",
            "[158,   160] loss: 1.472\n",
            "[158,   170] loss: 1.471\n",
            "[158,   180] loss: 1.469\n",
            "[158,   190] loss: 1.472\n",
            "[158,   200] loss: 1.473\n",
            "[158,   210] loss: 1.475\n",
            "[158,   220] loss: 1.474\n",
            "[158,   230] loss: 1.470\n",
            "[158,   240] loss: 1.470\n",
            "[158,   250] loss: 1.475\n",
            "[158,   260] loss: 1.470\n",
            "[158,   270] loss: 1.476\n",
            "[158,   280] loss: 1.467\n",
            "[158,   290] loss: 1.476\n",
            "[158,   300] loss: 1.477\n",
            "[158,   310] loss: 1.473\n",
            "[158,   320] loss: 1.471\n",
            "[158,   330] loss: 1.471\n",
            "[158,   340] loss: 1.474\n",
            "[158,   350] loss: 1.469\n",
            "[158,   360] loss: 1.474\n",
            "[158,   370] loss: 1.473\n",
            "[158,   380] loss: 1.468\n",
            "[158,   390] loss: 1.472\n",
            "[158,   400] loss: 1.468\n",
            "[158,   410] loss: 1.471\n",
            "[158,   420] loss: 1.470\n",
            "[158,   430] loss: 1.475\n",
            "epoch 157 val_loss 61.10666596889496 val_steps 40 val_acc 0.9364\n",
            "[159,    10] loss: 1.474\n",
            "[159,    20] loss: 1.470\n",
            "[159,    30] loss: 1.479\n",
            "[159,    40] loss: 1.473\n",
            "[159,    50] loss: 1.470\n",
            "[159,    60] loss: 1.467\n",
            "[159,    70] loss: 1.473\n",
            "[159,    80] loss: 1.466\n",
            "[159,    90] loss: 1.472\n",
            "[159,   100] loss: 1.477\n",
            "[159,   110] loss: 1.474\n",
            "[159,   120] loss: 1.469\n",
            "[159,   130] loss: 1.470\n",
            "[159,   140] loss: 1.471\n",
            "[159,   150] loss: 1.470\n",
            "[159,   160] loss: 1.474\n",
            "[159,   170] loss: 1.469\n",
            "[159,   180] loss: 1.472\n",
            "[159,   190] loss: 1.470\n",
            "[159,   200] loss: 1.470\n",
            "[159,   210] loss: 1.470\n",
            "[159,   220] loss: 1.473\n",
            "[159,   230] loss: 1.469\n",
            "[159,   240] loss: 1.469\n",
            "[159,   250] loss: 1.476\n",
            "[159,   260] loss: 1.475\n",
            "[159,   270] loss: 1.469\n",
            "[159,   280] loss: 1.474\n",
            "[159,   290] loss: 1.469\n",
            "[159,   300] loss: 1.469\n",
            "[159,   310] loss: 1.473\n",
            "[159,   320] loss: 1.470\n",
            "[159,   330] loss: 1.473\n",
            "[159,   340] loss: 1.468\n",
            "[159,   350] loss: 1.470\n",
            "[159,   360] loss: 1.472\n",
            "[159,   370] loss: 1.467\n",
            "[159,   380] loss: 1.472\n",
            "[159,   390] loss: 1.472\n",
            "[159,   400] loss: 1.472\n",
            "[159,   410] loss: 1.474\n",
            "[159,   420] loss: 1.469\n",
            "[159,   430] loss: 1.474\n",
            "epoch 158 val_loss 61.15927290916443 val_steps 40 val_acc 0.9336\n",
            "[160,    10] loss: 1.467\n",
            "[160,    20] loss: 1.472\n",
            "[160,    30] loss: 1.471\n",
            "[160,    40] loss: 1.470\n",
            "[160,    50] loss: 1.469\n",
            "[160,    60] loss: 1.473\n",
            "[160,    70] loss: 1.471\n",
            "[160,    80] loss: 1.471\n",
            "[160,    90] loss: 1.474\n",
            "[160,   100] loss: 1.469\n",
            "[160,   110] loss: 1.473\n",
            "[160,   120] loss: 1.473\n",
            "[160,   130] loss: 1.470\n",
            "[160,   140] loss: 1.470\n",
            "[160,   150] loss: 1.474\n",
            "[160,   160] loss: 1.472\n",
            "[160,   170] loss: 1.468\n",
            "[160,   180] loss: 1.471\n",
            "[160,   190] loss: 1.471\n",
            "[160,   200] loss: 1.472\n",
            "[160,   210] loss: 1.469\n",
            "[160,   220] loss: 1.470\n",
            "[160,   230] loss: 1.474\n",
            "[160,   240] loss: 1.470\n",
            "[160,   250] loss: 1.474\n",
            "[160,   260] loss: 1.469\n",
            "[160,   270] loss: 1.469\n",
            "[160,   280] loss: 1.468\n",
            "[160,   290] loss: 1.479\n",
            "[160,   300] loss: 1.469\n",
            "[160,   310] loss: 1.470\n",
            "[160,   320] loss: 1.467\n",
            "[160,   330] loss: 1.468\n",
            "[160,   340] loss: 1.473\n",
            "[160,   350] loss: 1.477\n",
            "[160,   360] loss: 1.475\n",
            "[160,   370] loss: 1.472\n",
            "[160,   380] loss: 1.473\n",
            "[160,   390] loss: 1.472\n",
            "[160,   400] loss: 1.472\n",
            "[160,   410] loss: 1.469\n",
            "[160,   420] loss: 1.472\n",
            "[160,   430] loss: 1.474\n",
            "epoch 159 val_loss 61.1939138174057 val_steps 40 val_acc 0.9318\n",
            "[161,    10] loss: 1.475\n",
            "[161,    20] loss: 1.472\n",
            "[161,    30] loss: 1.472\n",
            "[161,    40] loss: 1.471\n",
            "[161,    50] loss: 1.471\n",
            "[161,    60] loss: 1.472\n",
            "[161,    70] loss: 1.475\n",
            "[161,    80] loss: 1.474\n",
            "[161,    90] loss: 1.471\n",
            "[161,   100] loss: 1.475\n",
            "[161,   110] loss: 1.471\n",
            "[161,   120] loss: 1.472\n",
            "[161,   130] loss: 1.468\n",
            "[161,   140] loss: 1.476\n",
            "[161,   150] loss: 1.472\n",
            "[161,   160] loss: 1.471\n",
            "[161,   170] loss: 1.470\n",
            "[161,   180] loss: 1.469\n",
            "[161,   190] loss: 1.469\n",
            "[161,   200] loss: 1.471\n",
            "[161,   210] loss: 1.476\n",
            "[161,   220] loss: 1.473\n",
            "[161,   230] loss: 1.469\n",
            "[161,   240] loss: 1.473\n",
            "[161,   250] loss: 1.471\n",
            "[161,   260] loss: 1.469\n",
            "[161,   270] loss: 1.468\n",
            "[161,   280] loss: 1.474\n",
            "[161,   290] loss: 1.467\n",
            "[161,   300] loss: 1.470\n",
            "[161,   310] loss: 1.473\n",
            "[161,   320] loss: 1.471\n",
            "[161,   330] loss: 1.470\n",
            "[161,   340] loss: 1.467\n",
            "[161,   350] loss: 1.469\n",
            "[161,   360] loss: 1.477\n",
            "[161,   370] loss: 1.472\n",
            "[161,   380] loss: 1.465\n",
            "[161,   390] loss: 1.470\n",
            "[161,   400] loss: 1.469\n",
            "[161,   410] loss: 1.471\n",
            "[161,   420] loss: 1.468\n",
            "[161,   430] loss: 1.470\n",
            "epoch 160 val_loss 61.12228035926819 val_steps 40 val_acc 0.934\n",
            "[162,    10] loss: 1.467\n",
            "[162,    20] loss: 1.474\n",
            "[162,    30] loss: 1.472\n",
            "[162,    40] loss: 1.466\n",
            "[162,    50] loss: 1.469\n",
            "[162,    60] loss: 1.473\n",
            "[162,    70] loss: 1.474\n",
            "[162,    80] loss: 1.468\n",
            "[162,    90] loss: 1.474\n",
            "[162,   100] loss: 1.468\n",
            "[162,   110] loss: 1.475\n",
            "[162,   120] loss: 1.469\n",
            "[162,   130] loss: 1.472\n",
            "[162,   140] loss: 1.477\n",
            "[162,   150] loss: 1.472\n",
            "[162,   160] loss: 1.472\n",
            "[162,   170] loss: 1.479\n",
            "[162,   180] loss: 1.471\n",
            "[162,   190] loss: 1.472\n",
            "[162,   200] loss: 1.473\n",
            "[162,   210] loss: 1.474\n",
            "[162,   220] loss: 1.469\n",
            "[162,   230] loss: 1.473\n",
            "[162,   240] loss: 1.469\n",
            "[162,   250] loss: 1.469\n",
            "[162,   260] loss: 1.477\n",
            "[162,   270] loss: 1.469\n",
            "[162,   280] loss: 1.473\n",
            "[162,   290] loss: 1.472\n",
            "[162,   300] loss: 1.468\n",
            "[162,   310] loss: 1.473\n",
            "[162,   320] loss: 1.471\n",
            "[162,   330] loss: 1.470\n",
            "[162,   340] loss: 1.466\n",
            "[162,   350] loss: 1.474\n",
            "[162,   360] loss: 1.476\n",
            "[162,   370] loss: 1.471\n",
            "[162,   380] loss: 1.469\n",
            "[162,   390] loss: 1.469\n",
            "[162,   400] loss: 1.470\n",
            "[162,   410] loss: 1.468\n",
            "[162,   420] loss: 1.471\n",
            "[162,   430] loss: 1.468\n",
            "epoch 161 val_loss 61.08611524105072 val_steps 40 val_acc 0.9338\n",
            "[163,    10] loss: 1.471\n",
            "[163,    20] loss: 1.469\n",
            "[163,    30] loss: 1.470\n",
            "[163,    40] loss: 1.479\n",
            "[163,    50] loss: 1.473\n",
            "[163,    60] loss: 1.473\n",
            "[163,    70] loss: 1.471\n",
            "[163,    80] loss: 1.469\n",
            "[163,    90] loss: 1.470\n",
            "[163,   100] loss: 1.473\n",
            "[163,   110] loss: 1.469\n",
            "[163,   120] loss: 1.470\n",
            "[163,   130] loss: 1.474\n",
            "[163,   140] loss: 1.471\n",
            "[163,   150] loss: 1.468\n",
            "[163,   160] loss: 1.472\n",
            "[163,   170] loss: 1.471\n",
            "[163,   180] loss: 1.467\n",
            "[163,   190] loss: 1.476\n",
            "[163,   200] loss: 1.472\n",
            "[163,   210] loss: 1.468\n",
            "[163,   220] loss: 1.469\n",
            "[163,   230] loss: 1.472\n",
            "[163,   240] loss: 1.469\n",
            "[163,   250] loss: 1.471\n",
            "[163,   260] loss: 1.471\n",
            "[163,   270] loss: 1.472\n",
            "[163,   280] loss: 1.467\n",
            "[163,   290] loss: 1.477\n",
            "[163,   300] loss: 1.471\n",
            "[163,   310] loss: 1.469\n",
            "[163,   320] loss: 1.471\n",
            "[163,   330] loss: 1.471\n",
            "[163,   340] loss: 1.471\n",
            "[163,   350] loss: 1.473\n",
            "[163,   360] loss: 1.476\n",
            "[163,   370] loss: 1.467\n",
            "[163,   380] loss: 1.473\n",
            "[163,   390] loss: 1.475\n",
            "[163,   400] loss: 1.468\n",
            "[163,   410] loss: 1.472\n",
            "[163,   420] loss: 1.472\n",
            "[163,   430] loss: 1.472\n",
            "epoch 162 val_loss 61.132678270339966 val_steps 40 val_acc 0.9334\n",
            "[164,    10] loss: 1.472\n",
            "[164,    20] loss: 1.468\n",
            "[164,    30] loss: 1.472\n",
            "[164,    40] loss: 1.469\n",
            "[164,    50] loss: 1.469\n",
            "[164,    60] loss: 1.471\n",
            "[164,    70] loss: 1.473\n",
            "[164,    80] loss: 1.472\n",
            "[164,    90] loss: 1.472\n",
            "[164,   100] loss: 1.471\n",
            "[164,   110] loss: 1.471\n",
            "[164,   120] loss: 1.467\n",
            "[164,   130] loss: 1.470\n",
            "[164,   140] loss: 1.472\n",
            "[164,   150] loss: 1.471\n",
            "[164,   160] loss: 1.479\n",
            "[164,   170] loss: 1.467\n",
            "[164,   180] loss: 1.471\n",
            "[164,   190] loss: 1.470\n",
            "[164,   200] loss: 1.469\n",
            "[164,   210] loss: 1.469\n",
            "[164,   220] loss: 1.466\n",
            "[164,   230] loss: 1.468\n",
            "[164,   240] loss: 1.470\n",
            "[164,   250] loss: 1.471\n",
            "[164,   260] loss: 1.472\n",
            "[164,   270] loss: 1.476\n",
            "[164,   280] loss: 1.469\n",
            "[164,   290] loss: 1.474\n",
            "[164,   300] loss: 1.473\n",
            "[164,   310] loss: 1.475\n",
            "[164,   320] loss: 1.473\n",
            "[164,   330] loss: 1.474\n",
            "[164,   340] loss: 1.476\n",
            "[164,   350] loss: 1.472\n",
            "[164,   360] loss: 1.471\n",
            "[164,   370] loss: 1.475\n",
            "[164,   380] loss: 1.471\n",
            "[164,   390] loss: 1.471\n",
            "[164,   400] loss: 1.467\n",
            "[164,   410] loss: 1.470\n",
            "[164,   420] loss: 1.470\n",
            "[164,   430] loss: 1.476\n",
            "epoch 163 val_loss 61.371081471443176 val_steps 40 val_acc 0.9338\n",
            "[165,    10] loss: 1.472\n",
            "[165,    20] loss: 1.475\n",
            "[165,    30] loss: 1.471\n",
            "[165,    40] loss: 1.471\n",
            "[165,    50] loss: 1.468\n",
            "[165,    60] loss: 1.473\n",
            "[165,    70] loss: 1.474\n",
            "[165,    80] loss: 1.471\n",
            "[165,    90] loss: 1.465\n",
            "[165,   100] loss: 1.475\n",
            "[165,   110] loss: 1.468\n",
            "[165,   120] loss: 1.471\n",
            "[165,   130] loss: 1.473\n",
            "[165,   140] loss: 1.470\n",
            "[165,   150] loss: 1.470\n",
            "[165,   160] loss: 1.474\n",
            "[165,   170] loss: 1.473\n",
            "[165,   180] loss: 1.470\n",
            "[165,   190] loss: 1.472\n",
            "[165,   200] loss: 1.475\n",
            "[165,   210] loss: 1.467\n",
            "[165,   220] loss: 1.469\n",
            "[165,   230] loss: 1.473\n",
            "[165,   240] loss: 1.475\n",
            "[165,   250] loss: 1.473\n",
            "[165,   260] loss: 1.470\n",
            "[165,   270] loss: 1.470\n",
            "[165,   280] loss: 1.474\n",
            "[165,   290] loss: 1.473\n",
            "[165,   300] loss: 1.473\n",
            "[165,   310] loss: 1.466\n",
            "[165,   320] loss: 1.471\n",
            "[165,   330] loss: 1.474\n",
            "[165,   340] loss: 1.473\n",
            "[165,   350] loss: 1.471\n",
            "[165,   360] loss: 1.469\n",
            "[165,   370] loss: 1.472\n",
            "[165,   380] loss: 1.468\n",
            "[165,   390] loss: 1.472\n",
            "[165,   400] loss: 1.475\n",
            "[165,   410] loss: 1.468\n",
            "[165,   420] loss: 1.470\n",
            "[165,   430] loss: 1.468\n",
            "epoch 164 val_loss 61.33808672428131 val_steps 40 val_acc 0.9332\n",
            "[166,    10] loss: 1.472\n",
            "[166,    20] loss: 1.472\n",
            "[166,    30] loss: 1.468\n",
            "[166,    40] loss: 1.469\n",
            "[166,    50] loss: 1.469\n",
            "[166,    60] loss: 1.474\n",
            "[166,    70] loss: 1.471\n",
            "[166,    80] loss: 1.473\n",
            "[166,    90] loss: 1.469\n",
            "[166,   100] loss: 1.469\n",
            "[166,   110] loss: 1.469\n",
            "[166,   120] loss: 1.473\n",
            "[166,   130] loss: 1.472\n",
            "[166,   140] loss: 1.470\n",
            "[166,   150] loss: 1.475\n",
            "[166,   160] loss: 1.472\n",
            "[166,   170] loss: 1.469\n",
            "[166,   180] loss: 1.473\n",
            "[166,   190] loss: 1.471\n",
            "[166,   200] loss: 1.476\n",
            "[166,   210] loss: 1.473\n",
            "[166,   220] loss: 1.472\n",
            "[166,   230] loss: 1.474\n",
            "[166,   240] loss: 1.470\n",
            "[166,   250] loss: 1.470\n",
            "[166,   260] loss: 1.469\n",
            "[166,   270] loss: 1.471\n",
            "[166,   280] loss: 1.469\n",
            "[166,   290] loss: 1.469\n",
            "[166,   300] loss: 1.469\n",
            "[166,   310] loss: 1.476\n",
            "[166,   320] loss: 1.471\n",
            "[166,   330] loss: 1.472\n",
            "[166,   340] loss: 1.470\n",
            "[166,   350] loss: 1.475\n",
            "[166,   360] loss: 1.468\n",
            "[166,   370] loss: 1.472\n",
            "[166,   380] loss: 1.471\n",
            "[166,   390] loss: 1.472\n",
            "[166,   400] loss: 1.470\n",
            "[166,   410] loss: 1.469\n",
            "[166,   420] loss: 1.478\n",
            "[166,   430] loss: 1.469\n",
            "epoch 165 val_loss 61.09104239940643 val_steps 40 val_acc 0.9348\n",
            "[167,    10] loss: 1.475\n",
            "[167,    20] loss: 1.467\n",
            "[167,    30] loss: 1.467\n",
            "[167,    40] loss: 1.471\n",
            "[167,    50] loss: 1.468\n",
            "[167,    60] loss: 1.470\n",
            "[167,    70] loss: 1.470\n",
            "[167,    80] loss: 1.467\n",
            "[167,    90] loss: 1.468\n",
            "[167,   100] loss: 1.472\n",
            "[167,   110] loss: 1.474\n",
            "[167,   120] loss: 1.470\n",
            "[167,   130] loss: 1.473\n",
            "[167,   140] loss: 1.469\n",
            "[167,   150] loss: 1.466\n",
            "[167,   160] loss: 1.473\n",
            "[167,   170] loss: 1.473\n",
            "[167,   180] loss: 1.473\n",
            "[167,   190] loss: 1.467\n",
            "[167,   200] loss: 1.469\n",
            "[167,   210] loss: 1.474\n",
            "[167,   220] loss: 1.468\n",
            "[167,   230] loss: 1.470\n",
            "[167,   240] loss: 1.474\n",
            "[167,   250] loss: 1.473\n",
            "[167,   260] loss: 1.469\n",
            "[167,   270] loss: 1.473\n",
            "[167,   280] loss: 1.476\n",
            "[167,   290] loss: 1.472\n",
            "[167,   300] loss: 1.472\n",
            "[167,   310] loss: 1.470\n",
            "[167,   320] loss: 1.471\n",
            "[167,   330] loss: 1.470\n",
            "[167,   340] loss: 1.471\n",
            "[167,   350] loss: 1.472\n",
            "[167,   360] loss: 1.473\n",
            "[167,   370] loss: 1.472\n",
            "[167,   380] loss: 1.472\n",
            "[167,   390] loss: 1.474\n",
            "[167,   400] loss: 1.475\n",
            "[167,   410] loss: 1.471\n",
            "[167,   420] loss: 1.473\n",
            "[167,   430] loss: 1.474\n",
            "epoch 166 val_loss 61.23034060001373 val_steps 40 val_acc 0.934\n",
            "[168,    10] loss: 1.475\n",
            "[168,    20] loss: 1.472\n",
            "[168,    30] loss: 1.471\n",
            "[168,    40] loss: 1.470\n",
            "[168,    50] loss: 1.469\n",
            "[168,    60] loss: 1.471\n",
            "[168,    70] loss: 1.471\n",
            "[168,    80] loss: 1.470\n",
            "[168,    90] loss: 1.468\n",
            "[168,   100] loss: 1.472\n",
            "[168,   110] loss: 1.475\n",
            "[168,   120] loss: 1.465\n",
            "[168,   130] loss: 1.469\n",
            "[168,   140] loss: 1.466\n",
            "[168,   150] loss: 1.472\n",
            "[168,   160] loss: 1.469\n",
            "[168,   170] loss: 1.471\n",
            "[168,   180] loss: 1.475\n",
            "[168,   190] loss: 1.476\n",
            "[168,   200] loss: 1.470\n",
            "[168,   210] loss: 1.474\n",
            "[168,   220] loss: 1.472\n",
            "[168,   230] loss: 1.469\n",
            "[168,   240] loss: 1.469\n",
            "[168,   250] loss: 1.471\n",
            "[168,   260] loss: 1.471\n",
            "[168,   270] loss: 1.471\n",
            "[168,   280] loss: 1.469\n",
            "[168,   290] loss: 1.474\n",
            "[168,   300] loss: 1.472\n",
            "[168,   310] loss: 1.475\n",
            "[168,   320] loss: 1.473\n",
            "[168,   330] loss: 1.473\n",
            "[168,   340] loss: 1.470\n",
            "[168,   350] loss: 1.471\n",
            "[168,   360] loss: 1.468\n",
            "[168,   370] loss: 1.468\n",
            "[168,   380] loss: 1.469\n",
            "[168,   390] loss: 1.469\n",
            "[168,   400] loss: 1.477\n",
            "[168,   410] loss: 1.477\n",
            "[168,   420] loss: 1.475\n",
            "[168,   430] loss: 1.474\n",
            "epoch 167 val_loss 61.10278677940369 val_steps 40 val_acc 0.9344\n",
            "[169,    10] loss: 1.469\n",
            "[169,    20] loss: 1.471\n",
            "[169,    30] loss: 1.473\n",
            "[169,    40] loss: 1.472\n",
            "[169,    50] loss: 1.468\n",
            "[169,    60] loss: 1.473\n",
            "[169,    70] loss: 1.471\n",
            "[169,    80] loss: 1.468\n",
            "[169,    90] loss: 1.468\n",
            "[169,   100] loss: 1.473\n",
            "[169,   110] loss: 1.474\n",
            "[169,   120] loss: 1.471\n",
            "[169,   130] loss: 1.472\n",
            "[169,   140] loss: 1.473\n",
            "[169,   150] loss: 1.473\n",
            "[169,   160] loss: 1.474\n",
            "[169,   170] loss: 1.476\n",
            "[169,   180] loss: 1.472\n",
            "[169,   190] loss: 1.473\n",
            "[169,   200] loss: 1.474\n",
            "[169,   210] loss: 1.469\n",
            "[169,   220] loss: 1.468\n",
            "[169,   230] loss: 1.475\n",
            "[169,   240] loss: 1.471\n",
            "[169,   250] loss: 1.468\n",
            "[169,   260] loss: 1.475\n",
            "[169,   270] loss: 1.468\n",
            "[169,   280] loss: 1.471\n",
            "[169,   290] loss: 1.475\n",
            "[169,   300] loss: 1.471\n",
            "[169,   310] loss: 1.473\n",
            "[169,   320] loss: 1.466\n",
            "[169,   330] loss: 1.466\n",
            "[169,   340] loss: 1.478\n",
            "[169,   350] loss: 1.470\n",
            "[169,   360] loss: 1.473\n",
            "[169,   370] loss: 1.471\n",
            "[169,   380] loss: 1.470\n",
            "[169,   390] loss: 1.470\n",
            "[169,   400] loss: 1.470\n",
            "[169,   410] loss: 1.469\n",
            "[169,   420] loss: 1.469\n",
            "[169,   430] loss: 1.469\n",
            "epoch 168 val_loss 61.07177984714508 val_steps 40 val_acc 0.9342\n",
            "[170,    10] loss: 1.467\n",
            "[170,    20] loss: 1.473\n",
            "[170,    30] loss: 1.470\n",
            "[170,    40] loss: 1.470\n",
            "[170,    50] loss: 1.471\n",
            "[170,    60] loss: 1.467\n",
            "[170,    70] loss: 1.471\n",
            "[170,    80] loss: 1.470\n",
            "[170,    90] loss: 1.473\n",
            "[170,   100] loss: 1.472\n",
            "[170,   110] loss: 1.472\n",
            "[170,   120] loss: 1.474\n",
            "[170,   130] loss: 1.469\n",
            "[170,   140] loss: 1.472\n",
            "[170,   150] loss: 1.471\n",
            "[170,   160] loss: 1.469\n",
            "[170,   170] loss: 1.471\n",
            "[170,   180] loss: 1.468\n",
            "[170,   190] loss: 1.469\n",
            "[170,   200] loss: 1.469\n",
            "[170,   210] loss: 1.474\n",
            "[170,   220] loss: 1.470\n",
            "[170,   230] loss: 1.474\n",
            "[170,   240] loss: 1.476\n",
            "[170,   250] loss: 1.468\n",
            "[170,   260] loss: 1.470\n",
            "[170,   270] loss: 1.475\n",
            "[170,   280] loss: 1.471\n",
            "[170,   290] loss: 1.471\n",
            "[170,   300] loss: 1.471\n",
            "[170,   310] loss: 1.472\n",
            "[170,   320] loss: 1.471\n",
            "[170,   330] loss: 1.470\n",
            "[170,   340] loss: 1.472\n",
            "[170,   350] loss: 1.475\n",
            "[170,   360] loss: 1.472\n",
            "[170,   370] loss: 1.472\n",
            "[170,   380] loss: 1.473\n",
            "[170,   390] loss: 1.469\n",
            "[170,   400] loss: 1.470\n",
            "[170,   410] loss: 1.472\n",
            "[170,   420] loss: 1.471\n",
            "[170,   430] loss: 1.473\n",
            "epoch 169 val_loss 61.2425457239151 val_steps 40 val_acc 0.9332\n",
            "[171,    10] loss: 1.471\n",
            "[171,    20] loss: 1.471\n",
            "[171,    30] loss: 1.470\n",
            "[171,    40] loss: 1.469\n",
            "[171,    50] loss: 1.475\n",
            "[171,    60] loss: 1.473\n",
            "[171,    70] loss: 1.471\n",
            "[171,    80] loss: 1.471\n",
            "[171,    90] loss: 1.472\n",
            "[171,   100] loss: 1.475\n",
            "[171,   110] loss: 1.473\n",
            "[171,   120] loss: 1.472\n",
            "[171,   130] loss: 1.469\n",
            "[171,   140] loss: 1.472\n",
            "[171,   150] loss: 1.471\n",
            "[171,   160] loss: 1.472\n",
            "[171,   170] loss: 1.472\n",
            "[171,   180] loss: 1.468\n",
            "[171,   190] loss: 1.472\n",
            "[171,   200] loss: 1.473\n",
            "[171,   210] loss: 1.469\n",
            "[171,   220] loss: 1.476\n",
            "[171,   230] loss: 1.469\n",
            "[171,   240] loss: 1.468\n",
            "[171,   250] loss: 1.475\n",
            "[171,   260] loss: 1.472\n",
            "[171,   270] loss: 1.470\n",
            "[171,   280] loss: 1.471\n",
            "[171,   290] loss: 1.470\n",
            "[171,   300] loss: 1.470\n",
            "[171,   310] loss: 1.471\n",
            "[171,   320] loss: 1.474\n",
            "[171,   330] loss: 1.469\n",
            "[171,   340] loss: 1.475\n",
            "[171,   350] loss: 1.468\n",
            "[171,   360] loss: 1.468\n",
            "[171,   370] loss: 1.472\n",
            "[171,   380] loss: 1.472\n",
            "[171,   390] loss: 1.472\n",
            "[171,   400] loss: 1.471\n",
            "[171,   410] loss: 1.465\n",
            "[171,   420] loss: 1.467\n",
            "[171,   430] loss: 1.476\n",
            "epoch 170 val_loss 61.06844174861908 val_steps 40 val_acc 0.9344\n",
            "[172,    10] loss: 1.469\n",
            "[172,    20] loss: 1.478\n",
            "[172,    30] loss: 1.475\n",
            "[172,    40] loss: 1.476\n",
            "[172,    50] loss: 1.474\n",
            "[172,    60] loss: 1.473\n",
            "[172,    70] loss: 1.471\n",
            "[172,    80] loss: 1.468\n",
            "[172,    90] loss: 1.472\n",
            "[172,   100] loss: 1.468\n",
            "[172,   110] loss: 1.470\n",
            "[172,   120] loss: 1.470\n",
            "[172,   130] loss: 1.474\n",
            "[172,   140] loss: 1.472\n",
            "[172,   150] loss: 1.469\n",
            "[172,   160] loss: 1.472\n",
            "[172,   170] loss: 1.467\n",
            "[172,   180] loss: 1.470\n",
            "[172,   190] loss: 1.469\n",
            "[172,   200] loss: 1.474\n",
            "[172,   210] loss: 1.470\n",
            "[172,   220] loss: 1.468\n",
            "[172,   230] loss: 1.474\n",
            "[172,   240] loss: 1.474\n",
            "[172,   250] loss: 1.471\n",
            "[172,   260] loss: 1.469\n",
            "[172,   270] loss: 1.469\n",
            "[172,   280] loss: 1.474\n",
            "[172,   290] loss: 1.468\n",
            "[172,   300] loss: 1.469\n",
            "[172,   310] loss: 1.470\n",
            "[172,   320] loss: 1.471\n",
            "[172,   330] loss: 1.471\n",
            "[172,   340] loss: 1.469\n",
            "[172,   350] loss: 1.467\n",
            "[172,   360] loss: 1.473\n",
            "[172,   370] loss: 1.479\n",
            "[172,   380] loss: 1.469\n",
            "[172,   390] loss: 1.475\n",
            "[172,   400] loss: 1.470\n",
            "[172,   410] loss: 1.476\n",
            "[172,   420] loss: 1.468\n",
            "[172,   430] loss: 1.469\n",
            "epoch 171 val_loss 61.33353114128113 val_steps 40 val_acc 0.9334\n",
            "[173,    10] loss: 1.474\n",
            "[173,    20] loss: 1.468\n",
            "[173,    30] loss: 1.474\n",
            "[173,    40] loss: 1.476\n",
            "[173,    50] loss: 1.471\n",
            "[173,    60] loss: 1.465\n",
            "[173,    70] loss: 1.471\n",
            "[173,    80] loss: 1.469\n",
            "[173,    90] loss: 1.470\n",
            "[173,   100] loss: 1.473\n",
            "[173,   110] loss: 1.472\n",
            "[173,   120] loss: 1.471\n",
            "[173,   130] loss: 1.470\n",
            "[173,   140] loss: 1.470\n",
            "[173,   150] loss: 1.476\n",
            "[173,   160] loss: 1.468\n",
            "[173,   170] loss: 1.474\n",
            "[173,   180] loss: 1.469\n",
            "[173,   190] loss: 1.471\n",
            "[173,   200] loss: 1.469\n",
            "[173,   210] loss: 1.467\n",
            "[173,   220] loss: 1.471\n",
            "[173,   230] loss: 1.473\n",
            "[173,   240] loss: 1.471\n",
            "[173,   250] loss: 1.475\n",
            "[173,   260] loss: 1.473\n",
            "[173,   270] loss: 1.476\n",
            "[173,   280] loss: 1.470\n",
            "[173,   290] loss: 1.468\n",
            "[173,   300] loss: 1.467\n",
            "[173,   310] loss: 1.471\n",
            "[173,   320] loss: 1.471\n",
            "[173,   330] loss: 1.473\n",
            "[173,   340] loss: 1.474\n",
            "[173,   350] loss: 1.470\n",
            "[173,   360] loss: 1.469\n",
            "[173,   370] loss: 1.472\n",
            "[173,   380] loss: 1.473\n",
            "[173,   390] loss: 1.472\n",
            "[173,   400] loss: 1.472\n",
            "[173,   410] loss: 1.472\n",
            "[173,   420] loss: 1.472\n",
            "[173,   430] loss: 1.471\n",
            "epoch 172 val_loss 61.15599882602692 val_steps 40 val_acc 0.9308\n",
            "[174,    10] loss: 1.468\n",
            "[174,    20] loss: 1.475\n",
            "[174,    30] loss: 1.469\n",
            "[174,    40] loss: 1.469\n",
            "[174,    50] loss: 1.468\n",
            "[174,    60] loss: 1.469\n",
            "[174,    70] loss: 1.471\n",
            "[174,    80] loss: 1.473\n",
            "[174,    90] loss: 1.465\n",
            "[174,   100] loss: 1.472\n",
            "[174,   110] loss: 1.468\n",
            "[174,   120] loss: 1.470\n",
            "[174,   130] loss: 1.470\n",
            "[174,   140] loss: 1.475\n",
            "[174,   150] loss: 1.470\n",
            "[174,   160] loss: 1.468\n",
            "[174,   170] loss: 1.475\n",
            "[174,   180] loss: 1.472\n",
            "[174,   190] loss: 1.469\n",
            "[174,   200] loss: 1.468\n",
            "[174,   210] loss: 1.476\n",
            "[174,   220] loss: 1.474\n",
            "[174,   230] loss: 1.469\n",
            "[174,   240] loss: 1.470\n",
            "[174,   250] loss: 1.470\n",
            "[174,   260] loss: 1.471\n",
            "[174,   270] loss: 1.472\n",
            "[174,   280] loss: 1.472\n",
            "[174,   290] loss: 1.474\n",
            "[174,   300] loss: 1.472\n",
            "[174,   310] loss: 1.476\n",
            "[174,   320] loss: 1.469\n",
            "[174,   330] loss: 1.470\n",
            "[174,   340] loss: 1.472\n",
            "[174,   350] loss: 1.474\n",
            "[174,   360] loss: 1.472\n",
            "[174,   370] loss: 1.472\n",
            "[174,   380] loss: 1.472\n",
            "[174,   390] loss: 1.475\n",
            "[174,   400] loss: 1.472\n",
            "[174,   410] loss: 1.474\n",
            "[174,   420] loss: 1.469\n",
            "[174,   430] loss: 1.471\n",
            "epoch 173 val_loss 61.190282106399536 val_steps 40 val_acc 0.9338\n",
            "[175,    10] loss: 1.467\n",
            "[175,    20] loss: 1.472\n",
            "[175,    30] loss: 1.473\n",
            "[175,    40] loss: 1.469\n",
            "[175,    50] loss: 1.475\n",
            "[175,    60] loss: 1.474\n",
            "[175,    70] loss: 1.474\n",
            "[175,    80] loss: 1.474\n",
            "[175,    90] loss: 1.468\n",
            "[175,   100] loss: 1.469\n",
            "[175,   110] loss: 1.469\n",
            "[175,   120] loss: 1.472\n",
            "[175,   130] loss: 1.471\n",
            "[175,   140] loss: 1.472\n",
            "[175,   150] loss: 1.468\n",
            "[175,   160] loss: 1.470\n",
            "[175,   170] loss: 1.465\n",
            "[175,   180] loss: 1.471\n",
            "[175,   190] loss: 1.470\n",
            "[175,   200] loss: 1.472\n",
            "[175,   210] loss: 1.468\n",
            "[175,   220] loss: 1.468\n",
            "[175,   230] loss: 1.471\n",
            "[175,   240] loss: 1.473\n",
            "[175,   250] loss: 1.474\n",
            "[175,   260] loss: 1.472\n",
            "[175,   270] loss: 1.470\n",
            "[175,   280] loss: 1.471\n",
            "[175,   290] loss: 1.472\n",
            "[175,   300] loss: 1.472\n",
            "[175,   310] loss: 1.471\n",
            "[175,   320] loss: 1.474\n",
            "[175,   330] loss: 1.475\n",
            "[175,   340] loss: 1.470\n",
            "[175,   350] loss: 1.474\n",
            "[175,   360] loss: 1.473\n",
            "[175,   370] loss: 1.473\n",
            "[175,   380] loss: 1.473\n",
            "[175,   390] loss: 1.470\n",
            "[175,   400] loss: 1.465\n",
            "[175,   410] loss: 1.472\n",
            "[175,   420] loss: 1.473\n",
            "[175,   430] loss: 1.473\n",
            "epoch 174 val_loss 61.133991837501526 val_steps 40 val_acc 0.9324\n",
            "[176,    10] loss: 1.469\n",
            "[176,    20] loss: 1.472\n",
            "[176,    30] loss: 1.474\n",
            "[176,    40] loss: 1.470\n",
            "[176,    50] loss: 1.469\n",
            "[176,    60] loss: 1.476\n",
            "[176,    70] loss: 1.467\n",
            "[176,    80] loss: 1.472\n",
            "[176,    90] loss: 1.465\n",
            "[176,   100] loss: 1.471\n",
            "[176,   110] loss: 1.473\n",
            "[176,   120] loss: 1.466\n",
            "[176,   130] loss: 1.472\n",
            "[176,   140] loss: 1.468\n",
            "[176,   150] loss: 1.473\n",
            "[176,   160] loss: 1.470\n",
            "[176,   170] loss: 1.470\n",
            "[176,   180] loss: 1.474\n",
            "[176,   190] loss: 1.471\n",
            "[176,   200] loss: 1.474\n",
            "[176,   210] loss: 1.470\n",
            "[176,   220] loss: 1.475\n",
            "[176,   230] loss: 1.470\n",
            "[176,   240] loss: 1.473\n",
            "[176,   250] loss: 1.471\n",
            "[176,   260] loss: 1.474\n",
            "[176,   270] loss: 1.473\n",
            "[176,   280] loss: 1.472\n",
            "[176,   290] loss: 1.470\n",
            "[176,   300] loss: 1.470\n",
            "[176,   310] loss: 1.474\n",
            "[176,   320] loss: 1.472\n",
            "[176,   330] loss: 1.479\n",
            "[176,   340] loss: 1.475\n",
            "[176,   350] loss: 1.464\n",
            "[176,   360] loss: 1.468\n",
            "[176,   370] loss: 1.472\n",
            "[176,   380] loss: 1.470\n",
            "[176,   390] loss: 1.471\n",
            "[176,   400] loss: 1.471\n",
            "[176,   410] loss: 1.467\n",
            "[176,   420] loss: 1.473\n",
            "[176,   430] loss: 1.471\n",
            "epoch 175 val_loss 61.08128309249878 val_steps 40 val_acc 0.9332\n",
            "[177,    10] loss: 1.466\n",
            "[177,    20] loss: 1.468\n",
            "[177,    30] loss: 1.472\n",
            "[177,    40] loss: 1.469\n",
            "[177,    50] loss: 1.469\n",
            "[177,    60] loss: 1.472\n",
            "[177,    70] loss: 1.466\n",
            "[177,    80] loss: 1.472\n",
            "[177,    90] loss: 1.473\n",
            "[177,   100] loss: 1.471\n",
            "[177,   110] loss: 1.470\n",
            "[177,   120] loss: 1.475\n",
            "[177,   130] loss: 1.477\n",
            "[177,   140] loss: 1.475\n",
            "[177,   150] loss: 1.472\n",
            "[177,   160] loss: 1.474\n",
            "[177,   170] loss: 1.473\n",
            "[177,   180] loss: 1.471\n",
            "[177,   190] loss: 1.467\n",
            "[177,   200] loss: 1.466\n",
            "[177,   210] loss: 1.471\n",
            "[177,   220] loss: 1.474\n",
            "[177,   230] loss: 1.469\n",
            "[177,   240] loss: 1.469\n",
            "[177,   250] loss: 1.476\n",
            "[177,   260] loss: 1.470\n",
            "[177,   270] loss: 1.469\n",
            "[177,   280] loss: 1.472\n",
            "[177,   290] loss: 1.472\n",
            "[177,   300] loss: 1.473\n",
            "[177,   310] loss: 1.473\n",
            "[177,   320] loss: 1.471\n",
            "[177,   330] loss: 1.471\n",
            "[177,   340] loss: 1.472\n",
            "[177,   350] loss: 1.478\n",
            "[177,   360] loss: 1.468\n",
            "[177,   370] loss: 1.469\n",
            "[177,   380] loss: 1.466\n",
            "[177,   390] loss: 1.473\n",
            "[177,   400] loss: 1.473\n",
            "[177,   410] loss: 1.473\n",
            "[177,   420] loss: 1.473\n",
            "[177,   430] loss: 1.466\n",
            "epoch 176 val_loss 61.093690633773804 val_steps 40 val_acc 0.9332\n",
            "[178,    10] loss: 1.471\n",
            "[178,    20] loss: 1.473\n",
            "[178,    30] loss: 1.472\n",
            "[178,    40] loss: 1.472\n",
            "[178,    50] loss: 1.474\n",
            "[178,    60] loss: 1.472\n",
            "[178,    70] loss: 1.470\n",
            "[178,    80] loss: 1.467\n",
            "[178,    90] loss: 1.470\n",
            "[178,   100] loss: 1.474\n",
            "[178,   110] loss: 1.472\n",
            "[178,   120] loss: 1.477\n",
            "[178,   130] loss: 1.469\n",
            "[178,   140] loss: 1.470\n",
            "[178,   150] loss: 1.470\n",
            "[178,   160] loss: 1.469\n",
            "[178,   170] loss: 1.475\n",
            "[178,   180] loss: 1.470\n",
            "[178,   190] loss: 1.470\n",
            "[178,   200] loss: 1.471\n",
            "[178,   210] loss: 1.470\n",
            "[178,   220] loss: 1.466\n",
            "[178,   230] loss: 1.475\n",
            "[178,   240] loss: 1.475\n",
            "[178,   250] loss: 1.473\n",
            "[178,   260] loss: 1.469\n",
            "[178,   270] loss: 1.472\n",
            "[178,   280] loss: 1.474\n",
            "[178,   290] loss: 1.469\n",
            "[178,   300] loss: 1.472\n",
            "[178,   310] loss: 1.468\n",
            "[178,   320] loss: 1.471\n",
            "[178,   330] loss: 1.474\n",
            "[178,   340] loss: 1.471\n",
            "[178,   350] loss: 1.468\n",
            "[178,   360] loss: 1.466\n",
            "[178,   370] loss: 1.468\n",
            "[178,   380] loss: 1.472\n",
            "[178,   390] loss: 1.472\n",
            "[178,   400] loss: 1.475\n",
            "[178,   410] loss: 1.471\n",
            "[178,   420] loss: 1.470\n",
            "[178,   430] loss: 1.471\n",
            "epoch 177 val_loss 61.297953963279724 val_steps 40 val_acc 0.9318\n",
            "[179,    10] loss: 1.469\n",
            "[179,    20] loss: 1.470\n",
            "[179,    30] loss: 1.469\n",
            "[179,    40] loss: 1.470\n",
            "[179,    50] loss: 1.476\n",
            "[179,    60] loss: 1.472\n",
            "[179,    70] loss: 1.469\n",
            "[179,    80] loss: 1.471\n",
            "[179,    90] loss: 1.468\n",
            "[179,   100] loss: 1.471\n",
            "[179,   110] loss: 1.467\n",
            "[179,   120] loss: 1.470\n",
            "[179,   130] loss: 1.474\n",
            "[179,   140] loss: 1.472\n",
            "[179,   150] loss: 1.475\n",
            "[179,   160] loss: 1.475\n",
            "[179,   170] loss: 1.468\n",
            "[179,   180] loss: 1.471\n",
            "[179,   190] loss: 1.472\n",
            "[179,   200] loss: 1.475\n",
            "[179,   210] loss: 1.478\n",
            "[179,   220] loss: 1.475\n",
            "[179,   230] loss: 1.467\n",
            "[179,   240] loss: 1.470\n",
            "[179,   250] loss: 1.470\n",
            "[179,   260] loss: 1.472\n",
            "[179,   270] loss: 1.470\n",
            "[179,   280] loss: 1.473\n",
            "[179,   290] loss: 1.478\n",
            "[179,   300] loss: 1.470\n",
            "[179,   310] loss: 1.469\n",
            "[179,   320] loss: 1.470\n",
            "[179,   330] loss: 1.473\n",
            "[179,   340] loss: 1.470\n",
            "[179,   350] loss: 1.469\n",
            "[179,   360] loss: 1.466\n",
            "[179,   370] loss: 1.472\n",
            "[179,   380] loss: 1.475\n",
            "[179,   390] loss: 1.470\n",
            "[179,   400] loss: 1.469\n",
            "[179,   410] loss: 1.468\n",
            "[179,   420] loss: 1.472\n",
            "[179,   430] loss: 1.470\n",
            "epoch 178 val_loss 61.1152685880661 val_steps 40 val_acc 0.9344\n",
            "[180,    10] loss: 1.470\n",
            "[180,    20] loss: 1.469\n",
            "[180,    30] loss: 1.475\n",
            "[180,    40] loss: 1.466\n",
            "[180,    50] loss: 1.469\n",
            "[180,    60] loss: 1.473\n",
            "[180,    70] loss: 1.473\n",
            "[180,    80] loss: 1.469\n",
            "[180,    90] loss: 1.474\n",
            "[180,   100] loss: 1.470\n",
            "[180,   110] loss: 1.470\n",
            "[180,   120] loss: 1.470\n",
            "[180,   130] loss: 1.467\n",
            "[180,   140] loss: 1.476\n",
            "[180,   150] loss: 1.472\n",
            "[180,   160] loss: 1.468\n",
            "[180,   170] loss: 1.469\n",
            "[180,   180] loss: 1.479\n",
            "[180,   190] loss: 1.471\n",
            "[180,   200] loss: 1.473\n",
            "[180,   210] loss: 1.470\n",
            "[180,   220] loss: 1.472\n",
            "[180,   230] loss: 1.468\n",
            "[180,   240] loss: 1.469\n",
            "[180,   250] loss: 1.465\n",
            "[180,   260] loss: 1.470\n",
            "[180,   270] loss: 1.476\n",
            "[180,   280] loss: 1.473\n",
            "[180,   290] loss: 1.467\n",
            "[180,   300] loss: 1.475\n",
            "[180,   310] loss: 1.475\n",
            "[180,   320] loss: 1.472\n",
            "[180,   330] loss: 1.473\n",
            "[180,   340] loss: 1.468\n",
            "[180,   350] loss: 1.475\n",
            "[180,   360] loss: 1.472\n",
            "[180,   370] loss: 1.471\n",
            "[180,   380] loss: 1.471\n",
            "[180,   390] loss: 1.469\n",
            "[180,   400] loss: 1.475\n",
            "[180,   410] loss: 1.467\n",
            "[180,   420] loss: 1.475\n",
            "[180,   430] loss: 1.467\n",
            "epoch 179 val_loss 61.36436462402344 val_steps 40 val_acc 0.9328\n",
            "[181,    10] loss: 1.466\n",
            "[181,    20] loss: 1.478\n",
            "[181,    30] loss: 1.475\n",
            "[181,    40] loss: 1.475\n",
            "[181,    50] loss: 1.475\n",
            "[181,    60] loss: 1.468\n",
            "[181,    70] loss: 1.473\n",
            "[181,    80] loss: 1.472\n",
            "[181,    90] loss: 1.472\n",
            "[181,   100] loss: 1.472\n",
            "[181,   110] loss: 1.471\n",
            "[181,   120] loss: 1.470\n",
            "[181,   130] loss: 1.472\n",
            "[181,   140] loss: 1.468\n",
            "[181,   150] loss: 1.469\n",
            "[181,   160] loss: 1.469\n",
            "[181,   170] loss: 1.468\n",
            "[181,   180] loss: 1.474\n",
            "[181,   190] loss: 1.472\n",
            "[181,   200] loss: 1.470\n",
            "[181,   210] loss: 1.477\n",
            "[181,   220] loss: 1.466\n",
            "[181,   230] loss: 1.469\n",
            "[181,   240] loss: 1.476\n",
            "[181,   250] loss: 1.471\n",
            "[181,   260] loss: 1.472\n",
            "[181,   270] loss: 1.467\n",
            "[181,   280] loss: 1.475\n",
            "[181,   290] loss: 1.471\n",
            "[181,   300] loss: 1.471\n",
            "[181,   310] loss: 1.468\n",
            "[181,   320] loss: 1.465\n",
            "[181,   330] loss: 1.476\n",
            "[181,   340] loss: 1.473\n",
            "[181,   350] loss: 1.471\n",
            "[181,   360] loss: 1.471\n",
            "[181,   370] loss: 1.478\n",
            "[181,   380] loss: 1.465\n",
            "[181,   390] loss: 1.470\n",
            "[181,   400] loss: 1.467\n",
            "[181,   410] loss: 1.472\n",
            "[181,   420] loss: 1.469\n",
            "[181,   430] loss: 1.469\n",
            "epoch 180 val_loss 61.04258477687836 val_steps 40 val_acc 0.9352\n",
            "[182,    10] loss: 1.475\n",
            "[182,    20] loss: 1.472\n",
            "[182,    30] loss: 1.476\n",
            "[182,    40] loss: 1.470\n",
            "[182,    50] loss: 1.465\n",
            "[182,    60] loss: 1.470\n",
            "[182,    70] loss: 1.472\n",
            "[182,    80] loss: 1.469\n",
            "[182,    90] loss: 1.468\n",
            "[182,   100] loss: 1.470\n",
            "[182,   110] loss: 1.466\n",
            "[182,   120] loss: 1.477\n",
            "[182,   130] loss: 1.472\n",
            "[182,   140] loss: 1.475\n",
            "[182,   150] loss: 1.472\n",
            "[182,   160] loss: 1.469\n",
            "[182,   170] loss: 1.471\n",
            "[182,   180] loss: 1.466\n",
            "[182,   190] loss: 1.467\n",
            "[182,   200] loss: 1.476\n",
            "[182,   210] loss: 1.475\n",
            "[182,   220] loss: 1.470\n",
            "[182,   230] loss: 1.473\n",
            "[182,   240] loss: 1.471\n",
            "[182,   250] loss: 1.476\n",
            "[182,   260] loss: 1.473\n",
            "[182,   270] loss: 1.472\n",
            "[182,   280] loss: 1.472\n",
            "[182,   290] loss: 1.469\n",
            "[182,   300] loss: 1.472\n",
            "[182,   310] loss: 1.471\n",
            "[182,   320] loss: 1.472\n",
            "[182,   330] loss: 1.468\n",
            "[182,   340] loss: 1.467\n",
            "[182,   350] loss: 1.467\n",
            "[182,   360] loss: 1.472\n",
            "[182,   370] loss: 1.470\n",
            "[182,   380] loss: 1.472\n",
            "[182,   390] loss: 1.476\n",
            "[182,   400] loss: 1.476\n",
            "[182,   410] loss: 1.469\n",
            "[182,   420] loss: 1.467\n",
            "[182,   430] loss: 1.471\n",
            "epoch 181 val_loss 61.09866976737976 val_steps 40 val_acc 0.9332\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test accuracy\n"
      ],
      "metadata": {
        "id": "RElAIN2qh3oJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(net, testloader, device, epoch):\n",
        "    global best_acc\n",
        "    net.eval()\n",
        "    test_loss=0\n",
        "    correct=0\n",
        "    total=0\n",
        "    criterion=nn.CrossEntropyLoss()\n",
        "    test_steps=0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx,(inputs,targets)in enumerate(testloader):\n",
        "            inputs,targets=inputs.to(device),targets.to(device)\n",
        "            outputs=net(inputs)\n",
        "            loss=criterion(outputs,targets)\n",
        "\n",
        "            test_loss+=loss.item()\n",
        "            _,predicted=outputs.max(1)\n",
        "            total+=targets.size(0)\n",
        "            correct+=predicted.eq(targets).sum().item()\n",
        "            test_steps+=1\n",
        "    print(\"epoch {} test_loss {} test_steps {} test_acc {}\".format(epoch,test_loss,test_steps,correct/total))\n",
        "\n",
        "batch_size = 128\n",
        "_, _, testloader = data_loader(batch_size)\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "  device = \"cuda:0\"\n",
        "test(trained_net, testloader, device, 1)\n",
        "#print(\"test acc {}\".format(test_accuracy(trained_net)))"
      ],
      "metadata": {
        "id": "z2T0sHBWh7E9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c8f6949-6ce0-49b2-b4cf-a47b7b9d82d6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1 test_loss 120.62423348426819 test_steps 79 test_acc 0.9343\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ...\n"
      ],
      "metadata": {
        "id": "qhaxY8hFXwcY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DenseNet"
      ],
      "metadata": {
        "id": "4u-oCJdf3evv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model"
      ],
      "metadata": {
        "id": "1K2rIgSc4HdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, in_planes, growth_rate):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "        self.conv1 = nn.Conv2d(in_planes, 4*growth_rate, kernel_size=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(4*growth_rate)\n",
        "        self.conv2 = nn.Conv2d(4*growth_rate, growth_rate, kernel_size=3, padding=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(F.relu(self.bn1(x)))\n",
        "        out = self.conv2(F.relu(self.bn2(out)))\n",
        "        out = torch.cat([out,x], 1)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Transition(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes):\n",
        "        super(Transition, self).__init__()\n",
        "        self.bn = nn.BatchNorm2d(in_planes)\n",
        "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(F.relu(self.bn(x)))\n",
        "        out = F.avg_pool2d(out, 2)\n",
        "        return out\n",
        "\n",
        "\n",
        "class DenseNet(nn.Module):\n",
        "    def __init__(self, block, nblocks, growth_rate=12, reduction=0.5, num_classes=10):\n",
        "        super(DenseNet, self).__init__()\n",
        "        self.growth_rate = growth_rate\n",
        "\n",
        "        num_planes = 2*growth_rate\n",
        "        self.conv1 = nn.Conv2d(3, num_planes, kernel_size=3, padding=1, bias=False)\n",
        "\n",
        "        self.dense1 = self._make_dense_layers(block, num_planes, nblocks[0])\n",
        "        num_planes += nblocks[0]*growth_rate\n",
        "        out_planes = int(math.floor(num_planes*reduction))\n",
        "        self.trans1 = Transition(num_planes, out_planes)\n",
        "        num_planes = out_planes\n",
        "\n",
        "        self.dense2 = self._make_dense_layers(block, num_planes, nblocks[1])\n",
        "        num_planes += nblocks[1]*growth_rate\n",
        "        out_planes = int(math.floor(num_planes*reduction))\n",
        "        self.trans2 = Transition(num_planes, out_planes)\n",
        "        num_planes = out_planes\n",
        "\n",
        "        self.dense3 = self._make_dense_layers(block, num_planes, nblocks[2])\n",
        "        num_planes += nblocks[2]*growth_rate\n",
        "        out_planes = int(math.floor(num_planes*reduction))\n",
        "        self.trans3 = Transition(num_planes, out_planes)\n",
        "        num_planes = out_planes\n",
        "\n",
        "        self.dense4 = self._make_dense_layers(block, num_planes, nblocks[3])\n",
        "        num_planes += nblocks[3]*growth_rate\n",
        "\n",
        "        self.bn = nn.BatchNorm2d(num_planes)\n",
        "        self.linear = nn.Linear(num_planes, num_classes)\n",
        "\n",
        "    def _make_dense_layers(self, block, in_planes, nblock):\n",
        "        layers = []\n",
        "        for i in range(nblock):\n",
        "            layers.append(block(in_planes, self.growth_rate))\n",
        "            in_planes += self.growth_rate\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.trans1(self.dense1(out))\n",
        "        out = self.trans2(self.dense2(out))\n",
        "        out = self.trans3(self.dense3(out))\n",
        "        out = self.dense4(out)\n",
        "        out = F.avg_pool2d(F.relu(self.bn(out)), 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "def DenseNet121():\n",
        "    return DenseNet(Bottleneck, [6,12,24,16], growth_rate=12)"
      ],
      "metadata": {
        "id": "aGsV4myB3gtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train\n"
      ],
      "metadata": {
        "id": "azb3E3-x4Joi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "batch size 64 for 300 and 40 epochs\n",
        "init lr 0.1 divided by 10 at 50% and 75%\n",
        "weight decay 10^-4\n",
        "momentum 0.9\n",
        "dropout 0.2"
      ],
      "metadata": {
        "id": "OhgXD33CMxHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_dense(weight_decay=0.0001, momentum=0.9, batch_size=128):\n",
        "  net = torch.hub.load('pytorch/vision:v0.10.0', 'densenet121', pretrained=False)\n",
        "  \n",
        "  trainloader, valloader, testloader = data_loader(batch_size)\n",
        "  classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "  \n",
        "  device = \"cpu\"\n",
        "  if torch.cuda.is_available():\n",
        "      device = \"cuda:0\"\n",
        "      if torch.cuda.device_count() > 1:\n",
        "          net = nn.DataParallel(net)\n",
        "  net.to(device)\n",
        "\n",
        "  def _lr_lambda(current_step):\n",
        "      \"\"\"\n",
        "      _lr_lambda returns a multiplicative factor given an interger parameter epochs.\n",
        "      \"\"\"\n",
        "      if current_step < 25000:\n",
        "          _lr = 1\n",
        "      elif current_step < 37500:\n",
        "          _lr = .1\n",
        "      else:\n",
        "          _lr = .01\n",
        "      return _lr\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(net.parameters(), lr=0.1,\n",
        "                        momentum, weight_decay)\n",
        "  scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, _lr_lambda, last_epoch=-1, verbose=True)\n",
        "\n",
        "  for epoch in range(10):  # loop over the dataset multiple times\n",
        "        running_loss = 0.0\n",
        "        epoch_steps = 0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            epoch_steps += 1\n",
        "            if i % 2000 == 1999:  # print every 2000 mini-batches\n",
        "                print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1,\n",
        "                                                running_loss / epoch_steps))\n",
        "                running_loss = 0.0\n",
        "        \n",
        "\n",
        "        # Validation loss\n",
        "        val_loss = 0.0\n",
        "        val_steps = 0\n",
        "        total = 0\n",
        "        correct = 0\n",
        "        for i, data in enumerate(valloader, 0):\n",
        "            with torch.no_grad():\n",
        "                inputs, labels = data\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                outputs = net(inputs)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.cpu().numpy()\n",
        "                val_steps += 1\n",
        "        print(\"epoch {} val_loss {} val_steps {} val_acc {}\".format(epoch, val_loss, val_steps, correct / total))\n",
        "  print(\"Finished Training\")\n"
      ],
      "metadata": {
        "id": "cU8zRrz-4KlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dense()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "OU6-pYu9IFut",
        "outputId": "4cb8c454-eef9-4045-81f1-ff71ddb1c08c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "epoch 0 val_loss 35.10935813188553 val_steps 40 val_acc 0.7032\n",
            "epoch 1 val_loss 29.122162997722626 val_steps 40 val_acc 0.7608\n",
            "epoch 2 val_loss 25.626320630311966 val_steps 40 val_acc 0.7912\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-f0e61ace9a41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-28-dde171ec4a8a>\u001b[0m in \u001b[0;36mtrain_dense\u001b[0;34m(batch_size)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Config the Search space for Ray Tune"
      ],
      "metadata": {
        "id": "RetvMq4ZevAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch.optim as optim\n",
        "# from ray import tune\n",
        "# from ray.tune.examples.mnist_pytorch import get_data_loaders, ConvNet, train, test\n",
        "\n",
        "\n",
        "# def train_mnist(config):\n",
        "#     train_loader, test_loader = get_data_loaders()\n",
        "#     model = ConvNet()\n",
        "#     optimizer = optim.SGD(model.parameters(), lr=config[\"lr\"])\n",
        "\n",
        "#     device = \"cpu\"\n",
        "#     if torch.cuda.is_available():\n",
        "#         device = \"cuda:0\"\n",
        "#         if torch.cuda.device_count() > 1:\n",
        "#             net = nn.DataParallel(net)\n",
        "#     net.to(device)\n",
        "\n",
        "#     for i in range(10):\n",
        "#         train(model, optimizer, train_loader)\n",
        "#         acc = test(model, test_loader)\n",
        "#         tune.report(mean_accuracy=acc)\n",
        "\n",
        "\n",
        "# analysis = tune.run(\n",
        "#     train_mnist, config={\"lr\": tune.grid_search([0.001, 0.01, 0.1])})\n",
        "\n",
        "# print(\"Best config: \", analysis.get_best_config(metric=\"mean_accuracy\"))\n",
        "\n",
        "# # Get a dataframe for analyzing trial results.\n",
        "# df = analysis.dataframe()"
      ],
      "metadata": {
        "id": "F2QnqP0MB5dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from functools import partial\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from filelock import FileLock\n",
        "from torch.utils.data import random_split\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import psutil\n",
        "import ray\n",
        "ray._private.utils.get_system_memory = lambda: psutil.virtual_memory().total\n",
        "from ray import tune\n",
        "from ray.tune import CLIReporter\n",
        "from ray.tune.schedulers import ASHAScheduler"
      ],
      "metadata": {
        "id": "B0oYc63tmQtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data1(data_dir=\"./data\"):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "    trainset = torchvision.datasets.CIFAR10(\n",
        "        root=data_dir, train=True, download=True, transform=transform)\n",
        "\n",
        "    testset = torchvision.datasets.CIFAR10(\n",
        "        root=data_dir, train=False, download=True, transform=transform)\n",
        "\n",
        "    return trainset, testset"
      ],
      "metadata": {
        "id": "FRr6hVpAmM8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class Net(nn.Module):\n",
        "#     def __init__(self, l1=120, l2=84):\n",
        "#         super(Net, self).__init__()\n",
        "#         self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "#         self.pool = nn.MaxPool2d(2, 2)\n",
        "#         self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "#         self.fc1 = nn.Linear(16 * 5 * 5, 32)\n",
        "#         self.fc2 = nn.Linear(32, 16)\n",
        "#         self.fc3 = nn.Linear(16, 10)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.pool(F.relu(self.conv1(x)))\n",
        "#         x = self.pool(F.relu(self.conv2(x)))\n",
        "#         x = x.view(-1, 16 * 5 * 5)\n",
        "#         x = F.relu(self.fc1(x))\n",
        "#         x = F.relu(self.fc2(x))\n",
        "#         x = self.fc3(x)\n",
        "#         return x"
      ],
      "metadata": {
        "id": "HvYUyqpImLiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_cifar(config, checkpoint_dir=None, data_dir=None):\n",
        "    net = ResNet(3, enable_skip_connections=True) # resnet20\n",
        "    if torch.cuda.is_available():\n",
        "      net.cuda()\n",
        "\n",
        "    device = \"cpu\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda:0\"\n",
        "        if torch.cuda.device_count() > 1:\n",
        "            net = nn.DataParallel(net)\n",
        "    net.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9)\n",
        "\n",
        "    if checkpoint_dir:\n",
        "        model_state, optimizer_state = torch.load(\n",
        "            os.path.join(checkpoint_dir, \"checkpoint\"))\n",
        "        net.load_state_dict(model_state)\n",
        "        optimizer.load_state_dict(optimizer_state)\n",
        "\n",
        "    trainset, testset = load_data1(data_dir)\n",
        "\n",
        "    test_abs = int(len(trainset) * 0.8)\n",
        "    train_subset, val_subset = random_split(\n",
        "        trainset, [test_abs, len(trainset) - test_abs])\n",
        "    \n",
        "    trainloader, valloader, testloader = data_loader()\n",
        "\n",
        "    trainloader = torch.utils.data.DataLoader(\n",
        "        train_subset,\n",
        "        batch_size=int(config[\"batch_size\"]),\n",
        "        shuffle=True,\n",
        "        num_workers=8)\n",
        "    valloader = torch.utils.data.DataLoader(\n",
        "        val_subset,\n",
        "        batch_size=int(config[\"batch_size\"]),\n",
        "        shuffle=True,\n",
        "        num_workers=8)\n",
        "\n",
        "    for epoch in range(10):  # loop over the dataset multiple times\n",
        "        running_loss = 0.0\n",
        "        epoch_steps = 0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            print(\"device:\", device)\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            epoch_steps += 1\n",
        "            if i % 2000 == 1999:  # print every 2000 mini-batches\n",
        "                print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1,\n",
        "                                                running_loss / epoch_steps))\n",
        "                running_loss = 0.0\n",
        "\n",
        "        # Validation loss\n",
        "        val_loss = 0.0\n",
        "        val_steps = 0\n",
        "        total = 0\n",
        "        correct = 0\n",
        "        for i, data in enumerate(valloader, 0):\n",
        "            with torch.no_grad():\n",
        "                inputs, labels = data\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                outputs = net(inputs)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.cpu().numpy()\n",
        "                val_steps += 1\n",
        "\n",
        "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
        "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
        "            torch.save((net.state_dict(), optimizer.state_dict()), path)\n",
        "\n",
        "        tune.report(loss=(val_loss / val_steps), accuracy=correct / total)\n",
        "    print(\"Finished Training\")"
      ],
      "metadata": {
        "id": "j2e4YGx_mIe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_accuracy(net, device=\"cpu\"):\n",
        "    trainset, testset = load_data1()\n",
        "\n",
        "    device = \"cpu\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda:0\"\n",
        "        net.cuda()\n",
        "\n",
        "    testloader = torch.utils.data.DataLoader(\n",
        "        testset, batch_size=4, shuffle=False, num_workers=2)\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return correct / total"
      ],
      "metadata": {
        "id": "nNFxU9jKqyyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Original paper\n",
        "(Hyper) SGD wiht mini-batch size 128  \n",
        "(Hyper) learning rate starts from 0.1, divide it by 10 at 32K and 48K iterations,   \n",
        "terminate at 64k iterations  \n",
        "45k/5k train/val split\n",
        "(Done) Image agumentation: 4 pixels are padded on each side, and a 32x32 crop is randomly sampled from the padded image or its horizontal flip. for testing, only evaluate the single view of the original 32x32 image.  \n",
        "(Hyper) weight decay : 0.0001  \n",
        "(Hyper) momentum : 0.9  \n"
      ],
      "metadata": {
        "id": "VMTwwmdrbp8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(num_samples=10, max_num_epochs=10, gpus_per_trial=2):\n",
        "    data_dir = os.path.abspath(\"./data\")\n",
        "    load_data1(data_dir)\n",
        "    config = {\n",
        "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
        "        \"batch_size\": tune.choice([2, 4, 8, 16])\n",
        "    }\n",
        "    scheduler = ASHAScheduler(\n",
        "        metric=\"loss\",\n",
        "        mode=\"min\",\n",
        "        max_t=max_num_epochs,\n",
        "        grace_period=1,\n",
        "        reduction_factor=2)\n",
        "    reporter = CLIReporter(\n",
        "        # parameter_columns=[\"l1\", \"l2\", \"lr\", \"batch_size\"],\n",
        "        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
        "    result = tune.run(\n",
        "        partial(train_cifar, data_dir=data_dir),\n",
        "        resources_per_trial={\"cpu\": 2, \"gpu\": gpus_per_trial},\n",
        "        config=config,\n",
        "        num_samples=num_samples,\n",
        "        scheduler=scheduler,\n",
        "        progress_reporter=reporter)\n",
        "\n",
        "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
        "    print(\"Best trial config: {}\".format(best_trial.config))\n",
        "    print(\"Best trial final validation loss: {}\".format(\n",
        "        best_trial.last_result[\"loss\"]))\n",
        "    print(\"Best trial final validation accuracy: {}\".format(\n",
        "        best_trial.last_result[\"accuracy\"]))\n",
        "\n",
        "    best_trained_model = ResNet(3, enable_skip_connections=True)\n",
        "    # best_trained_model = Net(best_trial.config[\"l1\"], best_trial.config[\"l2\"])\n",
        "    device = \"cpu\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda:0\"\n",
        "        if gpus_per_trial > 1:\n",
        "            best_trained_model = nn.DataParallel(best_trained_model)\n",
        "    best_trained_model.to(device)\n",
        "    best_trained_model.cuda()\n",
        "\n",
        "    best_checkpoint_dir = best_trial.checkpoint.value\n",
        "    model_state, optimizer_state = torch.load(os.path.join(\n",
        "        best_checkpoint_dir, \"checkpoint\"))\n",
        "    best_trained_model.load_state_dict(model_state)\n",
        "\n",
        "    test_acc = test_accuracy(best_trained_model, device)\n",
        "    print(\"Best trial test set accuracy: {}\".format(test_acc))\n",
        "\n",
        "main(num_samples=5, max_num_epochs=10, gpus_per_trial=1)"
      ],
      "metadata": {
        "id": "PHajivQyDtYY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b685d729-5f3e-4286-c763-3a8f86894480"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-04-20 02:36:56,420\tWARNING callback.py:126 -- The TensorboardX logger cannot be instantiated because either TensorboardX or one of it's dependencies is not installed. Please make sure you have the latest version of TensorboardX installed: `pip install -U tensorboardx`\n",
            "2022-04-20 02:36:56,574\tINFO trial_runner.py:803 -- starting train_cifar_bee56_00000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2022-04-20 02:36:56 (running for 00:00:00.25)\n",
            "Memory usage on this node: 2.1/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.35 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/train_cifar_2022-04-20_02-36-56\n",
            "Number of trials: 5/5 (4 PENDING, 1 RUNNING)\n",
            "+-------------------------+----------+-----------------+--------------+-------------+\n",
            "| Trial name              | status   | loc             |   batch_size |          lr |\n",
            "|-------------------------+----------+-----------------+--------------+-------------|\n",
            "| train_cifar_bee56_00000 | RUNNING  | 172.28.0.2:2625 |            4 | 0.0285835   |\n",
            "| train_cifar_bee56_00001 | PENDING  |                 |            4 | 0.00398609  |\n",
            "| train_cifar_bee56_00002 | PENDING  |                 |            2 | 0.000279323 |\n",
            "| train_cifar_bee56_00003 | PENDING  |                 |            4 | 0.0333555   |\n",
            "| train_cifar_bee56_00004 | PENDING  |                 |            4 | 0.00959189  |\n",
            "+-------------------------+----------+-----------------+--------------+-------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m Files already downloaded and verified\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m device: cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m 2022-04-20 02:37:04,439\tERROR function_runner.py:281 -- Runner Thread raised error.\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m Traceback (most recent call last):\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 272, in run\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m     self._entrypoint()\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 351, in entrypoint\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m     self._status_reporter.get_checkpoint(),\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/util/tracing/tracing_helper.py\", line 462, in _resume_span\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m     return method(self, *_args, **_kwargs)\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 640, in _trainable_func\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m     output = fn()\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   File \"<ipython-input-35-fdcfbf8dff67>\", line 51, in train_cifar\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m     return forward_call(*input, **kwargs)\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   File \"<ipython-input-34-16e98d6e3158>\", line 75, in forward\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m     return forward_call(*input, **kwargs)\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   File \"<ipython-input-34-16e98d6e3158>\", line 33, in forward\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m     return forward_call(*input, **kwargs)\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\", line 446, in forward\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\", line 443, in _conv_forward\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m     self.padding, self.dilation, self.groups)\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m Exception in thread Thread-3:\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m Traceback (most recent call last):\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m     self.run()\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 298, in run\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m     raise e\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 272, in run\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m     self._entrypoint()\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 351, in entrypoint\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m     self._status_reporter.get_checkpoint(),\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/util/tracing/tracing_helper.py\", line 462, in _resume_span\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m     return method(self, *_args, **_kwargs)\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 640, in _trainable_func\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m     output = fn()\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   File \"<ipython-input-35-fdcfbf8dff67>\", line 51, in train_cifar\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m     return forward_call(*input, **kwargs)\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   File \"<ipython-input-34-16e98d6e3158>\", line 75, in forward\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m     return forward_call(*input, **kwargs)\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   File \"<ipython-input-34-16e98d6e3158>\", line 33, in forward\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m     return forward_call(*input, **kwargs)\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\", line 446, in forward\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\", line 443, in _conv_forward\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m     self.padding, self.dilation, self.groups)\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m \n",
            "2022-04-20 02:37:04,598\tERROR trial_runner.py:876 -- Trial train_cifar_bee56_00000: Error processing event.\n",
            "NoneType: None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2022-04-20 02:37:04 (running for 00:00:08.16)\n",
            "Memory usage on this node: 3.0/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.35 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/train_cifar_2022-04-20_02-36-56\n",
            "Number of trials: 5/5 (4 PENDING, 1 RUNNING)\n",
            "+-------------------------+----------+-----------------+--------------+-------------+\n",
            "| Trial name              | status   | loc             |   batch_size |          lr |\n",
            "|-------------------------+----------+-----------------+--------------+-------------|\n",
            "| train_cifar_bee56_00000 | RUNNING  | 172.28.0.2:2625 |            4 | 0.0285835   |\n",
            "| train_cifar_bee56_00001 | PENDING  |                 |            4 | 0.00398609  |\n",
            "| train_cifar_bee56_00002 | PENDING  |                 |            2 | 0.000279323 |\n",
            "| train_cifar_bee56_00003 | PENDING  |                 |            4 | 0.0333555   |\n",
            "| train_cifar_bee56_00004 | PENDING  |                 |            4 | 0.00959189  |\n",
            "+-------------------------+----------+-----------------+--------------+-------------+\n",
            "\n",
            "\n",
            "Result for train_cifar_bee56_00000:\n",
            "  date: 2022-04-20_02-36-59\n",
            "  experiment_id: 0c6c42295bb44010874510dd0f5f149e\n",
            "  hostname: 753d77ac8148\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2625\n",
            "  timestamp: 1650422219\n",
            "  trial_id: bee56_00000\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-04-20 02:37:05,395\tINFO trial_runner.py:803 -- starting train_cifar_bee56_00001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2022-04-20 02:37:10 (running for 00:00:13.99)\n",
            "Memory usage on this node: 2.9/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.35 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/train_cifar_2022-04-20_02-36-56\n",
            "Number of trials: 5/5 (1 ERROR, 3 PENDING, 1 RUNNING)\n",
            "+-------------------------+----------+-----------------+--------------+-------------+\n",
            "| Trial name              | status   | loc             |   batch_size |          lr |\n",
            "|-------------------------+----------+-----------------+--------------+-------------|\n",
            "| train_cifar_bee56_00001 | RUNNING  | 172.28.0.2:2710 |            4 | 0.00398609  |\n",
            "| train_cifar_bee56_00002 | PENDING  |                 |            2 | 0.000279323 |\n",
            "| train_cifar_bee56_00003 | PENDING  |                 |            4 | 0.0333555   |\n",
            "| train_cifar_bee56_00004 | PENDING  |                 |            4 | 0.00959189  |\n",
            "| train_cifar_bee56_00000 | ERROR    | 172.28.0.2:2625 |            4 | 0.0285835   |\n",
            "+-------------------------+----------+-----------------+--------------+-------------+\n",
            "Number of errored trials: 1\n",
            "+-------------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              |   # failures | error file                                                                                                                         |\n",
            "|-------------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------|\n",
            "| train_cifar_bee56_00000 |            1 | /root/ray_results/train_cifar_2022-04-20_02-36-56/train_cifar_bee56_00000_0_batch_size=4,lr=0.028584_2022-04-20_02-36-56/error.txt |\n",
            "+-------------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m Files already downloaded and verified\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m device: cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m 2022-04-20 02:37:13,125\tERROR function_runner.py:281 -- Runner Thread raised error.\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m Traceback (most recent call last):\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 272, in run\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m     self._entrypoint()\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 351, in entrypoint\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m     self._status_reporter.get_checkpoint(),\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/util/tracing/tracing_helper.py\", line 462, in _resume_span\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m     return method(self, *_args, **_kwargs)\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 640, in _trainable_func\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m     output = fn()\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   File \"<ipython-input-35-fdcfbf8dff67>\", line 51, in train_cifar\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m     return forward_call(*input, **kwargs)\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   File \"<ipython-input-34-16e98d6e3158>\", line 75, in forward\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m     return forward_call(*input, **kwargs)\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   File \"<ipython-input-34-16e98d6e3158>\", line 33, in forward\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m     return forward_call(*input, **kwargs)\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\", line 446, in forward\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\", line 443, in _conv_forward\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m     self.padding, self.dilation, self.groups)\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m Exception in thread Thread-3:\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m Traceback (most recent call last):\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m     self.run()\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 298, in run\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m     raise e\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 272, in run\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m     self._entrypoint()\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 351, in entrypoint\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m     self._status_reporter.get_checkpoint(),\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/util/tracing/tracing_helper.py\", line 462, in _resume_span\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m     return method(self, *_args, **_kwargs)\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 640, in _trainable_func\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m     output = fn()\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   File \"<ipython-input-35-fdcfbf8dff67>\", line 51, in train_cifar\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m     return forward_call(*input, **kwargs)\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   File \"<ipython-input-34-16e98d6e3158>\", line 75, in forward\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m     return forward_call(*input, **kwargs)\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   File \"<ipython-input-34-16e98d6e3158>\", line 33, in forward\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m     return forward_call(*input, **kwargs)\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\", line 446, in forward\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\", line 443, in _conv_forward\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m     self.padding, self.dilation, self.groups)\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m \n",
            "2022-04-20 02:37:13,287\tERROR trial_runner.py:876 -- Trial train_cifar_bee56_00001: Error processing event.\n",
            "NoneType: None\n",
            "2022-04-20 02:37:13,396\tINFO trial_runner.py:803 -- starting train_cifar_bee56_00002\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for train_cifar_bee56_00001:\n",
            "  date: 2022-04-20_02-37-08\n",
            "  experiment_id: a84939f2765d44518c53d0e5c839a937\n",
            "  hostname: 753d77ac8148\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2710\n",
            "  timestamp: 1650422228\n",
            "  trial_id: bee56_00001\n",
            "  \n",
            "== Status ==\n",
            "Current time: 2022-04-20 02:37:18 (running for 00:00:22.01)\n",
            "Memory usage on this node: 2.9/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.35 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/train_cifar_2022-04-20_02-36-56\n",
            "Number of trials: 5/5 (2 ERROR, 2 PENDING, 1 RUNNING)\n",
            "+-------------------------+----------+-----------------+--------------+-------------+\n",
            "| Trial name              | status   | loc             |   batch_size |          lr |\n",
            "|-------------------------+----------+-----------------+--------------+-------------|\n",
            "| train_cifar_bee56_00002 | RUNNING  | 172.28.0.2:2794 |            2 | 0.000279323 |\n",
            "| train_cifar_bee56_00003 | PENDING  |                 |            4 | 0.0333555   |\n",
            "| train_cifar_bee56_00004 | PENDING  |                 |            4 | 0.00959189  |\n",
            "| train_cifar_bee56_00000 | ERROR    | 172.28.0.2:2625 |            4 | 0.0285835   |\n",
            "| train_cifar_bee56_00001 | ERROR    | 172.28.0.2:2710 |            4 | 0.00398609  |\n",
            "+-------------------------+----------+-----------------+--------------+-------------+\n",
            "Number of errored trials: 2\n",
            "+-------------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              |   # failures | error file                                                                                                                          |\n",
            "|-------------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------|\n",
            "| train_cifar_bee56_00000 |            1 | /root/ray_results/train_cifar_2022-04-20_02-36-56/train_cifar_bee56_00000_0_batch_size=4,lr=0.028584_2022-04-20_02-36-56/error.txt  |\n",
            "| train_cifar_bee56_00001 |            1 | /root/ray_results/train_cifar_2022-04-20_02-36-56/train_cifar_bee56_00001_1_batch_size=4,lr=0.0039861_2022-04-20_02-37-05/error.txt |\n",
            "+-------------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m Files already downloaded and verified\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m device: cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m 2022-04-20 02:37:21,145\tERROR function_runner.py:281 -- Runner Thread raised error.\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m Traceback (most recent call last):\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 272, in run\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m     self._entrypoint()\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 351, in entrypoint\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m     self._status_reporter.get_checkpoint(),\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/util/tracing/tracing_helper.py\", line 462, in _resume_span\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m     return method(self, *_args, **_kwargs)\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 640, in _trainable_func\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m     output = fn()\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   File \"<ipython-input-35-fdcfbf8dff67>\", line 51, in train_cifar\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m     return forward_call(*input, **kwargs)\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   File \"<ipython-input-34-16e98d6e3158>\", line 75, in forward\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m     return forward_call(*input, **kwargs)\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   File \"<ipython-input-34-16e98d6e3158>\", line 33, in forward\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m     return forward_call(*input, **kwargs)\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\", line 446, in forward\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\", line 443, in _conv_forward\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m     self.padding, self.dilation, self.groups)\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m Exception in thread Thread-3:\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m Traceback (most recent call last):\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m     self.run()\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 298, in run\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m     raise e\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 272, in run\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m     self._entrypoint()\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 351, in entrypoint\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m     self._status_reporter.get_checkpoint(),\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/util/tracing/tracing_helper.py\", line 462, in _resume_span\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m     return method(self, *_args, **_kwargs)\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 640, in _trainable_func\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m     output = fn()\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   File \"<ipython-input-35-fdcfbf8dff67>\", line 51, in train_cifar\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m     return forward_call(*input, **kwargs)\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   File \"<ipython-input-34-16e98d6e3158>\", line 75, in forward\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m     return forward_call(*input, **kwargs)\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   File \"<ipython-input-34-16e98d6e3158>\", line 33, in forward\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m     return forward_call(*input, **kwargs)\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\", line 446, in forward\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\", line 443, in _conv_forward\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m     self.padding, self.dilation, self.groups)\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m \n",
            "2022-04-20 02:37:21,348\tERROR trial_runner.py:876 -- Trial train_cifar_bee56_00002: Error processing event.\n",
            "NoneType: None\n",
            "2022-04-20 02:37:21,397\tINFO trial_runner.py:803 -- starting train_cifar_bee56_00003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for train_cifar_bee56_00002:\n",
            "  date: 2022-04-20_02-37-16\n",
            "  experiment_id: b624c444353d4fa29dde8771f42b3652\n",
            "  hostname: 753d77ac8148\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2794\n",
            "  timestamp: 1650422236\n",
            "  trial_id: bee56_00002\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-04-20 02:37:22,414\tWARNING tune.py:651 -- SIGINT received (e.g. via Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C one more time (or send SIGINT/SIGKILL/SIGTERM) to skip. \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-51acda9eabda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best trial test set accuracy: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_num_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpus_per_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-36-51acda9eabda>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(num_samples, max_num_epochs, gpus_per_trial)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         progress_reporter=reporter)\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mbest_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"min\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"last\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, max_concurrent_trials, _experiment_checkpoint_dir, queue_trials, loggers, _remote)\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[0mprogress_reporter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_start_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtune_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_finished\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIGINT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m         \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhas_verbosity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVerbosity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV1_EXPERIMENT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m             \u001b[0m_report_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_reporter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/tune/trial_runner.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0mnext_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_trial_queue_and_get_next_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_and_handle_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop_experiment_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/tune/trial_runner.py\u001b[0m in \u001b[0;36m_wait_and_handle_event\u001b[0;34m(self, next_trial)\u001b[0m\n\u001b[1;32m    714\u001b[0m             \u001b[0;31m# Single wait of entire tune loop.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             future_result = self.trial_executor.get_next_executor_event(\n\u001b[0;32m--> 716\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_live_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_trial\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             )\n\u001b[1;32m    718\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfuture_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mExecutorEventType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPG_READY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/tune/ray_trial_executor.py\u001b[0m in \u001b[0;36mget_next_executor_event\u001b[0;34m(self, live_trials, next_trial_exists)\u001b[0m\n\u001b[1;32m    855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m             ready_futures, _ = ray.wait(\n\u001b[0;32m--> 857\u001b[0;31m                 \u001b[0mfutures_to_wait\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_returns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_next_event_wait\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    858\u001b[0m             )\n\u001b[1;32m    859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/_private/client_mode_hook.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"init\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_client_mode_enabled_by_default\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/worker.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_refs, num_returns, timeout, fetch_local)\u001b[0m\n\u001b[1;32m   1996\u001b[0m             \u001b[0mtimeout_milliseconds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m             \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_task_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1998\u001b[0;31m             \u001b[0mfetch_local\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1999\u001b[0m         )\n\u001b[1;32m   2000\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mready_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.CoreWorker.wait\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "gQ4JgbCP1pru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change these values if you want the training to run quicker or slower.\n",
        "EPOCH_SIZE = 512\n",
        "TEST_SIZE = 256\n",
        "\n",
        "def train(model, optimizer, train_loader):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        # We set this just for the example to run quickly.\n",
        "        if batch_idx * len(data) > EPOCH_SIZE:\n",
        "            return\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "def test(model, data_loader):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(data_loader):\n",
        "            # We set this just for the example to run quickly.\n",
        "            if batch_idx * len(data) > TEST_SIZE:\n",
        "                break\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            outputs = model(data)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "    return correct / total"
      ],
      "metadata": {
        "id": "zC_doY3o1s2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_cifar1(config):\n",
        "    # Data Setup\n",
        "    mnist_transforms = transforms.Compose(\n",
        "        [transforms.ToTensor(),\n",
        "         transforms.Normalize((0.1307, ), (0.3081, ))])\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        torchvision.datasets.CIFAR10(\"~/data\", train=True, download=True, transform=mnist_transforms),\n",
        "        batch_size=64,\n",
        "        shuffle=True)\n",
        "    test_loader = DataLoader(\n",
        "        torchvision.datasets.CIFAR10(\"~/data\", train=False, transform=mnist_transforms),\n",
        "        batch_size=64,\n",
        "        shuffle=True)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = ResNet(3, enable_skip_connections=True)\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = optim.SGD(\n",
        "        model.parameters(), lr=config[\"lr\"], momentum=config[\"momentum\"])\n",
        "    for i in range(10):\n",
        "        train(model, optimizer, train_loader)\n",
        "        acc = test(model, test_loader)\n",
        "\n",
        "        # Send the current training result back to Tune\n",
        "        tune.report(mean_accuracy=acc)\n",
        "\n",
        "        if i % 5 == 0:\n",
        "            # This saves the model to the trial directory\n",
        "            torch.save(model.state_dict(), \"./model.pth\")"
      ],
      "metadata": {
        "id": "JswhX1tQ16xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_space = {\n",
        "    \"lr\": tune.sample_from(lambda spec: 10 ** (-10 * np.random.rand())),\n",
        "    \"momentum\": tune.uniform(0.1, 0.9),\n",
        "}\n",
        "\n",
        "# Uncomment this to enable distributed execution\n",
        "# `ray.init(address=\"auto\")`\n",
        "\n",
        "# Download the dataset first\n",
        "torchvision.datasets.CIFAR10(\"~/data\", train=True, download=True)\n",
        "\n",
        "analysis = tune.run(train_cifar1, config=search_space)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aMezJl1K2jM1",
        "outputId": "3f6318de-8493-4612-c29f-d7a54998663b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-04-20 02:45:42,799\tWARNING callback.py:126 -- The TensorboardX logger cannot be instantiated because either TensorboardX or one of it's dependencies is not installed. Please make sure you have the latest version of TensorboardX installed: `pip install -U tensorboardx`\n",
            "2022-04-20 02:45:42,808\tWARNING tune.py:637 -- Tune detects GPUs, but no trials are using GPUs. To enable trials to use GPUs, set tune.run(resources_per_trial={'gpu': 1}...) which allows Tune to expose 1 GPU to each trial. You can also override `Trainable.default_resource_request` if using the Trainable API.\n",
            "2022-04-20 02:45:42,927\tINFO trial_runner.py:803 -- starting train_cifar1_f8a47_00000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-20 02:45:45 (running for 00:00:02.98)<br>Memory usage on this node: 2.2/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/2 CPUs, 0/1 GPUs, 0.0/7.35 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/train_cifar1_2022-04-20_02-45-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name              </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  momentum</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>train_cifar1_f8a47_00000</td><td>RUNNING </td><td>172.28.0.2:3168</td><td style=\"text-align: right;\">1.06222e-07</td><td style=\"text-align: right;\">  0.628584</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(train_cifar1 pid=3168)\u001b[0m Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-20 02:45:50 (running for 00:00:07.98)<br>Memory usage on this node: 2.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/2 CPUs, 0/1 GPUs, 0.0/7.35 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/train_cifar1_2022-04-20_02-45-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name              </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  momentum</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>train_cifar1_f8a47_00000</td><td>RUNNING </td><td>172.28.0.2:3168</td><td style=\"text-align: right;\">1.06222e-07</td><td style=\"text-align: right;\">  0.628584</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for train_cifar1_f8a47_00000:\n",
            "  date: 2022-04-20_02-45-52\n",
            "  done: false\n",
            "  experiment_id: c79eb786ed0e4c59be1a3212475a4b14\n",
            "  hostname: 753d77ac8148\n",
            "  iterations_since_restore: 1\n",
            "  mean_accuracy: 0.109375\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 3168\n",
            "  time_since_restore: 6.664595127105713\n",
            "  time_this_iter_s: 6.664595127105713\n",
            "  time_total_s: 6.664595127105713\n",
            "  timestamp: 1650422752\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: f8a47_00000\n",
            "  warmup_time: 0.003980159759521484\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-20 02:45:57 (running for 00:00:14.66)<br>Memory usage on this node: 2.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/2 CPUs, 0/1 GPUs, 0.0/7.35 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/train_cifar1_2022-04-20_02-45-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name              </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>train_cifar1_f8a47_00000</td><td>RUNNING </td><td>172.28.0.2:3168</td><td style=\"text-align: right;\">1.06222e-07</td><td style=\"text-align: right;\">  0.628584</td><td style=\"text-align: right;\">0.109375</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">          6.6646</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for train_cifar1_f8a47_00000:\n",
            "  date: 2022-04-20_02-45-57\n",
            "  done: false\n",
            "  experiment_id: c79eb786ed0e4c59be1a3212475a4b14\n",
            "  hostname: 753d77ac8148\n",
            "  iterations_since_restore: 2\n",
            "  mean_accuracy: 0.109375\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 3168\n",
            "  time_since_restore: 11.78224802017212\n",
            "  time_this_iter_s: 5.117652893066406\n",
            "  time_total_s: 11.78224802017212\n",
            "  timestamp: 1650422757\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 2\n",
            "  trial_id: f8a47_00000\n",
            "  warmup_time: 0.003980159759521484\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-20 02:46:02 (running for 00:00:19.77)<br>Memory usage on this node: 2.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/2 CPUs, 0/1 GPUs, 0.0/7.35 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/train_cifar1_2022-04-20_02-45-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name              </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>train_cifar1_f8a47_00000</td><td>RUNNING </td><td>172.28.0.2:3168</td><td style=\"text-align: right;\">1.06222e-07</td><td style=\"text-align: right;\">  0.628584</td><td style=\"text-align: right;\">0.109375</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         11.7822</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for train_cifar1_f8a47_00000:\n",
            "  date: 2022-04-20_02-46-02\n",
            "  done: false\n",
            "  experiment_id: c79eb786ed0e4c59be1a3212475a4b14\n",
            "  hostname: 753d77ac8148\n",
            "  iterations_since_restore: 3\n",
            "  mean_accuracy: 0.10625\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 3168\n",
            "  time_since_restore: 16.856648206710815\n",
            "  time_this_iter_s: 5.074400186538696\n",
            "  time_total_s: 16.856648206710815\n",
            "  timestamp: 1650422762\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 3\n",
            "  trial_id: f8a47_00000\n",
            "  warmup_time: 0.003980159759521484\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-20 02:46:07 (running for 00:00:24.85)<br>Memory usage on this node: 2.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/2 CPUs, 0/1 GPUs, 0.0/7.35 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/train_cifar1_2022-04-20_02-45-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name              </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">    acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>train_cifar1_f8a47_00000</td><td>RUNNING </td><td>172.28.0.2:3168</td><td style=\"text-align: right;\">1.06222e-07</td><td style=\"text-align: right;\">  0.628584</td><td style=\"text-align: right;\">0.10625</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         16.8566</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for train_cifar1_f8a47_00000:\n",
            "  date: 2022-04-20_02-46-07\n",
            "  done: false\n",
            "  experiment_id: c79eb786ed0e4c59be1a3212475a4b14\n",
            "  hostname: 753d77ac8148\n",
            "  iterations_since_restore: 4\n",
            "  mean_accuracy: 0.10625\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 3168\n",
            "  time_since_restore: 21.949237823486328\n",
            "  time_this_iter_s: 5.092589616775513\n",
            "  time_total_s: 21.949237823486328\n",
            "  timestamp: 1650422767\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 4\n",
            "  trial_id: f8a47_00000\n",
            "  warmup_time: 0.003980159759521484\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-20 02:46:12 (running for 00:00:29.93)<br>Memory usage on this node: 2.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/2 CPUs, 0/1 GPUs, 0.0/7.35 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/train_cifar1_2022-04-20_02-45-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name              </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">    acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>train_cifar1_f8a47_00000</td><td>RUNNING </td><td>172.28.0.2:3168</td><td style=\"text-align: right;\">1.06222e-07</td><td style=\"text-align: right;\">  0.628584</td><td style=\"text-align: right;\">0.10625</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         21.9492</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for train_cifar1_f8a47_00000:\n",
            "  date: 2022-04-20_02-46-12\n",
            "  done: false\n",
            "  experiment_id: c79eb786ed0e4c59be1a3212475a4b14\n",
            "  hostname: 753d77ac8148\n",
            "  iterations_since_restore: 5\n",
            "  mean_accuracy: 0.090625\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 3168\n",
            "  time_since_restore: 27.060617208480835\n",
            "  time_this_iter_s: 5.111379384994507\n",
            "  time_total_s: 27.060617208480835\n",
            "  timestamp: 1650422772\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 5\n",
            "  trial_id: f8a47_00000\n",
            "  warmup_time: 0.003980159759521484\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-20 02:46:17 (running for 00:00:35.05)<br>Memory usage on this node: 2.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/2 CPUs, 0/1 GPUs, 0.0/7.35 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/train_cifar1_2022-04-20_02-45-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name              </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>train_cifar1_f8a47_00000</td><td>RUNNING </td><td>172.28.0.2:3168</td><td style=\"text-align: right;\">1.06222e-07</td><td style=\"text-align: right;\">  0.628584</td><td style=\"text-align: right;\">0.090625</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         27.0606</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for train_cifar1_f8a47_00000:\n",
            "  date: 2022-04-20_02-46-17\n",
            "  done: false\n",
            "  experiment_id: c79eb786ed0e4c59be1a3212475a4b14\n",
            "  hostname: 753d77ac8148\n",
            "  iterations_since_restore: 6\n",
            "  mean_accuracy: 0.10625\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 3168\n",
            "  time_since_restore: 32.14216995239258\n",
            "  time_this_iter_s: 5.081552743911743\n",
            "  time_total_s: 32.14216995239258\n",
            "  timestamp: 1650422777\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 6\n",
            "  trial_id: f8a47_00000\n",
            "  warmup_time: 0.003980159759521484\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-20 02:46:22 (running for 00:00:40.13)<br>Memory usage on this node: 2.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/2 CPUs, 0/1 GPUs, 0.0/7.35 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/train_cifar1_2022-04-20_02-45-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name              </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">    acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>train_cifar1_f8a47_00000</td><td>RUNNING </td><td>172.28.0.2:3168</td><td style=\"text-align: right;\">1.06222e-07</td><td style=\"text-align: right;\">  0.628584</td><td style=\"text-align: right;\">0.10625</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         32.1422</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for train_cifar1_f8a47_00000:\n",
            "  date: 2022-04-20_02-46-23\n",
            "  done: false\n",
            "  experiment_id: c79eb786ed0e4c59be1a3212475a4b14\n",
            "  hostname: 753d77ac8148\n",
            "  iterations_since_restore: 7\n",
            "  mean_accuracy: 0.103125\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 3168\n",
            "  time_since_restore: 37.28174901008606\n",
            "  time_this_iter_s: 5.1395790576934814\n",
            "  time_total_s: 37.28174901008606\n",
            "  timestamp: 1650422783\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 7\n",
            "  trial_id: f8a47_00000\n",
            "  warmup_time: 0.003980159759521484\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-20 02:46:28 (running for 00:00:45.28)<br>Memory usage on this node: 2.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/2 CPUs, 0/1 GPUs, 0.0/7.35 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/train_cifar1_2022-04-20_02-45-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name              </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>train_cifar1_f8a47_00000</td><td>RUNNING </td><td>172.28.0.2:3168</td><td style=\"text-align: right;\">1.06222e-07</td><td style=\"text-align: right;\">  0.628584</td><td style=\"text-align: right;\">0.103125</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         37.2817</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for train_cifar1_f8a47_00000:\n",
            "  date: 2022-04-20_02-46-28\n",
            "  done: false\n",
            "  experiment_id: c79eb786ed0e4c59be1a3212475a4b14\n",
            "  hostname: 753d77ac8148\n",
            "  iterations_since_restore: 8\n",
            "  mean_accuracy: 0.10625\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 3168\n",
            "  time_since_restore: 42.40127110481262\n",
            "  time_this_iter_s: 5.1195220947265625\n",
            "  time_total_s: 42.40127110481262\n",
            "  timestamp: 1650422788\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 8\n",
            "  trial_id: f8a47_00000\n",
            "  warmup_time: 0.003980159759521484\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-20 02:46:33 (running for 00:00:50.39)<br>Memory usage on this node: 2.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/2 CPUs, 0/1 GPUs, 0.0/7.35 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/train_cifar1_2022-04-20_02-45-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name              </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">    acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>train_cifar1_f8a47_00000</td><td>RUNNING </td><td>172.28.0.2:3168</td><td style=\"text-align: right;\">1.06222e-07</td><td style=\"text-align: right;\">  0.628584</td><td style=\"text-align: right;\">0.10625</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         42.4013</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for train_cifar1_f8a47_00000:\n",
            "  date: 2022-04-20_02-46-33\n",
            "  done: false\n",
            "  experiment_id: c79eb786ed0e4c59be1a3212475a4b14\n",
            "  hostname: 753d77ac8148\n",
            "  iterations_since_restore: 9\n",
            "  mean_accuracy: 0.084375\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 3168\n",
            "  time_since_restore: 47.52450728416443\n",
            "  time_this_iter_s: 5.123236179351807\n",
            "  time_total_s: 47.52450728416443\n",
            "  timestamp: 1650422793\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 9\n",
            "  trial_id: f8a47_00000\n",
            "  warmup_time: 0.003980159759521484\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfs = analysis.trial_dataframes\n",
        "[d.mean_accuracy.plot() for d in dfs.values()]"
      ],
      "metadata": {
        "id": "vTKEK6ty2oj-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}