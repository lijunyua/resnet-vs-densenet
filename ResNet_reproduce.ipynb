{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSC413 Project.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "gQ4JgbCP1pru"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "96ecb36441b9461a9546e6f3bc881708": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_408cbd7c3e38479fa036050609ce45cd",
              "IPY_MODEL_85a424e37772441eba3bf96691442082",
              "IPY_MODEL_b493113959ca4398b4d4f6a5dddedbc8"
            ],
            "layout": "IPY_MODEL_f4135b2b90ac4576817fa24323a37d10"
          }
        },
        "408cbd7c3e38479fa036050609ce45cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1443531f27a54a41b1c2d0de46662ddf",
            "placeholder": "​",
            "style": "IPY_MODEL_3a71685bb9784828ba049951bb41ac17",
            "value": " 78%"
          }
        },
        "85a424e37772441eba3bf96691442082": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d61546328b8a45b695cb1ab739fd17c5",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_797c6db07fad4ff4bf257b2f129cd774",
            "value": 132206592
          }
        },
        "b493113959ca4398b4d4f6a5dddedbc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd051bd187054ffc8738b4581b442865",
            "placeholder": "​",
            "style": "IPY_MODEL_81e77c6dfaf84954a203ebca9c45e9b0",
            "value": " 132206592/170498071 [00:04&lt;00:01, 27622104.74it/s]"
          }
        },
        "f4135b2b90ac4576817fa24323a37d10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1443531f27a54a41b1c2d0de46662ddf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a71685bb9784828ba049951bb41ac17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d61546328b8a45b695cb1ab739fd17c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "797c6db07fad4ff4bf257b2f129cd774": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cd051bd187054ffc8738b4581b442865": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81e77c6dfaf84954a203ebca9c45e9b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup PyTorch and Ray Tune\n",
        "\n"
      ],
      "metadata": {
        "id": "jd1fAdsdXbeG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision\n",
        "!pip install ray"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1UUrCMJVIFe",
        "outputId": "e0992621-701f-491f-ca01-933d3b81a1ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.11.1+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: ray in /usr/local/lib/python3.7/dist-packages (1.12.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray) (1.0.3)\n",
            "Requirement already satisfied: protobuf>=3.15.3 in /usr/local/lib/python3.7/dist-packages (from ray) (3.17.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray) (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from ray) (1.21.6)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray) (4.3.3)\n",
            "Requirement already satisfied: virtualenv in /usr/local/lib/python3.7/dist-packages (from ray) (20.14.1)\n",
            "Requirement already satisfied: grpcio<=1.43.0,>=1.28.1 in /usr/local/lib/python3.7/dist-packages (from ray) (1.43.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray) (7.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray) (3.13)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from ray) (21.4.0)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.7/dist-packages (from ray) (1.3.0)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.7/dist-packages (from ray) (1.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ray) (2.23.0)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio<=1.43.0,>=1.28.1->ray) (1.15.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray) (0.18.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray) (4.1.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray) (4.11.3)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray) (5.7.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema->ray) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->ray) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->ray) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ray) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ray) (1.24.3)\n",
            "Requirement already satisfied: distlib<1,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from virtualenv->ray) (0.3.4)\n",
            "Requirement already satisfied: platformdirs<3,>=2 in /usr/local/lib/python3.7/dist-packages (from virtualenv->ray) (2.5.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import random_split\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.models as models\n",
        "from ray import tune\n",
        "from ray.tune import CLIReporter\n",
        "from ray.tune.schedulers import ASHAScheduler"
      ],
      "metadata": {
        "id": "Nbb9tvnHT_PV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper code"
      ],
      "metadata": {
        "id": "oW_tE8n4XAIf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data loader"
      ],
      "metadata": {
        "id": "Hah6W0v5Xuak"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "IqVsM1DqB1Go"
      },
      "outputs": [],
      "source": [
        "def data_loader(batch_size=4):\n",
        "  normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                      std=[0.229, 0.224, 0.225])\n",
        "\n",
        "  cifar10_training_data = torchvision.datasets.CIFAR10(\"/content\", \n",
        "                                              train=True,\n",
        "                                              transform = transforms.Compose([\n",
        "                                                                              transforms.RandomHorizontalFlip(),\n",
        "                                                                              transforms.RandomCrop(32, 4), # size 32x32, padding 4\n",
        "                                                                              transforms.ToTensor(),\n",
        "                                                                              normalize,]), \n",
        "                                              download=True)\n",
        "  \n",
        "  cifar10_val_data = torchvision.datasets.CIFAR10(\"/content\", \n",
        "                                              train=True,\n",
        "                                              transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                                                              normalize,]), \n",
        "                                              download=True)\n",
        "\n",
        "  cifar10_testing_data = torchvision.datasets.CIFAR10(\"/content\", \n",
        "                                              train=False,\n",
        "                                              transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                                                              normalize,]), \n",
        "                                              download=True)\n",
        "  # data = [torch.Size([10, 3, 32, 32]), torch.Size([10])]\n",
        "  num_train = len(cifar10_training_data)\n",
        "  indices = list(range(num_train))\n",
        "  split = 5000 #45k/5k train/val split\n",
        "  train_idx, valid_idx = indices[split:], indices[:split]\n",
        "  train_sampler = SubsetRandomSampler(train_idx)\n",
        "  valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "  cifar10_training_data_loader = torch.utils.data.DataLoader(cifar10_training_data, batch_size, sampler=train_sampler, shuffle=False)\n",
        "  cifar10_val_data_loader = torch.utils.data.DataLoader(cifar10_val_data, batch_size, sampler=valid_sampler, shuffle=False)\n",
        "  cifar10_testing_data_loader = torch.utils.data.DataLoader(cifar10_testing_data, batch_size, shuffle=True)\n",
        "  return cifar10_training_data_loader, cifar10_val_data_loader, cifar10_testing_data_loader\n",
        "\n",
        "  # for data in cifar10_training_data_loader:\n",
        "  #   # print(\"data: \", data)\n",
        "  #   images, labels = data[0], data[1]\n",
        "  #   print(\"images.shape: {}, labels.shape: {}\".format(images.shape, labels.shape))\n",
        "  #   break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_, _, _ = data_loader()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355,
          "referenced_widgets": [
            "96ecb36441b9461a9546e6f3bc881708",
            "408cbd7c3e38479fa036050609ce45cd",
            "85a424e37772441eba3bf96691442082",
            "b493113959ca4398b4d4f6a5dddedbc8",
            "f4135b2b90ac4576817fa24323a37d10",
            "1443531f27a54a41b1c2d0de46662ddf",
            "3a71685bb9784828ba049951bb41ac17",
            "d61546328b8a45b695cb1ab739fd17c5",
            "797c6db07fad4ff4bf257b2f129cd774",
            "cd051bd187054ffc8738b4581b442865",
            "81e77c6dfaf84954a203ebca9c45e9b0"
          ]
        },
        "id": "p8zRRwoSqpd5",
        "outputId": "ecfb4ea8-ced5-4418-e38b-f358245e79c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /content/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "96ecb36441b9461a9546e6f3bc881708"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-e25383aed28d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-3f4c884678f2>\u001b[0m in \u001b[0;36mdata_loader\u001b[0;34m(batch_size)\u001b[0m\n\u001b[1;32m     10\u001b[0m                                                                               \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                                                                               normalize,]), \n\u001b[0;32m---> 12\u001b[0;31m                                               download=True)\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   cifar10_val_data = torchvision.datasets.CIFAR10(\"/content\", \n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Files already downloaded and verified'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mdownload_and_extract_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtgz_md5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_and_extract_archive\u001b[0;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m     \u001b[0mdownload_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0marchive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_url\u001b[0;34m(url, root, filename, md5, max_redirect_hops)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Downloading '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0m_urlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mURLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'https'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36m_urlretrieve\u001b[0;34m(url, filename, chunk_size)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"User-Agent\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUSER_AGENT\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"User-Agent\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUSER_AGENT\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0;31m# Amount is not given (unbounded read) so we must check self.length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "bBcDeGA2QmmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A 2-conv-layer block of ResNet \n",
        "class block(nn.Module):\n",
        "  def __init__(self, num_filters, enable_subsample):\n",
        "    super().__init__()\n",
        "    if enable_subsample:\n",
        "      self.conv1 = nn.Conv2d(num_filters // 2, num_filters, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "    else:\n",
        "      self.conv1 = nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1, bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(num_filters)\n",
        "    self.relu1 = nn.ReLU()\n",
        "\n",
        "    self.conv2 = nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1, bias=False)\n",
        "    self.bn2 = nn.BatchNorm2d(num_filters)\n",
        "    self.relu2 = nn.ReLU()\n",
        "\n",
        "    # Weight initialization as in https://github.com/a-martyn/resnet/blob/master/resnet.py\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Conv2d):\n",
        "          nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "      elif isinstance(m, (nn.BatchNorm2d)):\n",
        "          nn.init.constant_(m.weight,1)\n",
        "          nn.init.constant_(m.bias, 0) \n",
        "\n",
        "  def forward(self, x, enable_skip_connections=False):\n",
        "    out = self.conv1(x)\n",
        "    out = self.bn1(out)\n",
        "    out = self.relu1(out)\n",
        "    out = self.conv2(out)\n",
        "    out = self.bn2(out)\n",
        "    if enable_skip_connections:\n",
        "      # print(out.shape, x.shape)\n",
        "      if out.shape != x.shape:\n",
        "        W_s = nn.Conv2d(x.shape[1], out.shape[1], kernel_size=1, stride=2).to(device='cuda')\n",
        "        x = W_s(x)\n",
        "        # print(\"after\", out.shape, x.shape)\n",
        "      else:\n",
        "        out = x + out\n",
        "    out = self.relu2(out)\n",
        "    return out\n",
        "\n",
        "# ResNet for CIFAR-10 as in paper\n",
        "class ResNet(nn.Module):\n",
        "  def __init__(self, n, enable_skip_connections=False):\n",
        "    super().__init__()\n",
        "    self.skip_connection = enable_skip_connections\n",
        "\n",
        "    self.num_layer1_filters = 16\n",
        "    self.num_layer2_filters = 32\n",
        "    self.num_layer3_filters = 64\n",
        "\n",
        "    self.layer0 = nn.Sequential(\n",
        "        nn.Conv2d(3, self.num_layer1_filters, kernel_size=3, padding=1, bias=False),\n",
        "        nn.BatchNorm2d(self.num_layer1_filters),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    \n",
        "    self.layer1 = nn.ModuleList([block(self.num_layer1_filters, enable_subsample=False) for i in range(n)])\n",
        "    self.layer2_subsample = block(self.num_layer2_filters, enable_subsample=True)\n",
        "    self.layer2 = nn.ModuleList([block(self.num_layer2_filters, enable_subsample=False) for i in range(n-1)])\n",
        "    self.layer3_subsample = block(self.num_layer3_filters, enable_subsample=True)\n",
        "    self.layer3 = nn.ModuleList([block(self.num_layer3_filters, enable_subsample=False) for i in range(n-1)])\n",
        "\n",
        "    self.avgpooling = nn.AdaptiveAvgPool2d(1)\n",
        "    self.fc_layer = nn.Linear(self.num_layer3_filters, 10)\n",
        "    self.softmax = nn.Softmax(dim=1)    \n",
        "  \n",
        "  def forward(self, x):\n",
        "    out = self.layer0(x)\n",
        "    for block in self.layer1:\n",
        "      out = block(out, self.skip_connection)\n",
        "    out = self.layer2_subsample(out, self.skip_connection)\n",
        "    for block in self.layer2:\n",
        "      out = block(out, self.skip_connection)\n",
        "    out = self.layer3_subsample(out, self.skip_connection)\n",
        "    for block in self.layer3:\n",
        "      out = block(out, self.skip_connection)\n",
        "    out = self.avgpooling(out)\n",
        "    out = out.reshape((-1, self.num_layer3_filters))\n",
        "    out = self.fc_layer(out)\n",
        "    out = self.softmax(out)\n",
        "\n",
        "    return out\n",
        "\n",
        "# curr_model = block(32, enable_subsample=False)\n",
        "# print(curr_model)\n",
        "\n",
        "# resnet20_plain = ResNet(3)\n",
        "# resnet20 = ResNet(3, enable_skip_connections=True)\n",
        "\n",
        "# resnet18 = models.resnet18()\n",
        "# print(resnet18)\n",
        "# print(sum(p.numel() for p in resnet20_plain.parameters()))\n",
        "# print(sum(p.numel() for p in resnet20.parameters()))"
      ],
      "metadata": {
        "id": "uUdRy-anQ3Vo"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Training"
      ],
      "metadata": {
        "id": "N3sCLtBu1f0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def resnet_training():\n",
        "\n",
        "  batch_size = 128\n",
        "  net = ResNet(3, True)\n",
        "  #net = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
        "\n",
        "  trainloader, valloader, testloader = data_loader(batch_size)\n",
        "  print(len(trainloader), len(valloader), len(testloader))\n",
        "  classes = ('plane', 'car', 'bird', 'cat',\n",
        "            'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "  device = \"cpu\"\n",
        "  if torch.cuda.is_available():\n",
        "      device = \"cuda:0\"\n",
        "      if torch.cuda.device_count() > 1:\n",
        "          net = nn.DataParallel(net)\n",
        "  net = net.to(device)\n",
        "  # https://discuss.pytorch.org/t/how-to-increase-the-learning-rate-without-using-cyclical-learning-rates/140208/4\n",
        "  def _lr_lambda(current_step):\n",
        "        \"\"\"\n",
        "        _lr_lambda returns a multiplicative factor given an interger parameter epochs.\n",
        "        \"\"\"\n",
        "        if current_step < 400:\n",
        "            _lr =.1\n",
        "        elif current_step < 32000:\n",
        "            _lr = 1\n",
        "        elif current_step < 48000:\n",
        "            _lr = .1\n",
        "        else:\n",
        "            _lr = .01\n",
        "\n",
        "        return _lr\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=0.0001)\n",
        "  scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, _lr_lambda, last_epoch=-1, verbose=False)\n",
        "  for epoch in range(182):  # loop over the dataset multiple times\n",
        "      running_loss = 0.0\n",
        "      for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 10 == 9:    # print every 10 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 10:.3f}')\n",
        "            running_loss = 0.0\n",
        "      \n",
        "      val_loss = 0.0\n",
        "      val_steps = 0\n",
        "      total = 0\n",
        "      correct = 0\n",
        "      for i, data in enumerate(valloader, 0):\n",
        "          with torch.no_grad():\n",
        "              inputs, labels = data\n",
        "              inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "              outputs = net(inputs)\n",
        "              _, predicted = torch.max(outputs.data, 1)\n",
        "              total += labels.size(0)\n",
        "              correct += (predicted == labels).sum().item()\n",
        "\n",
        "              loss = criterion(outputs, labels)\n",
        "              val_loss += loss.cpu().numpy()\n",
        "              val_steps += 1\n",
        "      print(\"epoch {} val_loss {} val_steps {} val_acc {}\".format(epoch, val_loss, val_steps, correct / total))\n",
        "  print('Finished Training')\n",
        "  return net\n",
        "trained_net = resnet_training()\n"
      ],
      "metadata": {
        "id": "d708Mz221-3l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92c50e49-fa2c-407f-de19-fa3f0d3cc9b4"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "[44,    60] loss: 1.628\n",
            "[44,    70] loss: 1.629\n",
            "[44,    80] loss: 1.648\n",
            "[44,    90] loss: 1.630\n",
            "[44,   100] loss: 1.633\n",
            "[44,   110] loss: 1.661\n",
            "[44,   120] loss: 1.641\n",
            "[44,   130] loss: 1.647\n",
            "[44,   140] loss: 1.643\n",
            "[44,   150] loss: 1.634\n",
            "[44,   160] loss: 1.668\n",
            "[44,   170] loss: 1.649\n",
            "[44,   180] loss: 1.649\n",
            "[44,   190] loss: 1.659\n",
            "[44,   200] loss: 1.638\n",
            "[44,   210] loss: 1.649\n",
            "[44,   220] loss: 1.637\n",
            "[44,   230] loss: 1.672\n",
            "[44,   240] loss: 1.652\n",
            "[44,   250] loss: 1.652\n",
            "[44,   260] loss: 1.656\n",
            "[44,   270] loss: 1.628\n",
            "[44,   280] loss: 1.639\n",
            "[44,   290] loss: 1.627\n",
            "[44,   300] loss: 1.643\n",
            "[44,   310] loss: 1.639\n",
            "[44,   320] loss: 1.641\n",
            "[44,   330] loss: 1.672\n",
            "[44,   340] loss: 1.650\n",
            "[44,   350] loss: 1.650\n",
            "epoch 43 val_loss 66.24126553535461 val_steps 40 val_acc 0.8048\n",
            "[45,    10] loss: 1.633\n",
            "[45,    20] loss: 1.639\n",
            "[45,    30] loss: 1.627\n",
            "[45,    40] loss: 1.658\n",
            "[45,    50] loss: 1.625\n",
            "[45,    60] loss: 1.643\n",
            "[45,    70] loss: 1.638\n",
            "[45,    80] loss: 1.641\n",
            "[45,    90] loss: 1.644\n",
            "[45,   100] loss: 1.646\n",
            "[45,   110] loss: 1.650\n",
            "[45,   120] loss: 1.649\n",
            "[45,   130] loss: 1.642\n",
            "[45,   140] loss: 1.632\n",
            "[45,   150] loss: 1.640\n",
            "[45,   160] loss: 1.661\n",
            "[45,   170] loss: 1.647\n",
            "[45,   180] loss: 1.656\n",
            "[45,   190] loss: 1.634\n",
            "[45,   200] loss: 1.652\n",
            "[45,   210] loss: 1.647\n",
            "[45,   220] loss: 1.645\n",
            "[45,   230] loss: 1.644\n",
            "[45,   240] loss: 1.641\n",
            "[45,   250] loss: 1.640\n",
            "[45,   260] loss: 1.643\n",
            "[45,   270] loss: 1.666\n",
            "[45,   280] loss: 1.659\n",
            "[45,   290] loss: 1.631\n",
            "[45,   300] loss: 1.624\n",
            "[45,   310] loss: 1.644\n",
            "[45,   320] loss: 1.642\n",
            "[45,   330] loss: 1.640\n",
            "[45,   340] loss: 1.652\n",
            "[45,   350] loss: 1.659\n",
            "epoch 44 val_loss 66.70099365711212 val_steps 40 val_acc 0.7978\n",
            "[46,    10] loss: 1.656\n",
            "[46,    20] loss: 1.646\n",
            "[46,    30] loss: 1.650\n",
            "[46,    40] loss: 1.652\n",
            "[46,    50] loss: 1.652\n",
            "[46,    60] loss: 1.647\n",
            "[46,    70] loss: 1.618\n",
            "[46,    80] loss: 1.647\n",
            "[46,    90] loss: 1.648\n",
            "[46,   100] loss: 1.646\n",
            "[46,   110] loss: 1.652\n",
            "[46,   120] loss: 1.665\n",
            "[46,   130] loss: 1.642\n",
            "[46,   140] loss: 1.637\n",
            "[46,   150] loss: 1.626\n",
            "[46,   160] loss: 1.636\n",
            "[46,   170] loss: 1.648\n",
            "[46,   180] loss: 1.637\n",
            "[46,   190] loss: 1.635\n",
            "[46,   200] loss: 1.650\n",
            "[46,   210] loss: 1.650\n",
            "[46,   220] loss: 1.628\n",
            "[46,   230] loss: 1.666\n",
            "[46,   240] loss: 1.664\n",
            "[46,   250] loss: 1.658\n",
            "[46,   260] loss: 1.657\n",
            "[46,   270] loss: 1.643\n",
            "[46,   280] loss: 1.639\n",
            "[46,   290] loss: 1.659\n",
            "[46,   300] loss: 1.636\n",
            "[46,   310] loss: 1.671\n",
            "[46,   320] loss: 1.636\n",
            "[46,   330] loss: 1.639\n",
            "[46,   340] loss: 1.646\n",
            "[46,   350] loss: 1.655\n",
            "epoch 45 val_loss 66.93045020103455 val_steps 40 val_acc 0.787\n",
            "[47,    10] loss: 1.631\n",
            "[47,    20] loss: 1.642\n",
            "[47,    30] loss: 1.627\n",
            "[47,    40] loss: 1.623\n",
            "[47,    50] loss: 1.624\n",
            "[47,    60] loss: 1.623\n",
            "[47,    70] loss: 1.641\n",
            "[47,    80] loss: 1.633\n",
            "[47,    90] loss: 1.648\n",
            "[47,   100] loss: 1.645\n",
            "[47,   110] loss: 1.660\n",
            "[47,   120] loss: 1.646\n",
            "[47,   130] loss: 1.638\n",
            "[47,   140] loss: 1.634\n",
            "[47,   150] loss: 1.633\n",
            "[47,   160] loss: 1.620\n",
            "[47,   170] loss: 1.636\n",
            "[47,   180] loss: 1.636\n",
            "[47,   190] loss: 1.632\n",
            "[47,   200] loss: 1.642\n",
            "[47,   210] loss: 1.632\n",
            "[47,   220] loss: 1.635\n",
            "[47,   230] loss: 1.656\n",
            "[47,   240] loss: 1.646\n",
            "[47,   250] loss: 1.640\n",
            "[47,   260] loss: 1.629\n",
            "[47,   270] loss: 1.656\n",
            "[47,   280] loss: 1.645\n",
            "[47,   290] loss: 1.639\n",
            "[47,   300] loss: 1.642\n",
            "[47,   310] loss: 1.635\n",
            "[47,   320] loss: 1.638\n",
            "[47,   330] loss: 1.635\n",
            "[47,   340] loss: 1.636\n",
            "[47,   350] loss: 1.641\n",
            "epoch 46 val_loss 66.90730106830597 val_steps 40 val_acc 0.7908\n",
            "[48,    10] loss: 1.628\n",
            "[48,    20] loss: 1.636\n",
            "[48,    30] loss: 1.638\n",
            "[48,    40] loss: 1.634\n",
            "[48,    50] loss: 1.648\n",
            "[48,    60] loss: 1.633\n",
            "[48,    70] loss: 1.658\n",
            "[48,    80] loss: 1.629\n",
            "[48,    90] loss: 1.632\n",
            "[48,   100] loss: 1.623\n",
            "[48,   110] loss: 1.634\n",
            "[48,   120] loss: 1.634\n",
            "[48,   130] loss: 1.635\n",
            "[48,   140] loss: 1.630\n",
            "[48,   150] loss: 1.644\n",
            "[48,   160] loss: 1.642\n",
            "[48,   170] loss: 1.652\n",
            "[48,   180] loss: 1.633\n",
            "[48,   190] loss: 1.639\n",
            "[48,   200] loss: 1.638\n",
            "[48,   210] loss: 1.644\n",
            "[48,   220] loss: 1.646\n",
            "[48,   230] loss: 1.648\n",
            "[48,   240] loss: 1.650\n",
            "[48,   250] loss: 1.656\n",
            "[48,   260] loss: 1.636\n",
            "[48,   270] loss: 1.618\n",
            "[48,   280] loss: 1.653\n",
            "[48,   290] loss: 1.659\n",
            "[48,   300] loss: 1.650\n",
            "[48,   310] loss: 1.628\n",
            "[48,   320] loss: 1.660\n",
            "[48,   330] loss: 1.628\n",
            "[48,   340] loss: 1.637\n",
            "[48,   350] loss: 1.630\n",
            "epoch 47 val_loss 66.63869857788086 val_steps 40 val_acc 0.7956\n",
            "[49,    10] loss: 1.640\n",
            "[49,    20] loss: 1.645\n",
            "[49,    30] loss: 1.636\n",
            "[49,    40] loss: 1.634\n",
            "[49,    50] loss: 1.641\n",
            "[49,    60] loss: 1.636\n",
            "[49,    70] loss: 1.646\n",
            "[49,    80] loss: 1.648\n",
            "[49,    90] loss: 1.647\n",
            "[49,   100] loss: 1.628\n",
            "[49,   110] loss: 1.626\n",
            "[49,   120] loss: 1.635\n",
            "[49,   130] loss: 1.647\n",
            "[49,   140] loss: 1.630\n",
            "[49,   150] loss: 1.646\n",
            "[49,   160] loss: 1.642\n",
            "[49,   170] loss: 1.646\n",
            "[49,   180] loss: 1.632\n",
            "[49,   190] loss: 1.629\n",
            "[49,   200] loss: 1.655\n",
            "[49,   210] loss: 1.646\n",
            "[49,   220] loss: 1.629\n",
            "[49,   230] loss: 1.641\n",
            "[49,   240] loss: 1.633\n",
            "[49,   250] loss: 1.652\n",
            "[49,   260] loss: 1.622\n",
            "[49,   270] loss: 1.633\n",
            "[49,   280] loss: 1.638\n",
            "[49,   290] loss: 1.620\n",
            "[49,   300] loss: 1.643\n",
            "[49,   310] loss: 1.613\n",
            "[49,   320] loss: 1.630\n",
            "[49,   330] loss: 1.632\n",
            "[49,   340] loss: 1.630\n",
            "[49,   350] loss: 1.656\n",
            "epoch 48 val_loss 66.88538181781769 val_steps 40 val_acc 0.7968\n",
            "[50,    10] loss: 1.631\n",
            "[50,    20] loss: 1.633\n",
            "[50,    30] loss: 1.624\n",
            "[50,    40] loss: 1.630\n",
            "[50,    50] loss: 1.639\n",
            "[50,    60] loss: 1.619\n",
            "[50,    70] loss: 1.645\n",
            "[50,    80] loss: 1.621\n",
            "[50,    90] loss: 1.624\n",
            "[50,   100] loss: 1.627\n",
            "[50,   110] loss: 1.645\n",
            "[50,   120] loss: 1.642\n",
            "[50,   130] loss: 1.651\n",
            "[50,   140] loss: 1.661\n",
            "[50,   150] loss: 1.631\n",
            "[50,   160] loss: 1.654\n",
            "[50,   170] loss: 1.633\n",
            "[50,   180] loss: 1.647\n",
            "[50,   190] loss: 1.631\n",
            "[50,   200] loss: 1.642\n",
            "[50,   210] loss: 1.627\n",
            "[50,   220] loss: 1.642\n",
            "[50,   230] loss: 1.654\n",
            "[50,   240] loss: 1.633\n",
            "[50,   250] loss: 1.641\n",
            "[50,   260] loss: 1.646\n",
            "[50,   270] loss: 1.644\n",
            "[50,   280] loss: 1.637\n",
            "[50,   290] loss: 1.636\n",
            "[50,   300] loss: 1.634\n",
            "[50,   310] loss: 1.666\n",
            "[50,   320] loss: 1.643\n",
            "[50,   330] loss: 1.640\n",
            "[50,   340] loss: 1.667\n",
            "[50,   350] loss: 1.630\n",
            "epoch 49 val_loss 67.3123015165329 val_steps 40 val_acc 0.7876\n",
            "[51,    10] loss: 1.657\n",
            "[51,    20] loss: 1.637\n",
            "[51,    30] loss: 1.662\n",
            "[51,    40] loss: 1.656\n",
            "[51,    50] loss: 1.650\n",
            "[51,    60] loss: 1.623\n",
            "[51,    70] loss: 1.639\n",
            "[51,    80] loss: 1.635\n",
            "[51,    90] loss: 1.661\n",
            "[51,   100] loss: 1.621\n",
            "[51,   110] loss: 1.629\n",
            "[51,   120] loss: 1.638\n",
            "[51,   130] loss: 1.625\n",
            "[51,   140] loss: 1.638\n",
            "[51,   150] loss: 1.660\n",
            "[51,   160] loss: 1.622\n",
            "[51,   170] loss: 1.650\n",
            "[51,   180] loss: 1.641\n",
            "[51,   190] loss: 1.643\n",
            "[51,   200] loss: 1.635\n",
            "[51,   210] loss: 1.629\n",
            "[51,   220] loss: 1.654\n",
            "[51,   230] loss: 1.624\n",
            "[51,   240] loss: 1.671\n",
            "[51,   250] loss: 1.647\n",
            "[51,   260] loss: 1.646\n",
            "[51,   270] loss: 1.630\n",
            "[51,   280] loss: 1.639\n",
            "[51,   290] loss: 1.633\n",
            "[51,   300] loss: 1.631\n",
            "[51,   310] loss: 1.635\n",
            "[51,   320] loss: 1.634\n",
            "[51,   330] loss: 1.639\n",
            "[51,   340] loss: 1.636\n",
            "[51,   350] loss: 1.639\n",
            "epoch 50 val_loss 65.80301773548126 val_steps 40 val_acc 0.8204\n",
            "[52,    10] loss: 1.634\n",
            "[52,    20] loss: 1.632\n",
            "[52,    30] loss: 1.641\n",
            "[52,    40] loss: 1.618\n",
            "[52,    50] loss: 1.639\n",
            "[52,    60] loss: 1.635\n",
            "[52,    70] loss: 1.636\n",
            "[52,    80] loss: 1.651\n",
            "[52,    90] loss: 1.631\n",
            "[52,   100] loss: 1.639\n",
            "[52,   110] loss: 1.624\n",
            "[52,   120] loss: 1.640\n",
            "[52,   130] loss: 1.649\n",
            "[52,   140] loss: 1.627\n",
            "[52,   150] loss: 1.662\n",
            "[52,   160] loss: 1.651\n",
            "[52,   170] loss: 1.640\n",
            "[52,   180] loss: 1.629\n",
            "[52,   190] loss: 1.629\n",
            "[52,   200] loss: 1.619\n",
            "[52,   210] loss: 1.650\n",
            "[52,   220] loss: 1.634\n",
            "[52,   230] loss: 1.656\n",
            "[52,   240] loss: 1.637\n",
            "[52,   250] loss: 1.627\n",
            "[52,   260] loss: 1.625\n",
            "[52,   270] loss: 1.626\n",
            "[52,   280] loss: 1.630\n",
            "[52,   290] loss: 1.651\n",
            "[52,   300] loss: 1.637\n",
            "[52,   310] loss: 1.638\n",
            "[52,   320] loss: 1.640\n",
            "[52,   330] loss: 1.642\n",
            "[52,   340] loss: 1.621\n",
            "[52,   350] loss: 1.639\n",
            "epoch 51 val_loss 66.45343220233917 val_steps 40 val_acc 0.798\n",
            "[53,    10] loss: 1.633\n",
            "[53,    20] loss: 1.634\n",
            "[53,    30] loss: 1.610\n",
            "[53,    40] loss: 1.643\n",
            "[53,    50] loss: 1.617\n",
            "[53,    60] loss: 1.637\n",
            "[53,    70] loss: 1.625\n",
            "[53,    80] loss: 1.636\n",
            "[53,    90] loss: 1.645\n",
            "[53,   100] loss: 1.632\n",
            "[53,   110] loss: 1.642\n",
            "[53,   120] loss: 1.618\n",
            "[53,   130] loss: 1.647\n",
            "[53,   140] loss: 1.644\n",
            "[53,   150] loss: 1.631\n",
            "[53,   160] loss: 1.631\n",
            "[53,   170] loss: 1.631\n",
            "[53,   180] loss: 1.639\n",
            "[53,   190] loss: 1.623\n",
            "[53,   200] loss: 1.645\n",
            "[53,   210] loss: 1.639\n",
            "[53,   220] loss: 1.634\n",
            "[53,   230] loss: 1.634\n",
            "[53,   240] loss: 1.629\n",
            "[53,   250] loss: 1.627\n",
            "[53,   260] loss: 1.644\n",
            "[53,   270] loss: 1.644\n",
            "[53,   280] loss: 1.643\n",
            "[53,   290] loss: 1.639\n",
            "[53,   300] loss: 1.638\n",
            "[53,   310] loss: 1.645\n",
            "[53,   320] loss: 1.649\n",
            "[53,   330] loss: 1.647\n",
            "[53,   340] loss: 1.640\n",
            "[53,   350] loss: 1.637\n",
            "epoch 52 val_loss 66.4576187133789 val_steps 40 val_acc 0.8084\n",
            "[54,    10] loss: 1.642\n",
            "[54,    20] loss: 1.651\n",
            "[54,    30] loss: 1.633\n",
            "[54,    40] loss: 1.642\n",
            "[54,    50] loss: 1.630\n",
            "[54,    60] loss: 1.646\n",
            "[54,    70] loss: 1.637\n",
            "[54,    80] loss: 1.627\n",
            "[54,    90] loss: 1.640\n",
            "[54,   100] loss: 1.630\n",
            "[54,   110] loss: 1.632\n",
            "[54,   120] loss: 1.618\n",
            "[54,   130] loss: 1.632\n",
            "[54,   140] loss: 1.629\n",
            "[54,   150] loss: 1.631\n",
            "[54,   160] loss: 1.640\n",
            "[54,   170] loss: 1.647\n",
            "[54,   180] loss: 1.638\n",
            "[54,   190] loss: 1.653\n",
            "[54,   200] loss: 1.646\n",
            "[54,   210] loss: 1.646\n",
            "[54,   220] loss: 1.632\n",
            "[54,   230] loss: 1.627\n",
            "[54,   240] loss: 1.615\n",
            "[54,   250] loss: 1.625\n",
            "[54,   260] loss: 1.643\n",
            "[54,   270] loss: 1.633\n",
            "[54,   280] loss: 1.636\n",
            "[54,   290] loss: 1.639\n",
            "[54,   300] loss: 1.619\n",
            "[54,   310] loss: 1.635\n",
            "[54,   320] loss: 1.638\n",
            "[54,   330] loss: 1.636\n",
            "[54,   340] loss: 1.620\n",
            "[54,   350] loss: 1.634\n",
            "epoch 53 val_loss 67.03941559791565 val_steps 40 val_acc 0.7904\n",
            "[55,    10] loss: 1.646\n",
            "[55,    20] loss: 1.629\n",
            "[55,    30] loss: 1.640\n",
            "[55,    40] loss: 1.625\n",
            "[55,    50] loss: 1.634\n",
            "[55,    60] loss: 1.635\n",
            "[55,    70] loss: 1.641\n",
            "[55,    80] loss: 1.624\n",
            "[55,    90] loss: 1.625\n",
            "[55,   100] loss: 1.622\n",
            "[55,   110] loss: 1.638\n",
            "[55,   120] loss: 1.660\n",
            "[55,   130] loss: 1.626\n",
            "[55,   140] loss: 1.629\n",
            "[55,   150] loss: 1.640\n",
            "[55,   160] loss: 1.626\n",
            "[55,   170] loss: 1.630\n",
            "[55,   180] loss: 1.631\n",
            "[55,   190] loss: 1.634\n",
            "[55,   200] loss: 1.628\n",
            "[55,   210] loss: 1.637\n",
            "[55,   220] loss: 1.629\n",
            "[55,   230] loss: 1.635\n",
            "[55,   240] loss: 1.660\n",
            "[55,   250] loss: 1.651\n",
            "[55,   260] loss: 1.622\n",
            "[55,   270] loss: 1.644\n",
            "[55,   280] loss: 1.632\n",
            "[55,   290] loss: 1.641\n",
            "[55,   300] loss: 1.642\n",
            "[55,   310] loss: 1.623\n",
            "[55,   320] loss: 1.633\n",
            "[55,   330] loss: 1.634\n",
            "[55,   340] loss: 1.642\n",
            "[55,   350] loss: 1.624\n",
            "epoch 54 val_loss 66.583571434021 val_steps 40 val_acc 0.7998\n",
            "[56,    10] loss: 1.623\n",
            "[56,    20] loss: 1.637\n",
            "[56,    30] loss: 1.629\n",
            "[56,    40] loss: 1.643\n",
            "[56,    50] loss: 1.631\n",
            "[56,    60] loss: 1.606\n",
            "[56,    70] loss: 1.620\n",
            "[56,    80] loss: 1.629\n",
            "[56,    90] loss: 1.655\n",
            "[56,   100] loss: 1.641\n",
            "[56,   110] loss: 1.671\n",
            "[56,   120] loss: 1.636\n",
            "[56,   130] loss: 1.626\n",
            "[56,   140] loss: 1.635\n",
            "[56,   150] loss: 1.630\n",
            "[56,   160] loss: 1.626\n",
            "[56,   170] loss: 1.623\n",
            "[56,   180] loss: 1.645\n",
            "[56,   190] loss: 1.629\n",
            "[56,   200] loss: 1.644\n",
            "[56,   210] loss: 1.649\n",
            "[56,   220] loss: 1.631\n",
            "[56,   230] loss: 1.630\n",
            "[56,   240] loss: 1.619\n",
            "[56,   250] loss: 1.627\n",
            "[56,   260] loss: 1.627\n",
            "[56,   270] loss: 1.638\n",
            "[56,   280] loss: 1.635\n",
            "[56,   290] loss: 1.661\n",
            "[56,   300] loss: 1.636\n",
            "[56,   310] loss: 1.630\n",
            "[56,   320] loss: 1.644\n",
            "[56,   330] loss: 1.637\n",
            "[56,   340] loss: 1.648\n",
            "[56,   350] loss: 1.636\n",
            "epoch 55 val_loss 67.05516195297241 val_steps 40 val_acc 0.7904\n",
            "[57,    10] loss: 1.635\n",
            "[57,    20] loss: 1.625\n",
            "[57,    30] loss: 1.627\n",
            "[57,    40] loss: 1.643\n",
            "[57,    50] loss: 1.633\n",
            "[57,    60] loss: 1.617\n",
            "[57,    70] loss: 1.630\n",
            "[57,    80] loss: 1.655\n",
            "[57,    90] loss: 1.641\n",
            "[57,   100] loss: 1.631\n",
            "[57,   110] loss: 1.645\n",
            "[57,   120] loss: 1.643\n",
            "[57,   130] loss: 1.624\n",
            "[57,   140] loss: 1.622\n",
            "[57,   150] loss: 1.627\n",
            "[57,   160] loss: 1.637\n",
            "[57,   170] loss: 1.621\n",
            "[57,   180] loss: 1.634\n",
            "[57,   190] loss: 1.638\n",
            "[57,   200] loss: 1.614\n",
            "[57,   210] loss: 1.630\n",
            "[57,   220] loss: 1.647\n",
            "[57,   230] loss: 1.626\n",
            "[57,   240] loss: 1.634\n",
            "[57,   250] loss: 1.655\n",
            "[57,   260] loss: 1.625\n",
            "[57,   270] loss: 1.645\n",
            "[57,   280] loss: 1.643\n",
            "[57,   290] loss: 1.636\n",
            "[57,   300] loss: 1.623\n",
            "[57,   310] loss: 1.623\n",
            "[57,   320] loss: 1.628\n",
            "[57,   330] loss: 1.646\n",
            "[57,   340] loss: 1.635\n",
            "[57,   350] loss: 1.637\n",
            "epoch 56 val_loss 66.25882971286774 val_steps 40 val_acc 0.81\n",
            "[58,    10] loss: 1.629\n",
            "[58,    20] loss: 1.618\n",
            "[58,    30] loss: 1.644\n",
            "[58,    40] loss: 1.641\n",
            "[58,    50] loss: 1.630\n",
            "[58,    60] loss: 1.609\n",
            "[58,    70] loss: 1.627\n",
            "[58,    80] loss: 1.638\n",
            "[58,    90] loss: 1.633\n",
            "[58,   100] loss: 1.617\n",
            "[58,   110] loss: 1.631\n",
            "[58,   120] loss: 1.632\n",
            "[58,   130] loss: 1.637\n",
            "[58,   140] loss: 1.633\n",
            "[58,   150] loss: 1.663\n",
            "[58,   160] loss: 1.630\n",
            "[58,   170] loss: 1.611\n",
            "[58,   180] loss: 1.631\n",
            "[58,   190] loss: 1.611\n",
            "[58,   200] loss: 1.624\n",
            "[58,   210] loss: 1.633\n",
            "[58,   220] loss: 1.631\n",
            "[58,   230] loss: 1.636\n",
            "[58,   240] loss: 1.637\n",
            "[58,   250] loss: 1.630\n",
            "[58,   260] loss: 1.628\n",
            "[58,   270] loss: 1.640\n",
            "[58,   280] loss: 1.634\n",
            "[58,   290] loss: 1.645\n",
            "[58,   300] loss: 1.637\n",
            "[58,   310] loss: 1.620\n",
            "[58,   320] loss: 1.638\n",
            "[58,   330] loss: 1.628\n",
            "[58,   340] loss: 1.628\n",
            "[58,   350] loss: 1.646\n",
            "epoch 57 val_loss 66.30185234546661 val_steps 40 val_acc 0.8106\n",
            "[59,    10] loss: 1.637\n",
            "[59,    20] loss: 1.620\n",
            "[59,    30] loss: 1.626\n",
            "[59,    40] loss: 1.639\n",
            "[59,    50] loss: 1.623\n",
            "[59,    60] loss: 1.635\n",
            "[59,    70] loss: 1.611\n",
            "[59,    80] loss: 1.622\n",
            "[59,    90] loss: 1.652\n",
            "[59,   100] loss: 1.605\n",
            "[59,   110] loss: 1.621\n",
            "[59,   120] loss: 1.616\n",
            "[59,   130] loss: 1.630\n",
            "[59,   140] loss: 1.614\n",
            "[59,   150] loss: 1.641\n",
            "[59,   160] loss: 1.619\n",
            "[59,   170] loss: 1.630\n",
            "[59,   180] loss: 1.635\n",
            "[59,   190] loss: 1.636\n",
            "[59,   200] loss: 1.621\n",
            "[59,   210] loss: 1.631\n",
            "[59,   220] loss: 1.621\n",
            "[59,   230] loss: 1.625\n",
            "[59,   240] loss: 1.615\n",
            "[59,   250] loss: 1.616\n",
            "[59,   260] loss: 1.630\n",
            "[59,   270] loss: 1.640\n",
            "[59,   280] loss: 1.637\n",
            "[59,   290] loss: 1.641\n",
            "[59,   300] loss: 1.648\n",
            "[59,   310] loss: 1.632\n",
            "[59,   320] loss: 1.630\n",
            "[59,   330] loss: 1.650\n",
            "[59,   340] loss: 1.630\n",
            "[59,   350] loss: 1.636\n",
            "epoch 58 val_loss 66.29909098148346 val_steps 40 val_acc 0.8024\n",
            "[60,    10] loss: 1.609\n",
            "[60,    20] loss: 1.642\n",
            "[60,    30] loss: 1.647\n",
            "[60,    40] loss: 1.623\n",
            "[60,    50] loss: 1.635\n",
            "[60,    60] loss: 1.631\n",
            "[60,    70] loss: 1.631\n",
            "[60,    80] loss: 1.639\n",
            "[60,    90] loss: 1.624\n",
            "[60,   100] loss: 1.634\n",
            "[60,   110] loss: 1.613\n",
            "[60,   120] loss: 1.611\n",
            "[60,   130] loss: 1.629\n",
            "[60,   140] loss: 1.631\n",
            "[60,   150] loss: 1.625\n",
            "[60,   160] loss: 1.628\n",
            "[60,   170] loss: 1.620\n",
            "[60,   180] loss: 1.628\n",
            "[60,   190] loss: 1.626\n",
            "[60,   200] loss: 1.620\n",
            "[60,   210] loss: 1.632\n",
            "[60,   220] loss: 1.630\n",
            "[60,   230] loss: 1.626\n",
            "[60,   240] loss: 1.640\n",
            "[60,   250] loss: 1.622\n",
            "[60,   260] loss: 1.636\n",
            "[60,   270] loss: 1.632\n",
            "[60,   280] loss: 1.649\n",
            "[60,   290] loss: 1.622\n",
            "[60,   300] loss: 1.623\n",
            "[60,   310] loss: 1.621\n",
            "[60,   320] loss: 1.643\n",
            "[60,   330] loss: 1.630\n",
            "[60,   340] loss: 1.623\n",
            "[60,   350] loss: 1.637\n",
            "epoch 59 val_loss 65.97182559967041 val_steps 40 val_acc 0.8154\n",
            "[61,    10] loss: 1.619\n",
            "[61,    20] loss: 1.635\n",
            "[61,    30] loss: 1.622\n",
            "[61,    40] loss: 1.631\n",
            "[61,    50] loss: 1.622\n",
            "[61,    60] loss: 1.641\n",
            "[61,    70] loss: 1.633\n",
            "[61,    80] loss: 1.621\n",
            "[61,    90] loss: 1.625\n",
            "[61,   100] loss: 1.621\n",
            "[61,   110] loss: 1.640\n",
            "[61,   120] loss: 1.613\n",
            "[61,   130] loss: 1.633\n",
            "[61,   140] loss: 1.652\n",
            "[61,   150] loss: 1.620\n",
            "[61,   160] loss: 1.622\n",
            "[61,   170] loss: 1.622\n",
            "[61,   180] loss: 1.630\n",
            "[61,   190] loss: 1.630\n",
            "[61,   200] loss: 1.620\n",
            "[61,   210] loss: 1.621\n",
            "[61,   220] loss: 1.646\n",
            "[61,   230] loss: 1.639\n",
            "[61,   240] loss: 1.638\n",
            "[61,   250] loss: 1.621\n",
            "[61,   260] loss: 1.638\n",
            "[61,   270] loss: 1.632\n",
            "[61,   280] loss: 1.626\n",
            "[61,   290] loss: 1.623\n",
            "[61,   300] loss: 1.640\n",
            "[61,   310] loss: 1.619\n",
            "[61,   320] loss: 1.639\n",
            "[61,   330] loss: 1.624\n",
            "[61,   340] loss: 1.627\n",
            "[61,   350] loss: 1.631\n",
            "epoch 60 val_loss 66.94751644134521 val_steps 40 val_acc 0.7942\n",
            "[62,    10] loss: 1.623\n",
            "[62,    20] loss: 1.616\n",
            "[62,    30] loss: 1.642\n",
            "[62,    40] loss: 1.627\n",
            "[62,    50] loss: 1.637\n",
            "[62,    60] loss: 1.637\n",
            "[62,    70] loss: 1.626\n",
            "[62,    80] loss: 1.626\n",
            "[62,    90] loss: 1.639\n",
            "[62,   100] loss: 1.623\n",
            "[62,   110] loss: 1.637\n",
            "[62,   120] loss: 1.634\n",
            "[62,   130] loss: 1.646\n",
            "[62,   140] loss: 1.631\n",
            "[62,   150] loss: 1.597\n",
            "[62,   160] loss: 1.636\n",
            "[62,   170] loss: 1.634\n",
            "[62,   180] loss: 1.627\n",
            "[62,   190] loss: 1.621\n",
            "[62,   200] loss: 1.623\n",
            "[62,   210] loss: 1.625\n",
            "[62,   220] loss: 1.625\n",
            "[62,   230] loss: 1.616\n",
            "[62,   240] loss: 1.620\n",
            "[62,   250] loss: 1.615\n",
            "[62,   260] loss: 1.631\n",
            "[62,   270] loss: 1.639\n",
            "[62,   280] loss: 1.658\n",
            "[62,   290] loss: 1.628\n",
            "[62,   300] loss: 1.625\n",
            "[62,   310] loss: 1.641\n",
            "[62,   320] loss: 1.658\n",
            "[62,   330] loss: 1.648\n",
            "[62,   340] loss: 1.632\n",
            "[62,   350] loss: 1.615\n",
            "epoch 61 val_loss 66.63729012012482 val_steps 40 val_acc 0.803\n",
            "[63,    10] loss: 1.617\n",
            "[63,    20] loss: 1.624\n",
            "[63,    30] loss: 1.648\n",
            "[63,    40] loss: 1.611\n",
            "[63,    50] loss: 1.637\n",
            "[63,    60] loss: 1.608\n",
            "[63,    70] loss: 1.613\n",
            "[63,    80] loss: 1.650\n",
            "[63,    90] loss: 1.633\n",
            "[63,   100] loss: 1.630\n",
            "[63,   110] loss: 1.641\n",
            "[63,   120] loss: 1.625\n",
            "[63,   130] loss: 1.630\n",
            "[63,   140] loss: 1.641\n",
            "[63,   150] loss: 1.633\n",
            "[63,   160] loss: 1.642\n",
            "[63,   170] loss: 1.630\n",
            "[63,   180] loss: 1.624\n",
            "[63,   190] loss: 1.618\n",
            "[63,   200] loss: 1.619\n",
            "[63,   210] loss: 1.631\n",
            "[63,   220] loss: 1.645\n",
            "[63,   230] loss: 1.628\n",
            "[63,   240] loss: 1.621\n",
            "[63,   250] loss: 1.645\n",
            "[63,   260] loss: 1.615\n",
            "[63,   270] loss: 1.634\n",
            "[63,   280] loss: 1.621\n",
            "[63,   290] loss: 1.634\n",
            "[63,   300] loss: 1.621\n",
            "[63,   310] loss: 1.628\n",
            "[63,   320] loss: 1.634\n",
            "[63,   330] loss: 1.634\n",
            "[63,   340] loss: 1.630\n",
            "[63,   350] loss: 1.619\n",
            "epoch 62 val_loss 66.4759361743927 val_steps 40 val_acc 0.806\n",
            "[64,    10] loss: 1.629\n",
            "[64,    20] loss: 1.640\n",
            "[64,    30] loss: 1.616\n",
            "[64,    40] loss: 1.625\n",
            "[64,    50] loss: 1.628\n",
            "[64,    60] loss: 1.638\n",
            "[64,    70] loss: 1.616\n",
            "[64,    80] loss: 1.638\n",
            "[64,    90] loss: 1.626\n",
            "[64,   100] loss: 1.634\n",
            "[64,   110] loss: 1.637\n",
            "[64,   120] loss: 1.630\n",
            "[64,   130] loss: 1.635\n",
            "[64,   140] loss: 1.642\n",
            "[64,   150] loss: 1.625\n",
            "[64,   160] loss: 1.627\n",
            "[64,   170] loss: 1.644\n",
            "[64,   180] loss: 1.633\n",
            "[64,   190] loss: 1.610\n",
            "[64,   200] loss: 1.652\n",
            "[64,   210] loss: 1.638\n",
            "[64,   220] loss: 1.626\n",
            "[64,   230] loss: 1.631\n",
            "[64,   240] loss: 1.626\n",
            "[64,   250] loss: 1.620\n",
            "[64,   260] loss: 1.617\n",
            "[64,   270] loss: 1.626\n",
            "[64,   280] loss: 1.627\n",
            "[64,   290] loss: 1.648\n",
            "[64,   300] loss: 1.611\n",
            "[64,   310] loss: 1.615\n",
            "[64,   320] loss: 1.636\n",
            "[64,   330] loss: 1.619\n",
            "[64,   340] loss: 1.637\n",
            "[64,   350] loss: 1.632\n",
            "epoch 63 val_loss 66.38328635692596 val_steps 40 val_acc 0.8026\n",
            "[65,    10] loss: 1.632\n",
            "[65,    20] loss: 1.625\n",
            "[65,    30] loss: 1.604\n",
            "[65,    40] loss: 1.624\n",
            "[65,    50] loss: 1.619\n",
            "[65,    60] loss: 1.622\n",
            "[65,    70] loss: 1.641\n",
            "[65,    80] loss: 1.638\n",
            "[65,    90] loss: 1.643\n",
            "[65,   100] loss: 1.637\n",
            "[65,   110] loss: 1.623\n",
            "[65,   120] loss: 1.637\n",
            "[65,   130] loss: 1.638\n",
            "[65,   140] loss: 1.625\n",
            "[65,   150] loss: 1.632\n",
            "[65,   160] loss: 1.625\n",
            "[65,   170] loss: 1.631\n",
            "[65,   180] loss: 1.622\n",
            "[65,   190] loss: 1.611\n",
            "[65,   200] loss: 1.626\n",
            "[65,   210] loss: 1.634\n",
            "[65,   220] loss: 1.643\n",
            "[65,   230] loss: 1.637\n",
            "[65,   240] loss: 1.621\n",
            "[65,   250] loss: 1.623\n",
            "[65,   260] loss: 1.631\n",
            "[65,   270] loss: 1.623\n",
            "[65,   280] loss: 1.639\n",
            "[65,   290] loss: 1.633\n",
            "[65,   300] loss: 1.631\n",
            "[65,   310] loss: 1.634\n",
            "[65,   320] loss: 1.639\n",
            "[65,   330] loss: 1.629\n",
            "[65,   340] loss: 1.616\n",
            "[65,   350] loss: 1.624\n",
            "epoch 64 val_loss 66.5426607131958 val_steps 40 val_acc 0.8014\n",
            "[66,    10] loss: 1.626\n",
            "[66,    20] loss: 1.622\n",
            "[66,    30] loss: 1.606\n",
            "[66,    40] loss: 1.607\n",
            "[66,    50] loss: 1.639\n",
            "[66,    60] loss: 1.623\n",
            "[66,    70] loss: 1.628\n",
            "[66,    80] loss: 1.622\n",
            "[66,    90] loss: 1.607\n",
            "[66,   100] loss: 1.630\n",
            "[66,   110] loss: 1.649\n",
            "[66,   120] loss: 1.620\n",
            "[66,   130] loss: 1.620\n",
            "[66,   140] loss: 1.618\n",
            "[66,   150] loss: 1.641\n",
            "[66,   160] loss: 1.633\n",
            "[66,   170] loss: 1.641\n",
            "[66,   180] loss: 1.631\n",
            "[66,   190] loss: 1.641\n",
            "[66,   200] loss: 1.630\n",
            "[66,   210] loss: 1.639\n",
            "[66,   220] loss: 1.642\n",
            "[66,   230] loss: 1.633\n",
            "[66,   240] loss: 1.634\n",
            "[66,   250] loss: 1.635\n",
            "[66,   260] loss: 1.634\n",
            "[66,   270] loss: 1.632\n",
            "[66,   280] loss: 1.629\n",
            "[66,   290] loss: 1.630\n",
            "[66,   300] loss: 1.623\n",
            "[66,   310] loss: 1.615\n",
            "[66,   320] loss: 1.636\n",
            "[66,   330] loss: 1.639\n",
            "[66,   340] loss: 1.628\n",
            "[66,   350] loss: 1.634\n",
            "epoch 65 val_loss 66.13713097572327 val_steps 40 val_acc 0.8176\n",
            "[67,    10] loss: 1.614\n",
            "[67,    20] loss: 1.614\n",
            "[67,    30] loss: 1.616\n",
            "[67,    40] loss: 1.619\n",
            "[67,    50] loss: 1.618\n",
            "[67,    60] loss: 1.635\n",
            "[67,    70] loss: 1.633\n",
            "[67,    80] loss: 1.631\n",
            "[67,    90] loss: 1.622\n",
            "[67,   100] loss: 1.633\n",
            "[67,   110] loss: 1.615\n",
            "[67,   120] loss: 1.632\n",
            "[67,   130] loss: 1.614\n",
            "[67,   140] loss: 1.636\n",
            "[67,   150] loss: 1.622\n",
            "[67,   160] loss: 1.624\n",
            "[67,   170] loss: 1.625\n",
            "[67,   180] loss: 1.616\n",
            "[67,   190] loss: 1.634\n",
            "[67,   200] loss: 1.625\n",
            "[67,   210] loss: 1.637\n",
            "[67,   220] loss: 1.622\n",
            "[67,   230] loss: 1.644\n",
            "[67,   240] loss: 1.634\n",
            "[67,   250] loss: 1.620\n",
            "[67,   260] loss: 1.633\n",
            "[67,   270] loss: 1.626\n",
            "[67,   280] loss: 1.634\n",
            "[67,   290] loss: 1.632\n",
            "[67,   300] loss: 1.624\n",
            "[67,   310] loss: 1.612\n",
            "[67,   320] loss: 1.618\n",
            "[67,   330] loss: 1.626\n",
            "[67,   340] loss: 1.622\n",
            "[67,   350] loss: 1.635\n",
            "epoch 66 val_loss 66.84095287322998 val_steps 40 val_acc 0.7884\n",
            "[68,    10] loss: 1.622\n",
            "[68,    20] loss: 1.628\n",
            "[68,    30] loss: 1.642\n",
            "[68,    40] loss: 1.622\n",
            "[68,    50] loss: 1.624\n",
            "[68,    60] loss: 1.614\n",
            "[68,    70] loss: 1.618\n",
            "[68,    80] loss: 1.609\n",
            "[68,    90] loss: 1.615\n",
            "[68,   100] loss: 1.625\n",
            "[68,   110] loss: 1.640\n",
            "[68,   120] loss: 1.630\n",
            "[68,   130] loss: 1.627\n",
            "[68,   140] loss: 1.631\n",
            "[68,   150] loss: 1.630\n",
            "[68,   160] loss: 1.636\n",
            "[68,   170] loss: 1.627\n",
            "[68,   180] loss: 1.628\n",
            "[68,   190] loss: 1.600\n",
            "[68,   200] loss: 1.613\n",
            "[68,   210] loss: 1.623\n",
            "[68,   220] loss: 1.635\n",
            "[68,   230] loss: 1.626\n",
            "[68,   240] loss: 1.621\n",
            "[68,   250] loss: 1.644\n",
            "[68,   260] loss: 1.628\n",
            "[68,   270] loss: 1.604\n",
            "[68,   280] loss: 1.637\n",
            "[68,   290] loss: 1.622\n",
            "[68,   300] loss: 1.628\n",
            "[68,   310] loss: 1.646\n",
            "[68,   320] loss: 1.621\n",
            "[68,   330] loss: 1.639\n",
            "[68,   340] loss: 1.633\n",
            "[68,   350] loss: 1.627\n",
            "epoch 67 val_loss 66.0868376493454 val_steps 40 val_acc 0.818\n",
            "[69,    10] loss: 1.631\n",
            "[69,    20] loss: 1.617\n",
            "[69,    30] loss: 1.629\n",
            "[69,    40] loss: 1.601\n",
            "[69,    50] loss: 1.616\n",
            "[69,    60] loss: 1.607\n",
            "[69,    70] loss: 1.615\n",
            "[69,    80] loss: 1.622\n",
            "[69,    90] loss: 1.626\n",
            "[69,   100] loss: 1.619\n",
            "[69,   110] loss: 1.653\n",
            "[69,   120] loss: 1.624\n",
            "[69,   130] loss: 1.631\n",
            "[69,   140] loss: 1.612\n",
            "[69,   150] loss: 1.631\n",
            "[69,   160] loss: 1.619\n",
            "[69,   170] loss: 1.613\n",
            "[69,   180] loss: 1.639\n",
            "[69,   190] loss: 1.629\n",
            "[69,   200] loss: 1.616\n",
            "[69,   210] loss: 1.631\n",
            "[69,   220] loss: 1.624\n",
            "[69,   230] loss: 1.624\n",
            "[69,   240] loss: 1.617\n",
            "[69,   250] loss: 1.614\n",
            "[69,   260] loss: 1.645\n",
            "[69,   270] loss: 1.620\n",
            "[69,   280] loss: 1.639\n",
            "[69,   290] loss: 1.629\n",
            "[69,   300] loss: 1.634\n",
            "[69,   310] loss: 1.616\n",
            "[69,   320] loss: 1.633\n",
            "[69,   330] loss: 1.641\n",
            "[69,   340] loss: 1.621\n",
            "[69,   350] loss: 1.624\n",
            "epoch 68 val_loss 66.26095306873322 val_steps 40 val_acc 0.807\n",
            "[70,    10] loss: 1.612\n",
            "[70,    20] loss: 1.637\n",
            "[70,    30] loss: 1.634\n",
            "[70,    40] loss: 1.632\n",
            "[70,    50] loss: 1.623\n",
            "[70,    60] loss: 1.619\n",
            "[70,    70] loss: 1.608\n",
            "[70,    80] loss: 1.618\n",
            "[70,    90] loss: 1.635\n",
            "[70,   100] loss: 1.606\n",
            "[70,   110] loss: 1.608\n",
            "[70,   120] loss: 1.632\n",
            "[70,   130] loss: 1.616\n",
            "[70,   140] loss: 1.618\n",
            "[70,   150] loss: 1.603\n",
            "[70,   160] loss: 1.627\n",
            "[70,   170] loss: 1.617\n",
            "[70,   180] loss: 1.610\n",
            "[70,   190] loss: 1.620\n",
            "[70,   200] loss: 1.602\n",
            "[70,   210] loss: 1.633\n",
            "[70,   220] loss: 1.645\n",
            "[70,   230] loss: 1.625\n",
            "[70,   240] loss: 1.642\n",
            "[70,   250] loss: 1.614\n",
            "[70,   260] loss: 1.638\n",
            "[70,   270] loss: 1.619\n",
            "[70,   280] loss: 1.637\n",
            "[70,   290] loss: 1.635\n",
            "[70,   300] loss: 1.615\n",
            "[70,   310] loss: 1.619\n",
            "[70,   320] loss: 1.621\n",
            "[70,   330] loss: 1.634\n",
            "[70,   340] loss: 1.637\n",
            "[70,   350] loss: 1.627\n",
            "epoch 69 val_loss 66.02931499481201 val_steps 40 val_acc 0.8148\n",
            "[71,    10] loss: 1.621\n",
            "[71,    20] loss: 1.649\n",
            "[71,    30] loss: 1.645\n",
            "[71,    40] loss: 1.612\n",
            "[71,    50] loss: 1.621\n",
            "[71,    60] loss: 1.635\n",
            "[71,    70] loss: 1.614\n",
            "[71,    80] loss: 1.620\n",
            "[71,    90] loss: 1.629\n",
            "[71,   100] loss: 1.610\n",
            "[71,   110] loss: 1.616\n",
            "[71,   120] loss: 1.628\n",
            "[71,   130] loss: 1.633\n",
            "[71,   140] loss: 1.617\n",
            "[71,   150] loss: 1.637\n",
            "[71,   160] loss: 1.636\n",
            "[71,   170] loss: 1.628\n",
            "[71,   180] loss: 1.616\n",
            "[71,   190] loss: 1.622\n",
            "[71,   200] loss: 1.608\n",
            "[71,   210] loss: 1.628\n",
            "[71,   220] loss: 1.629\n",
            "[71,   230] loss: 1.633\n",
            "[71,   240] loss: 1.615\n",
            "[71,   250] loss: 1.624\n",
            "[71,   260] loss: 1.631\n",
            "[71,   270] loss: 1.632\n",
            "[71,   280] loss: 1.614\n",
            "[71,   290] loss: 1.620\n",
            "[71,   300] loss: 1.628\n",
            "[71,   310] loss: 1.615\n",
            "[71,   320] loss: 1.632\n",
            "[71,   330] loss: 1.625\n",
            "[71,   340] loss: 1.634\n",
            "[71,   350] loss: 1.633\n",
            "epoch 70 val_loss 66.32370400428772 val_steps 40 val_acc 0.8098\n",
            "[72,    10] loss: 1.629\n",
            "[72,    20] loss: 1.639\n",
            "[72,    30] loss: 1.623\n",
            "[72,    40] loss: 1.611\n",
            "[72,    50] loss: 1.619\n",
            "[72,    60] loss: 1.624\n",
            "[72,    70] loss: 1.627\n",
            "[72,    80] loss: 1.618\n",
            "[72,    90] loss: 1.601\n",
            "[72,   100] loss: 1.616\n",
            "[72,   110] loss: 1.619\n",
            "[72,   120] loss: 1.641\n",
            "[72,   130] loss: 1.621\n",
            "[72,   140] loss: 1.647\n",
            "[72,   150] loss: 1.635\n",
            "[72,   160] loss: 1.653\n",
            "[72,   170] loss: 1.632\n",
            "[72,   180] loss: 1.619\n",
            "[72,   190] loss: 1.620\n",
            "[72,   200] loss: 1.642\n",
            "[72,   210] loss: 1.627\n",
            "[72,   220] loss: 1.622\n",
            "[72,   230] loss: 1.628\n",
            "[72,   240] loss: 1.635\n",
            "[72,   250] loss: 1.645\n",
            "[72,   260] loss: 1.624\n",
            "[72,   270] loss: 1.635\n",
            "[72,   280] loss: 1.624\n",
            "[72,   290] loss: 1.629\n",
            "[72,   300] loss: 1.630\n",
            "[72,   310] loss: 1.627\n",
            "[72,   320] loss: 1.634\n",
            "[72,   330] loss: 1.622\n",
            "[72,   340] loss: 1.612\n",
            "[72,   350] loss: 1.634\n",
            "epoch 71 val_loss 66.16445171833038 val_steps 40 val_acc 0.8092\n",
            "[73,    10] loss: 1.620\n",
            "[73,    20] loss: 1.612\n",
            "[73,    30] loss: 1.609\n",
            "[73,    40] loss: 1.605\n",
            "[73,    50] loss: 1.624\n",
            "[73,    60] loss: 1.618\n",
            "[73,    70] loss: 1.621\n",
            "[73,    80] loss: 1.626\n",
            "[73,    90] loss: 1.622\n",
            "[73,   100] loss: 1.630\n",
            "[73,   110] loss: 1.623\n",
            "[73,   120] loss: 1.642\n",
            "[73,   130] loss: 1.598\n",
            "[73,   140] loss: 1.613\n",
            "[73,   150] loss: 1.630\n",
            "[73,   160] loss: 1.616\n",
            "[73,   170] loss: 1.635\n",
            "[73,   180] loss: 1.631\n",
            "[73,   190] loss: 1.630\n",
            "[73,   200] loss: 1.633\n",
            "[73,   210] loss: 1.621\n",
            "[73,   220] loss: 1.622\n",
            "[73,   230] loss: 1.610\n",
            "[73,   240] loss: 1.614\n",
            "[73,   250] loss: 1.600\n",
            "[73,   260] loss: 1.626\n",
            "[73,   270] loss: 1.623\n",
            "[73,   280] loss: 1.608\n",
            "[73,   290] loss: 1.629\n",
            "[73,   300] loss: 1.615\n",
            "[73,   310] loss: 1.622\n",
            "[73,   320] loss: 1.638\n",
            "[73,   330] loss: 1.632\n",
            "[73,   340] loss: 1.619\n",
            "[73,   350] loss: 1.608\n",
            "epoch 72 val_loss 65.91634023189545 val_steps 40 val_acc 0.8164\n",
            "[74,    10] loss: 1.624\n",
            "[74,    20] loss: 1.616\n",
            "[74,    30] loss: 1.649\n",
            "[74,    40] loss: 1.605\n",
            "[74,    50] loss: 1.622\n",
            "[74,    60] loss: 1.608\n",
            "[74,    70] loss: 1.592\n",
            "[74,    80] loss: 1.635\n",
            "[74,    90] loss: 1.623\n",
            "[74,   100] loss: 1.629\n",
            "[74,   110] loss: 1.618\n",
            "[74,   120] loss: 1.624\n",
            "[74,   130] loss: 1.619\n",
            "[74,   140] loss: 1.591\n",
            "[74,   150] loss: 1.631\n",
            "[74,   160] loss: 1.617\n",
            "[74,   170] loss: 1.617\n",
            "[74,   180] loss: 1.622\n",
            "[74,   190] loss: 1.620\n",
            "[74,   200] loss: 1.622\n",
            "[74,   210] loss: 1.642\n",
            "[74,   220] loss: 1.610\n",
            "[74,   230] loss: 1.611\n",
            "[74,   240] loss: 1.611\n",
            "[74,   250] loss: 1.605\n",
            "[74,   260] loss: 1.629\n",
            "[74,   270] loss: 1.625\n",
            "[74,   280] loss: 1.625\n",
            "[74,   290] loss: 1.624\n",
            "[74,   300] loss: 1.617\n",
            "[74,   310] loss: 1.622\n",
            "[74,   320] loss: 1.623\n",
            "[74,   330] loss: 1.630\n",
            "[74,   340] loss: 1.632\n",
            "[74,   350] loss: 1.630\n",
            "epoch 73 val_loss 65.70520329475403 val_steps 40 val_acc 0.822\n",
            "[75,    10] loss: 1.622\n",
            "[75,    20] loss: 1.628\n",
            "[75,    30] loss: 1.642\n",
            "[75,    40] loss: 1.624\n",
            "[75,    50] loss: 1.614\n",
            "[75,    60] loss: 1.619\n",
            "[75,    70] loss: 1.605\n",
            "[75,    80] loss: 1.615\n",
            "[75,    90] loss: 1.617\n",
            "[75,   100] loss: 1.639\n",
            "[75,   110] loss: 1.642\n",
            "[75,   120] loss: 1.610\n",
            "[75,   130] loss: 1.628\n",
            "[75,   140] loss: 1.613\n",
            "[75,   150] loss: 1.624\n",
            "[75,   160] loss: 1.631\n",
            "[75,   170] loss: 1.618\n",
            "[75,   180] loss: 1.625\n",
            "[75,   190] loss: 1.620\n",
            "[75,   200] loss: 1.617\n",
            "[75,   210] loss: 1.620\n",
            "[75,   220] loss: 1.614\n",
            "[75,   230] loss: 1.628\n",
            "[75,   240] loss: 1.608\n",
            "[75,   250] loss: 1.624\n",
            "[75,   260] loss: 1.613\n",
            "[75,   270] loss: 1.601\n",
            "[75,   280] loss: 1.611\n",
            "[75,   290] loss: 1.622\n",
            "[75,   300] loss: 1.601\n",
            "[75,   310] loss: 1.628\n",
            "[75,   320] loss: 1.617\n",
            "[75,   330] loss: 1.627\n",
            "[75,   340] loss: 1.630\n",
            "[75,   350] loss: 1.642\n",
            "epoch 74 val_loss 66.1756284236908 val_steps 40 val_acc 0.8094\n",
            "[76,    10] loss: 1.620\n",
            "[76,    20] loss: 1.614\n",
            "[76,    30] loss: 1.620\n",
            "[76,    40] loss: 1.610\n",
            "[76,    50] loss: 1.619\n",
            "[76,    60] loss: 1.621\n",
            "[76,    70] loss: 1.614\n",
            "[76,    80] loss: 1.625\n",
            "[76,    90] loss: 1.621\n",
            "[76,   100] loss: 1.624\n",
            "[76,   110] loss: 1.620\n",
            "[76,   120] loss: 1.622\n",
            "[76,   130] loss: 1.632\n",
            "[76,   140] loss: 1.629\n",
            "[76,   150] loss: 1.609\n",
            "[76,   160] loss: 1.634\n",
            "[76,   170] loss: 1.622\n",
            "[76,   180] loss: 1.628\n",
            "[76,   190] loss: 1.626\n",
            "[76,   200] loss: 1.627\n",
            "[76,   210] loss: 1.622\n",
            "[76,   220] loss: 1.620\n",
            "[76,   230] loss: 1.610\n",
            "[76,   240] loss: 1.636\n",
            "[76,   250] loss: 1.626\n",
            "[76,   260] loss: 1.603\n",
            "[76,   270] loss: 1.615\n",
            "[76,   280] loss: 1.616\n",
            "[76,   290] loss: 1.631\n",
            "[76,   300] loss: 1.604\n",
            "[76,   310] loss: 1.622\n",
            "[76,   320] loss: 1.643\n",
            "[76,   330] loss: 1.628\n",
            "[76,   340] loss: 1.631\n",
            "[76,   350] loss: 1.642\n",
            "epoch 75 val_loss 66.2785918712616 val_steps 40 val_acc 0.8074\n",
            "[77,    10] loss: 1.602\n",
            "[77,    20] loss: 1.602\n",
            "[77,    30] loss: 1.634\n",
            "[77,    40] loss: 1.618\n",
            "[77,    50] loss: 1.632\n",
            "[77,    60] loss: 1.627\n",
            "[77,    70] loss: 1.611\n",
            "[77,    80] loss: 1.638\n",
            "[77,    90] loss: 1.614\n",
            "[77,   100] loss: 1.631\n",
            "[77,   110] loss: 1.612\n",
            "[77,   120] loss: 1.632\n",
            "[77,   130] loss: 1.614\n",
            "[77,   140] loss: 1.615\n",
            "[77,   150] loss: 1.620\n",
            "[77,   160] loss: 1.628\n",
            "[77,   170] loss: 1.606\n",
            "[77,   180] loss: 1.628\n",
            "[77,   190] loss: 1.614\n",
            "[77,   200] loss: 1.613\n",
            "[77,   210] loss: 1.617\n",
            "[77,   220] loss: 1.642\n",
            "[77,   230] loss: 1.626\n",
            "[77,   240] loss: 1.610\n",
            "[77,   250] loss: 1.629\n",
            "[77,   260] loss: 1.592\n",
            "[77,   270] loss: 1.603\n",
            "[77,   280] loss: 1.613\n",
            "[77,   290] loss: 1.609\n",
            "[77,   300] loss: 1.634\n",
            "[77,   310] loss: 1.617\n",
            "[77,   320] loss: 1.626\n",
            "[77,   330] loss: 1.623\n",
            "[77,   340] loss: 1.622\n",
            "[77,   350] loss: 1.616\n",
            "epoch 76 val_loss 66.87088310718536 val_steps 40 val_acc 0.7912\n",
            "[78,    10] loss: 1.616\n",
            "[78,    20] loss: 1.615\n",
            "[78,    30] loss: 1.627\n",
            "[78,    40] loss: 1.595\n",
            "[78,    50] loss: 1.614\n",
            "[78,    60] loss: 1.615\n",
            "[78,    70] loss: 1.625\n",
            "[78,    80] loss: 1.614\n",
            "[78,    90] loss: 1.611\n",
            "[78,   100] loss: 1.626\n",
            "[78,   110] loss: 1.617\n",
            "[78,   120] loss: 1.617\n",
            "[78,   130] loss: 1.615\n",
            "[78,   140] loss: 1.620\n",
            "[78,   150] loss: 1.617\n",
            "[78,   160] loss: 1.598\n",
            "[78,   170] loss: 1.602\n",
            "[78,   180] loss: 1.607\n",
            "[78,   190] loss: 1.616\n",
            "[78,   200] loss: 1.627\n",
            "[78,   210] loss: 1.625\n",
            "[78,   220] loss: 1.629\n",
            "[78,   230] loss: 1.623\n",
            "[78,   240] loss: 1.630\n",
            "[78,   250] loss: 1.626\n",
            "[78,   260] loss: 1.623\n",
            "[78,   270] loss: 1.628\n",
            "[78,   280] loss: 1.611\n",
            "[78,   290] loss: 1.625\n",
            "[78,   300] loss: 1.623\n",
            "[78,   310] loss: 1.619\n",
            "[78,   320] loss: 1.629\n",
            "[78,   330] loss: 1.628\n",
            "[78,   340] loss: 1.619\n",
            "[78,   350] loss: 1.615\n",
            "epoch 77 val_loss 66.48584151268005 val_steps 40 val_acc 0.8008\n",
            "[79,    10] loss: 1.618\n",
            "[79,    20] loss: 1.621\n",
            "[79,    30] loss: 1.611\n",
            "[79,    40] loss: 1.623\n",
            "[79,    50] loss: 1.639\n",
            "[79,    60] loss: 1.638\n",
            "[79,    70] loss: 1.630\n",
            "[79,    80] loss: 1.604\n",
            "[79,    90] loss: 1.604\n",
            "[79,   100] loss: 1.630\n",
            "[79,   110] loss: 1.611\n",
            "[79,   120] loss: 1.626\n",
            "[79,   130] loss: 1.618\n",
            "[79,   140] loss: 1.619\n",
            "[79,   150] loss: 1.615\n",
            "[79,   160] loss: 1.622\n",
            "[79,   170] loss: 1.599\n",
            "[79,   180] loss: 1.608\n",
            "[79,   190] loss: 1.615\n",
            "[79,   200] loss: 1.599\n",
            "[79,   210] loss: 1.642\n",
            "[79,   220] loss: 1.628\n",
            "[79,   230] loss: 1.632\n",
            "[79,   240] loss: 1.625\n",
            "[79,   250] loss: 1.614\n",
            "[79,   260] loss: 1.601\n",
            "[79,   270] loss: 1.609\n",
            "[79,   280] loss: 1.604\n",
            "[79,   290] loss: 1.608\n",
            "[79,   300] loss: 1.617\n",
            "[79,   310] loss: 1.614\n",
            "[79,   320] loss: 1.611\n",
            "[79,   330] loss: 1.624\n",
            "[79,   340] loss: 1.623\n",
            "[79,   350] loss: 1.625\n",
            "epoch 78 val_loss 66.23680233955383 val_steps 40 val_acc 0.8092\n",
            "[80,    10] loss: 1.629\n",
            "[80,    20] loss: 1.611\n",
            "[80,    30] loss: 1.630\n",
            "[80,    40] loss: 1.624\n",
            "[80,    50] loss: 1.613\n",
            "[80,    60] loss: 1.627\n",
            "[80,    70] loss: 1.603\n",
            "[80,    80] loss: 1.613\n",
            "[80,    90] loss: 1.607\n",
            "[80,   100] loss: 1.622\n",
            "[80,   110] loss: 1.607\n",
            "[80,   120] loss: 1.618\n",
            "[80,   130] loss: 1.625\n",
            "[80,   140] loss: 1.616\n",
            "[80,   150] loss: 1.614\n",
            "[80,   160] loss: 1.621\n",
            "[80,   170] loss: 1.635\n",
            "[80,   180] loss: 1.641\n",
            "[80,   190] loss: 1.625\n",
            "[80,   200] loss: 1.621\n",
            "[80,   210] loss: 1.611\n",
            "[80,   220] loss: 1.630\n",
            "[80,   230] loss: 1.615\n",
            "[80,   240] loss: 1.616\n",
            "[80,   250] loss: 1.617\n",
            "[80,   260] loss: 1.622\n",
            "[80,   270] loss: 1.624\n",
            "[80,   280] loss: 1.617\n",
            "[80,   290] loss: 1.607\n",
            "[80,   300] loss: 1.632\n",
            "[80,   310] loss: 1.624\n",
            "[80,   320] loss: 1.635\n",
            "[80,   330] loss: 1.617\n",
            "[80,   340] loss: 1.638\n",
            "[80,   350] loss: 1.625\n",
            "epoch 79 val_loss 65.14767801761627 val_steps 40 val_acc 0.8344\n",
            "[81,    10] loss: 1.605\n",
            "[81,    20] loss: 1.624\n",
            "[81,    30] loss: 1.634\n",
            "[81,    40] loss: 1.616\n",
            "[81,    50] loss: 1.622\n",
            "[81,    60] loss: 1.621\n",
            "[81,    70] loss: 1.637\n",
            "[81,    80] loss: 1.625\n",
            "[81,    90] loss: 1.633\n",
            "[81,   100] loss: 1.626\n",
            "[81,   110] loss: 1.633\n",
            "[81,   120] loss: 1.631\n",
            "[81,   130] loss: 1.624\n",
            "[81,   140] loss: 1.637\n",
            "[81,   150] loss: 1.607\n",
            "[81,   160] loss: 1.615\n",
            "[81,   170] loss: 1.619\n",
            "[81,   180] loss: 1.634\n",
            "[81,   190] loss: 1.626\n",
            "[81,   200] loss: 1.605\n",
            "[81,   210] loss: 1.616\n",
            "[81,   220] loss: 1.639\n",
            "[81,   230] loss: 1.617\n",
            "[81,   240] loss: 1.610\n",
            "[81,   250] loss: 1.633\n",
            "[81,   260] loss: 1.626\n",
            "[81,   270] loss: 1.610\n",
            "[81,   280] loss: 1.610\n",
            "[81,   290] loss: 1.626\n",
            "[81,   300] loss: 1.619\n",
            "[81,   310] loss: 1.626\n",
            "[81,   320] loss: 1.612\n",
            "[81,   330] loss: 1.623\n",
            "[81,   340] loss: 1.612\n",
            "[81,   350] loss: 1.646\n",
            "epoch 80 val_loss 65.73304057121277 val_steps 40 val_acc 0.8178\n",
            "[82,    10] loss: 1.591\n",
            "[82,    20] loss: 1.612\n",
            "[82,    30] loss: 1.629\n",
            "[82,    40] loss: 1.618\n",
            "[82,    50] loss: 1.620\n",
            "[82,    60] loss: 1.608\n",
            "[82,    70] loss: 1.630\n",
            "[82,    80] loss: 1.615\n",
            "[82,    90] loss: 1.620\n",
            "[82,   100] loss: 1.619\n",
            "[82,   110] loss: 1.618\n",
            "[82,   120] loss: 1.620\n",
            "[82,   130] loss: 1.618\n",
            "[82,   140] loss: 1.644\n",
            "[82,   150] loss: 1.605\n",
            "[82,   160] loss: 1.619\n",
            "[82,   170] loss: 1.630\n",
            "[82,   180] loss: 1.625\n",
            "[82,   190] loss: 1.632\n",
            "[82,   200] loss: 1.623\n",
            "[82,   210] loss: 1.604\n",
            "[82,   220] loss: 1.598\n",
            "[82,   230] loss: 1.613\n",
            "[82,   240] loss: 1.614\n",
            "[82,   250] loss: 1.604\n",
            "[82,   260] loss: 1.605\n",
            "[82,   270] loss: 1.624\n",
            "[82,   280] loss: 1.619\n",
            "[82,   290] loss: 1.599\n",
            "[82,   300] loss: 1.611\n",
            "[82,   310] loss: 1.621\n",
            "[82,   320] loss: 1.616\n",
            "[82,   330] loss: 1.605\n",
            "[82,   340] loss: 1.598\n",
            "[82,   350] loss: 1.627\n",
            "epoch 81 val_loss 66.29865086078644 val_steps 40 val_acc 0.804\n",
            "[83,    10] loss: 1.611\n",
            "[83,    20] loss: 1.598\n",
            "[83,    30] loss: 1.605\n",
            "[83,    40] loss: 1.629\n",
            "[83,    50] loss: 1.623\n",
            "[83,    60] loss: 1.619\n",
            "[83,    70] loss: 1.609\n",
            "[83,    80] loss: 1.630\n",
            "[83,    90] loss: 1.609\n",
            "[83,   100] loss: 1.602\n",
            "[83,   110] loss: 1.635\n",
            "[83,   120] loss: 1.629\n",
            "[83,   130] loss: 1.620\n",
            "[83,   140] loss: 1.618\n",
            "[83,   150] loss: 1.610\n",
            "[83,   160] loss: 1.604\n",
            "[83,   170] loss: 1.608\n",
            "[83,   180] loss: 1.614\n",
            "[83,   190] loss: 1.627\n",
            "[83,   200] loss: 1.628\n",
            "[83,   210] loss: 1.613\n",
            "[83,   220] loss: 1.634\n",
            "[83,   230] loss: 1.620\n",
            "[83,   240] loss: 1.625\n",
            "[83,   250] loss: 1.618\n",
            "[83,   260] loss: 1.610\n",
            "[83,   270] loss: 1.615\n",
            "[83,   280] loss: 1.628\n",
            "[83,   290] loss: 1.623\n",
            "[83,   300] loss: 1.610\n",
            "[83,   310] loss: 1.610\n",
            "[83,   320] loss: 1.628\n",
            "[83,   330] loss: 1.626\n",
            "[83,   340] loss: 1.619\n",
            "[83,   350] loss: 1.628\n",
            "epoch 82 val_loss 66.22123062610626 val_steps 40 val_acc 0.8068\n",
            "[84,    10] loss: 1.618\n",
            "[84,    20] loss: 1.601\n",
            "[84,    30] loss: 1.617\n",
            "[84,    40] loss: 1.612\n",
            "[84,    50] loss: 1.605\n",
            "[84,    60] loss: 1.621\n",
            "[84,    70] loss: 1.618\n",
            "[84,    80] loss: 1.613\n",
            "[84,    90] loss: 1.618\n",
            "[84,   100] loss: 1.607\n",
            "[84,   110] loss: 1.610\n",
            "[84,   120] loss: 1.609\n",
            "[84,   130] loss: 1.604\n",
            "[84,   140] loss: 1.612\n",
            "[84,   150] loss: 1.614\n",
            "[84,   160] loss: 1.612\n",
            "[84,   170] loss: 1.606\n",
            "[84,   180] loss: 1.613\n",
            "[84,   190] loss: 1.600\n",
            "[84,   200] loss: 1.612\n",
            "[84,   210] loss: 1.624\n",
            "[84,   220] loss: 1.616\n",
            "[84,   230] loss: 1.614\n",
            "[84,   240] loss: 1.611\n",
            "[84,   250] loss: 1.621\n",
            "[84,   260] loss: 1.620\n",
            "[84,   270] loss: 1.628\n",
            "[84,   280] loss: 1.612\n",
            "[84,   290] loss: 1.617\n",
            "[84,   300] loss: 1.620\n",
            "[84,   310] loss: 1.627\n",
            "[84,   320] loss: 1.630\n",
            "[84,   330] loss: 1.626\n",
            "[84,   340] loss: 1.627\n",
            "[84,   350] loss: 1.621\n",
            "epoch 83 val_loss 66.13055741786957 val_steps 40 val_acc 0.8142\n",
            "[85,    10] loss: 1.611\n",
            "[85,    20] loss: 1.612\n",
            "[85,    30] loss: 1.618\n",
            "[85,    40] loss: 1.625\n",
            "[85,    50] loss: 1.609\n",
            "[85,    60] loss: 1.639\n",
            "[85,    70] loss: 1.633\n",
            "[85,    80] loss: 1.617\n",
            "[85,    90] loss: 1.623\n",
            "[85,   100] loss: 1.610\n",
            "[85,   110] loss: 1.599\n",
            "[85,   120] loss: 1.608\n",
            "[85,   130] loss: 1.597\n",
            "[85,   140] loss: 1.616\n",
            "[85,   150] loss: 1.596\n",
            "[85,   160] loss: 1.616\n",
            "[85,   170] loss: 1.625\n",
            "[85,   180] loss: 1.622\n",
            "[85,   190] loss: 1.628\n",
            "[85,   200] loss: 1.637\n",
            "[85,   210] loss: 1.631\n",
            "[85,   220] loss: 1.596\n",
            "[85,   230] loss: 1.605\n",
            "[85,   240] loss: 1.632\n",
            "[85,   250] loss: 1.634\n",
            "[85,   260] loss: 1.634\n",
            "[85,   270] loss: 1.621\n",
            "[85,   280] loss: 1.616\n",
            "[85,   290] loss: 1.614\n",
            "[85,   300] loss: 1.619\n",
            "[85,   310] loss: 1.623\n",
            "[85,   320] loss: 1.602\n",
            "[85,   330] loss: 1.616\n",
            "[85,   340] loss: 1.625\n",
            "[85,   350] loss: 1.608\n",
            "epoch 84 val_loss 66.18848788738251 val_steps 40 val_acc 0.8152\n",
            "[86,    10] loss: 1.612\n",
            "[86,    20] loss: 1.623\n",
            "[86,    30] loss: 1.606\n",
            "[86,    40] loss: 1.605\n",
            "[86,    50] loss: 1.611\n",
            "[86,    60] loss: 1.620\n",
            "[86,    70] loss: 1.619\n",
            "[86,    80] loss: 1.628\n",
            "[86,    90] loss: 1.606\n",
            "[86,   100] loss: 1.623\n",
            "[86,   110] loss: 1.605\n",
            "[86,   120] loss: 1.618\n",
            "[86,   130] loss: 1.620\n",
            "[86,   140] loss: 1.615\n",
            "[86,   150] loss: 1.620\n",
            "[86,   160] loss: 1.623\n",
            "[86,   170] loss: 1.599\n",
            "[86,   180] loss: 1.616\n",
            "[86,   190] loss: 1.602\n",
            "[86,   200] loss: 1.618\n",
            "[86,   210] loss: 1.617\n",
            "[86,   220] loss: 1.617\n",
            "[86,   230] loss: 1.609\n",
            "[86,   240] loss: 1.613\n",
            "[86,   250] loss: 1.602\n",
            "[86,   260] loss: 1.610\n",
            "[86,   270] loss: 1.628\n",
            "[86,   280] loss: 1.611\n",
            "[86,   290] loss: 1.594\n",
            "[86,   300] loss: 1.617\n",
            "[86,   310] loss: 1.606\n",
            "[86,   320] loss: 1.607\n",
            "[86,   330] loss: 1.618\n",
            "[86,   340] loss: 1.635\n",
            "[86,   350] loss: 1.623\n",
            "epoch 85 val_loss 66.68767154216766 val_steps 40 val_acc 0.7986\n",
            "[87,    10] loss: 1.633\n",
            "[87,    20] loss: 1.614\n",
            "[87,    30] loss: 1.600\n",
            "[87,    40] loss: 1.613\n",
            "[87,    50] loss: 1.600\n",
            "[87,    60] loss: 1.629\n",
            "[87,    70] loss: 1.618\n",
            "[87,    80] loss: 1.608\n",
            "[87,    90] loss: 1.621\n",
            "[87,   100] loss: 1.606\n",
            "[87,   110] loss: 1.614\n",
            "[87,   120] loss: 1.624\n",
            "[87,   130] loss: 1.605\n",
            "[87,   140] loss: 1.610\n",
            "[87,   150] loss: 1.621\n",
            "[87,   160] loss: 1.621\n",
            "[87,   170] loss: 1.604\n",
            "[87,   180] loss: 1.611\n",
            "[87,   190] loss: 1.615\n",
            "[87,   200] loss: 1.618\n",
            "[87,   210] loss: 1.616\n",
            "[87,   220] loss: 1.617\n",
            "[87,   230] loss: 1.605\n",
            "[87,   240] loss: 1.609\n",
            "[87,   250] loss: 1.619\n",
            "[87,   260] loss: 1.631\n",
            "[87,   270] loss: 1.616\n",
            "[87,   280] loss: 1.617\n",
            "[87,   290] loss: 1.603\n",
            "[87,   300] loss: 1.614\n",
            "[87,   310] loss: 1.601\n",
            "[87,   320] loss: 1.623\n",
            "[87,   330] loss: 1.615\n",
            "[87,   340] loss: 1.608\n",
            "[87,   350] loss: 1.602\n",
            "epoch 86 val_loss 66.35574126243591 val_steps 40 val_acc 0.8078\n",
            "[88,    10] loss: 1.604\n",
            "[88,    20] loss: 1.604\n",
            "[88,    30] loss: 1.608\n",
            "[88,    40] loss: 1.606\n",
            "[88,    50] loss: 1.608\n",
            "[88,    60] loss: 1.598\n",
            "[88,    70] loss: 1.622\n",
            "[88,    80] loss: 1.587\n",
            "[88,    90] loss: 1.610\n",
            "[88,   100] loss: 1.591\n",
            "[88,   110] loss: 1.603\n",
            "[88,   120] loss: 1.608\n",
            "[88,   130] loss: 1.613\n",
            "[88,   140] loss: 1.606\n",
            "[88,   150] loss: 1.611\n",
            "[88,   160] loss: 1.619\n",
            "[88,   170] loss: 1.611\n",
            "[88,   180] loss: 1.618\n",
            "[88,   190] loss: 1.611\n",
            "[88,   200] loss: 1.614\n",
            "[88,   210] loss: 1.610\n",
            "[88,   220] loss: 1.623\n",
            "[88,   230] loss: 1.617\n",
            "[88,   240] loss: 1.634\n",
            "[88,   250] loss: 1.608\n",
            "[88,   260] loss: 1.619\n",
            "[88,   270] loss: 1.626\n",
            "[88,   280] loss: 1.615\n",
            "[88,   290] loss: 1.622\n",
            "[88,   300] loss: 1.607\n",
            "[88,   310] loss: 1.601\n",
            "[88,   320] loss: 1.618\n",
            "[88,   330] loss: 1.617\n",
            "[88,   340] loss: 1.613\n",
            "[88,   350] loss: 1.612\n",
            "epoch 87 val_loss 66.24849081039429 val_steps 40 val_acc 0.8128\n",
            "[89,    10] loss: 1.597\n",
            "[89,    20] loss: 1.613\n",
            "[89,    30] loss: 1.602\n",
            "[89,    40] loss: 1.612\n",
            "[89,    50] loss: 1.613\n",
            "[89,    60] loss: 1.627\n",
            "[89,    70] loss: 1.615\n",
            "[89,    80] loss: 1.600\n",
            "[89,    90] loss: 1.623\n",
            "[89,   100] loss: 1.639\n",
            "[89,   110] loss: 1.619\n",
            "[89,   120] loss: 1.618\n",
            "[89,   130] loss: 1.608\n",
            "[89,   140] loss: 1.620\n",
            "[89,   150] loss: 1.609\n",
            "[89,   160] loss: 1.604\n",
            "[89,   170] loss: 1.617\n",
            "[89,   180] loss: 1.616\n",
            "[89,   190] loss: 1.599\n",
            "[89,   200] loss: 1.600\n",
            "[89,   210] loss: 1.583\n",
            "[89,   220] loss: 1.615\n",
            "[89,   230] loss: 1.605\n",
            "[89,   240] loss: 1.629\n",
            "[89,   250] loss: 1.608\n",
            "[89,   260] loss: 1.619\n",
            "[89,   270] loss: 1.603\n",
            "[89,   280] loss: 1.611\n",
            "[89,   290] loss: 1.624\n",
            "[89,   300] loss: 1.628\n",
            "[89,   310] loss: 1.609\n",
            "[89,   320] loss: 1.603\n",
            "[89,   330] loss: 1.618\n",
            "[89,   340] loss: 1.611\n",
            "[89,   350] loss: 1.621\n",
            "epoch 88 val_loss 66.26684403419495 val_steps 40 val_acc 0.8126\n",
            "[90,    10] loss: 1.609\n",
            "[90,    20] loss: 1.603\n",
            "[90,    30] loss: 1.606\n",
            "[90,    40] loss: 1.594\n",
            "[90,    50] loss: 1.611\n",
            "[90,    60] loss: 1.617\n",
            "[90,    70] loss: 1.599\n",
            "[90,    80] loss: 1.621\n",
            "[90,    90] loss: 1.615\n",
            "[90,   100] loss: 1.626\n",
            "[90,   110] loss: 1.605\n",
            "[90,   120] loss: 1.595\n",
            "[90,   130] loss: 1.615\n",
            "[90,   140] loss: 1.616\n",
            "[90,   150] loss: 1.619\n",
            "[90,   160] loss: 1.620\n",
            "[90,   170] loss: 1.612\n",
            "[90,   180] loss: 1.611\n",
            "[90,   190] loss: 1.598\n",
            "[90,   200] loss: 1.605\n",
            "[90,   210] loss: 1.619\n",
            "[90,   220] loss: 1.633\n",
            "[90,   230] loss: 1.627\n",
            "[90,   240] loss: 1.648\n",
            "[90,   250] loss: 1.628\n",
            "[90,   260] loss: 1.644\n",
            "[90,   270] loss: 1.632\n",
            "[90,   280] loss: 1.613\n",
            "[90,   290] loss: 1.625\n",
            "[90,   300] loss: 1.625\n",
            "[90,   310] loss: 1.619\n",
            "[90,   320] loss: 1.629\n",
            "[90,   330] loss: 1.607\n",
            "[90,   340] loss: 1.603\n",
            "[90,   350] loss: 1.637\n",
            "epoch 89 val_loss 66.98892664909363 val_steps 40 val_acc 0.7914\n",
            "[91,    10] loss: 1.624\n",
            "[91,    20] loss: 1.626\n",
            "[91,    30] loss: 1.608\n",
            "[91,    40] loss: 1.628\n",
            "[91,    50] loss: 1.631\n",
            "[91,    60] loss: 1.624\n",
            "[91,    70] loss: 1.595\n",
            "[91,    80] loss: 1.595\n",
            "[91,    90] loss: 1.611\n",
            "[91,   100] loss: 1.609\n",
            "[91,   110] loss: 1.618\n",
            "[91,   120] loss: 1.609\n",
            "[91,   130] loss: 1.607\n",
            "[91,   140] loss: 1.606\n",
            "[91,   150] loss: 1.614\n",
            "[91,   160] loss: 1.624\n",
            "[91,   170] loss: 1.619\n",
            "[91,   180] loss: 1.638\n",
            "[91,   190] loss: 1.611\n",
            "[91,   200] loss: 1.625\n",
            "[91,   210] loss: 1.616\n",
            "[91,   220] loss: 1.621\n",
            "[91,   230] loss: 1.624\n",
            "[91,   240] loss: 1.622\n",
            "[91,   250] loss: 1.639\n",
            "[91,   260] loss: 1.610\n",
            "[91,   270] loss: 1.611\n",
            "[91,   280] loss: 1.623\n",
            "[91,   290] loss: 1.612\n",
            "[91,   300] loss: 1.626\n",
            "[91,   310] loss: 1.611\n",
            "[91,   320] loss: 1.627\n",
            "[91,   330] loss: 1.629\n",
            "[91,   340] loss: 1.616\n",
            "[91,   350] loss: 1.605\n",
            "epoch 90 val_loss 65.81409013271332 val_steps 40 val_acc 0.8242\n",
            "[92,    10] loss: 1.613\n",
            "[92,    20] loss: 1.589\n",
            "[92,    30] loss: 1.578\n",
            "[92,    40] loss: 1.599\n",
            "[92,    50] loss: 1.579\n",
            "[92,    60] loss: 1.575\n",
            "[92,    70] loss: 1.586\n",
            "[92,    80] loss: 1.597\n",
            "[92,    90] loss: 1.588\n",
            "[92,   100] loss: 1.560\n",
            "[92,   110] loss: 1.585\n",
            "[92,   120] loss: 1.584\n",
            "[92,   130] loss: 1.569\n",
            "[92,   140] loss: 1.570\n",
            "[92,   150] loss: 1.588\n",
            "[92,   160] loss: 1.573\n",
            "[92,   170] loss: 1.579\n",
            "[92,   180] loss: 1.570\n",
            "[92,   190] loss: 1.580\n",
            "[92,   200] loss: 1.582\n",
            "[92,   210] loss: 1.577\n",
            "[92,   220] loss: 1.580\n",
            "[92,   230] loss: 1.563\n",
            "[92,   240] loss: 1.581\n",
            "[92,   250] loss: 1.572\n",
            "[92,   260] loss: 1.575\n",
            "[92,   270] loss: 1.565\n",
            "[92,   280] loss: 1.566\n",
            "[92,   290] loss: 1.574\n",
            "[92,   300] loss: 1.568\n",
            "[92,   310] loss: 1.592\n",
            "[92,   320] loss: 1.574\n",
            "[92,   330] loss: 1.583\n",
            "[92,   340] loss: 1.573\n",
            "[92,   350] loss: 1.567\n",
            "epoch 91 val_loss 64.94275963306427 val_steps 40 val_acc 0.8428\n",
            "[93,    10] loss: 1.571\n",
            "[93,    20] loss: 1.570\n",
            "[93,    30] loss: 1.556\n",
            "[93,    40] loss: 1.583\n",
            "[93,    50] loss: 1.560\n",
            "[93,    60] loss: 1.572\n",
            "[93,    70] loss: 1.574\n",
            "[93,    80] loss: 1.559\n",
            "[93,    90] loss: 1.574\n",
            "[93,   100] loss: 1.579\n",
            "[93,   110] loss: 1.589\n",
            "[93,   120] loss: 1.579\n",
            "[93,   130] loss: 1.587\n",
            "[93,   140] loss: 1.569\n",
            "[93,   150] loss: 1.553\n",
            "[93,   160] loss: 1.559\n",
            "[93,   170] loss: 1.567\n",
            "[93,   180] loss: 1.566\n",
            "[93,   190] loss: 1.563\n",
            "[93,   200] loss: 1.572\n",
            "[93,   210] loss: 1.566\n",
            "[93,   220] loss: 1.567\n",
            "[93,   230] loss: 1.561\n",
            "[93,   240] loss: 1.570\n",
            "[93,   250] loss: 1.566\n",
            "[93,   260] loss: 1.557\n",
            "[93,   270] loss: 1.568\n",
            "[93,   280] loss: 1.563\n",
            "[93,   290] loss: 1.567\n",
            "[93,   300] loss: 1.549\n",
            "[93,   310] loss: 1.571\n",
            "[93,   320] loss: 1.571\n",
            "[93,   330] loss: 1.585\n",
            "[93,   340] loss: 1.553\n",
            "[93,   350] loss: 1.569\n",
            "epoch 92 val_loss 64.74834609031677 val_steps 40 val_acc 0.8508\n",
            "[94,    10] loss: 1.559\n",
            "[94,    20] loss: 1.563\n",
            "[94,    30] loss: 1.566\n",
            "[94,    40] loss: 1.560\n",
            "[94,    50] loss: 1.553\n",
            "[94,    60] loss: 1.561\n",
            "[94,    70] loss: 1.558\n",
            "[94,    80] loss: 1.566\n",
            "[94,    90] loss: 1.564\n",
            "[94,   100] loss: 1.563\n",
            "[94,   110] loss: 1.569\n",
            "[94,   120] loss: 1.564\n",
            "[94,   130] loss: 1.569\n",
            "[94,   140] loss: 1.571\n",
            "[94,   150] loss: 1.570\n",
            "[94,   160] loss: 1.568\n",
            "[94,   170] loss: 1.562\n",
            "[94,   180] loss: 1.556\n",
            "[94,   190] loss: 1.579\n",
            "[94,   200] loss: 1.570\n",
            "[94,   210] loss: 1.554\n",
            "[94,   220] loss: 1.585\n",
            "[94,   230] loss: 1.567\n",
            "[94,   240] loss: 1.560\n",
            "[94,   250] loss: 1.564\n",
            "[94,   260] loss: 1.561\n",
            "[94,   270] loss: 1.569\n",
            "[94,   280] loss: 1.562\n",
            "[94,   290] loss: 1.567\n",
            "[94,   300] loss: 1.554\n",
            "[94,   310] loss: 1.561\n",
            "[94,   320] loss: 1.553\n",
            "[94,   330] loss: 1.551\n",
            "[94,   340] loss: 1.554\n",
            "[94,   350] loss: 1.553\n",
            "epoch 93 val_loss 64.4807779788971 val_steps 40 val_acc 0.8478\n",
            "[95,    10] loss: 1.551\n",
            "[95,    20] loss: 1.582\n",
            "[95,    30] loss: 1.552\n",
            "[95,    40] loss: 1.559\n",
            "[95,    50] loss: 1.571\n",
            "[95,    60] loss: 1.561\n",
            "[95,    70] loss: 1.559\n",
            "[95,    80] loss: 1.564\n",
            "[95,    90] loss: 1.556\n",
            "[95,   100] loss: 1.560\n",
            "[95,   110] loss: 1.557\n",
            "[95,   120] loss: 1.556\n",
            "[95,   130] loss: 1.567\n",
            "[95,   140] loss: 1.561\n",
            "[95,   150] loss: 1.564\n",
            "[95,   160] loss: 1.565\n",
            "[95,   170] loss: 1.562\n",
            "[95,   180] loss: 1.538\n",
            "[95,   190] loss: 1.563\n",
            "[95,   200] loss: 1.581\n",
            "[95,   210] loss: 1.557\n",
            "[95,   220] loss: 1.567\n",
            "[95,   230] loss: 1.559\n",
            "[95,   240] loss: 1.564\n",
            "[95,   250] loss: 1.554\n",
            "[95,   260] loss: 1.560\n",
            "[95,   270] loss: 1.553\n",
            "[95,   280] loss: 1.562\n",
            "[95,   290] loss: 1.544\n",
            "[95,   300] loss: 1.551\n",
            "[95,   310] loss: 1.552\n",
            "[95,   320] loss: 1.572\n",
            "[95,   330] loss: 1.551\n",
            "[95,   340] loss: 1.568\n",
            "[95,   350] loss: 1.569\n",
            "epoch 94 val_loss 64.52994811534882 val_steps 40 val_acc 0.8522\n",
            "[96,    10] loss: 1.558\n",
            "[96,    20] loss: 1.557\n",
            "[96,    30] loss: 1.549\n",
            "[96,    40] loss: 1.564\n",
            "[96,    50] loss: 1.554\n",
            "[96,    60] loss: 1.554\n",
            "[96,    70] loss: 1.560\n",
            "[96,    80] loss: 1.563\n",
            "[96,    90] loss: 1.579\n",
            "[96,   100] loss: 1.550\n",
            "[96,   110] loss: 1.548\n",
            "[96,   120] loss: 1.565\n",
            "[96,   130] loss: 1.556\n",
            "[96,   140] loss: 1.561\n",
            "[96,   150] loss: 1.554\n",
            "[96,   160] loss: 1.575\n",
            "[96,   170] loss: 1.551\n",
            "[96,   180] loss: 1.565\n",
            "[96,   190] loss: 1.567\n",
            "[96,   200] loss: 1.555\n",
            "[96,   210] loss: 1.551\n",
            "[96,   220] loss: 1.563\n",
            "[96,   230] loss: 1.553\n",
            "[96,   240] loss: 1.549\n",
            "[96,   250] loss: 1.546\n",
            "[96,   260] loss: 1.558\n",
            "[96,   270] loss: 1.549\n",
            "[96,   280] loss: 1.566\n",
            "[96,   290] loss: 1.559\n",
            "[96,   300] loss: 1.544\n",
            "[96,   310] loss: 1.567\n",
            "[96,   320] loss: 1.564\n",
            "[96,   330] loss: 1.562\n",
            "[96,   340] loss: 1.556\n",
            "[96,   350] loss: 1.558\n",
            "epoch 95 val_loss 64.58794379234314 val_steps 40 val_acc 0.8458\n",
            "[97,    10] loss: 1.548\n",
            "[97,    20] loss: 1.555\n",
            "[97,    30] loss: 1.546\n",
            "[97,    40] loss: 1.571\n",
            "[97,    50] loss: 1.556\n",
            "[97,    60] loss: 1.559\n",
            "[97,    70] loss: 1.560\n",
            "[97,    80] loss: 1.560\n",
            "[97,    90] loss: 1.560\n",
            "[97,   100] loss: 1.561\n",
            "[97,   110] loss: 1.558\n",
            "[97,   120] loss: 1.552\n",
            "[97,   130] loss: 1.566\n",
            "[97,   140] loss: 1.559\n",
            "[97,   150] loss: 1.567\n",
            "[97,   160] loss: 1.572\n",
            "[97,   170] loss: 1.558\n",
            "[97,   180] loss: 1.553\n",
            "[97,   190] loss: 1.554\n",
            "[97,   200] loss: 1.579\n",
            "[97,   210] loss: 1.553\n",
            "[97,   220] loss: 1.551\n",
            "[97,   230] loss: 1.554\n",
            "[97,   240] loss: 1.551\n",
            "[97,   250] loss: 1.542\n",
            "[97,   260] loss: 1.553\n",
            "[97,   270] loss: 1.557\n",
            "[97,   280] loss: 1.565\n",
            "[97,   290] loss: 1.558\n",
            "[97,   300] loss: 1.554\n",
            "[97,   310] loss: 1.561\n",
            "[97,   320] loss: 1.558\n",
            "[97,   330] loss: 1.549\n",
            "[97,   340] loss: 1.566\n",
            "[97,   350] loss: 1.544\n",
            "epoch 96 val_loss 64.04882442951202 val_steps 40 val_acc 0.862\n",
            "[98,    10] loss: 1.540\n",
            "[98,    20] loss: 1.552\n",
            "[98,    30] loss: 1.548\n",
            "[98,    40] loss: 1.560\n",
            "[98,    50] loss: 1.557\n",
            "[98,    60] loss: 1.554\n",
            "[98,    70] loss: 1.555\n",
            "[98,    80] loss: 1.556\n",
            "[98,    90] loss: 1.561\n",
            "[98,   100] loss: 1.545\n",
            "[98,   110] loss: 1.551\n",
            "[98,   120] loss: 1.543\n",
            "[98,   130] loss: 1.561\n",
            "[98,   140] loss: 1.557\n",
            "[98,   150] loss: 1.552\n",
            "[98,   160] loss: 1.553\n",
            "[98,   170] loss: 1.549\n",
            "[98,   180] loss: 1.562\n",
            "[98,   190] loss: 1.559\n",
            "[98,   200] loss: 1.547\n",
            "[98,   210] loss: 1.550\n",
            "[98,   220] loss: 1.559\n",
            "[98,   230] loss: 1.557\n",
            "[98,   240] loss: 1.553\n",
            "[98,   250] loss: 1.554\n",
            "[98,   260] loss: 1.557\n",
            "[98,   270] loss: 1.566\n",
            "[98,   280] loss: 1.542\n",
            "[98,   290] loss: 1.558\n",
            "[98,   300] loss: 1.561\n",
            "[98,   310] loss: 1.550\n",
            "[98,   320] loss: 1.560\n",
            "[98,   330] loss: 1.560\n",
            "[98,   340] loss: 1.551\n",
            "[98,   350] loss: 1.540\n",
            "epoch 97 val_loss 64.21503460407257 val_steps 40 val_acc 0.8572\n",
            "[99,    10] loss: 1.554\n",
            "[99,    20] loss: 1.549\n",
            "[99,    30] loss: 1.544\n",
            "[99,    40] loss: 1.556\n",
            "[99,    50] loss: 1.550\n",
            "[99,    60] loss: 1.549\n",
            "[99,    70] loss: 1.554\n",
            "[99,    80] loss: 1.546\n",
            "[99,    90] loss: 1.555\n",
            "[99,   100] loss: 1.545\n",
            "[99,   110] loss: 1.548\n",
            "[99,   120] loss: 1.567\n",
            "[99,   130] loss: 1.555\n",
            "[99,   140] loss: 1.542\n",
            "[99,   150] loss: 1.547\n",
            "[99,   160] loss: 1.548\n",
            "[99,   170] loss: 1.550\n",
            "[99,   180] loss: 1.554\n",
            "[99,   190] loss: 1.566\n",
            "[99,   200] loss: 1.551\n",
            "[99,   210] loss: 1.559\n",
            "[99,   220] loss: 1.554\n",
            "[99,   230] loss: 1.572\n",
            "[99,   240] loss: 1.562\n",
            "[99,   250] loss: 1.549\n",
            "[99,   260] loss: 1.555\n",
            "[99,   270] loss: 1.545\n",
            "[99,   280] loss: 1.546\n",
            "[99,   290] loss: 1.543\n",
            "[99,   300] loss: 1.564\n",
            "[99,   310] loss: 1.554\n",
            "[99,   320] loss: 1.552\n",
            "[99,   330] loss: 1.556\n",
            "[99,   340] loss: 1.546\n",
            "[99,   350] loss: 1.558\n",
            "epoch 98 val_loss 64.35833978652954 val_steps 40 val_acc 0.858\n",
            "[100,    10] loss: 1.543\n",
            "[100,    20] loss: 1.560\n",
            "[100,    30] loss: 1.561\n",
            "[100,    40] loss: 1.553\n",
            "[100,    50] loss: 1.550\n",
            "[100,    60] loss: 1.544\n",
            "[100,    70] loss: 1.566\n",
            "[100,    80] loss: 1.560\n",
            "[100,    90] loss: 1.541\n",
            "[100,   100] loss: 1.554\n",
            "[100,   110] loss: 1.544\n",
            "[100,   120] loss: 1.559\n",
            "[100,   130] loss: 1.561\n",
            "[100,   140] loss: 1.561\n",
            "[100,   150] loss: 1.546\n",
            "[100,   160] loss: 1.551\n",
            "[100,   170] loss: 1.545\n",
            "[100,   180] loss: 1.545\n",
            "[100,   190] loss: 1.550\n",
            "[100,   200] loss: 1.555\n",
            "[100,   210] loss: 1.548\n",
            "[100,   220] loss: 1.563\n",
            "[100,   230] loss: 1.559\n",
            "[100,   240] loss: 1.546\n",
            "[100,   250] loss: 1.541\n",
            "[100,   260] loss: 1.541\n",
            "[100,   270] loss: 1.545\n",
            "[100,   280] loss: 1.547\n",
            "[100,   290] loss: 1.542\n",
            "[100,   300] loss: 1.547\n",
            "[100,   310] loss: 1.548\n",
            "[100,   320] loss: 1.555\n",
            "[100,   330] loss: 1.549\n",
            "[100,   340] loss: 1.558\n",
            "[100,   350] loss: 1.551\n",
            "epoch 99 val_loss 64.45178401470184 val_steps 40 val_acc 0.8548\n",
            "[101,    10] loss: 1.540\n",
            "[101,    20] loss: 1.546\n",
            "[101,    30] loss: 1.546\n",
            "[101,    40] loss: 1.538\n",
            "[101,    50] loss: 1.549\n",
            "[101,    60] loss: 1.544\n",
            "[101,    70] loss: 1.548\n",
            "[101,    80] loss: 1.557\n",
            "[101,    90] loss: 1.539\n",
            "[101,   100] loss: 1.548\n",
            "[101,   110] loss: 1.564\n",
            "[101,   120] loss: 1.551\n",
            "[101,   130] loss: 1.545\n",
            "[101,   140] loss: 1.562\n",
            "[101,   150] loss: 1.551\n",
            "[101,   160] loss: 1.549\n",
            "[101,   170] loss: 1.541\n",
            "[101,   180] loss: 1.561\n",
            "[101,   190] loss: 1.551\n",
            "[101,   200] loss: 1.554\n",
            "[101,   210] loss: 1.543\n",
            "[101,   220] loss: 1.548\n",
            "[101,   230] loss: 1.536\n",
            "[101,   240] loss: 1.551\n",
            "[101,   250] loss: 1.553\n",
            "[101,   260] loss: 1.550\n",
            "[101,   270] loss: 1.560\n",
            "[101,   280] loss: 1.556\n",
            "[101,   290] loss: 1.553\n",
            "[101,   300] loss: 1.552\n",
            "[101,   310] loss: 1.550\n",
            "[101,   320] loss: 1.539\n",
            "[101,   330] loss: 1.557\n",
            "[101,   340] loss: 1.554\n",
            "[101,   350] loss: 1.557\n",
            "epoch 100 val_loss 64.41942155361176 val_steps 40 val_acc 0.8546\n",
            "[102,    10] loss: 1.548\n",
            "[102,    20] loss: 1.545\n",
            "[102,    30] loss: 1.549\n",
            "[102,    40] loss: 1.559\n",
            "[102,    50] loss: 1.560\n",
            "[102,    60] loss: 1.545\n",
            "[102,    70] loss: 1.557\n",
            "[102,    80] loss: 1.535\n",
            "[102,    90] loss: 1.546\n",
            "[102,   100] loss: 1.551\n",
            "[102,   110] loss: 1.538\n",
            "[102,   120] loss: 1.554\n",
            "[102,   130] loss: 1.556\n",
            "[102,   140] loss: 1.552\n",
            "[102,   150] loss: 1.548\n",
            "[102,   160] loss: 1.556\n",
            "[102,   170] loss: 1.551\n",
            "[102,   180] loss: 1.541\n",
            "[102,   190] loss: 1.554\n",
            "[102,   200] loss: 1.535\n",
            "[102,   210] loss: 1.544\n",
            "[102,   220] loss: 1.543\n",
            "[102,   230] loss: 1.533\n",
            "[102,   240] loss: 1.539\n",
            "[102,   250] loss: 1.544\n",
            "[102,   260] loss: 1.558\n",
            "[102,   270] loss: 1.548\n",
            "[102,   280] loss: 1.551\n",
            "[102,   290] loss: 1.556\n",
            "[102,   300] loss: 1.559\n",
            "[102,   310] loss: 1.553\n",
            "[102,   320] loss: 1.544\n",
            "[102,   330] loss: 1.536\n",
            "[102,   340] loss: 1.544\n",
            "[102,   350] loss: 1.561\n",
            "epoch 101 val_loss 64.17863953113556 val_steps 40 val_acc 0.862\n",
            "[103,    10] loss: 1.548\n",
            "[103,    20] loss: 1.550\n",
            "[103,    30] loss: 1.536\n",
            "[103,    40] loss: 1.554\n",
            "[103,    50] loss: 1.550\n",
            "[103,    60] loss: 1.545\n",
            "[103,    70] loss: 1.557\n",
            "[103,    80] loss: 1.547\n",
            "[103,    90] loss: 1.547\n",
            "[103,   100] loss: 1.546\n",
            "[103,   110] loss: 1.539\n",
            "[103,   120] loss: 1.548\n",
            "[103,   130] loss: 1.545\n",
            "[103,   140] loss: 1.539\n",
            "[103,   150] loss: 1.547\n",
            "[103,   160] loss: 1.538\n",
            "[103,   170] loss: 1.556\n",
            "[103,   180] loss: 1.561\n",
            "[103,   190] loss: 1.561\n",
            "[103,   200] loss: 1.552\n",
            "[103,   210] loss: 1.544\n",
            "[103,   220] loss: 1.540\n",
            "[103,   230] loss: 1.551\n",
            "[103,   240] loss: 1.553\n",
            "[103,   250] loss: 1.539\n",
            "[103,   260] loss: 1.536\n",
            "[103,   270] loss: 1.543\n",
            "[103,   280] loss: 1.546\n",
            "[103,   290] loss: 1.543\n",
            "[103,   300] loss: 1.550\n",
            "[103,   310] loss: 1.553\n",
            "[103,   320] loss: 1.554\n",
            "[103,   330] loss: 1.549\n",
            "[103,   340] loss: 1.539\n",
            "[103,   350] loss: 1.546\n",
            "epoch 102 val_loss 64.15763819217682 val_steps 40 val_acc 0.8638\n",
            "[104,    10] loss: 1.556\n",
            "[104,    20] loss: 1.547\n",
            "[104,    30] loss: 1.550\n",
            "[104,    40] loss: 1.553\n",
            "[104,    50] loss: 1.543\n",
            "[104,    60] loss: 1.548\n",
            "[104,    70] loss: 1.542\n",
            "[104,    80] loss: 1.554\n",
            "[104,    90] loss: 1.534\n",
            "[104,   100] loss: 1.548\n",
            "[104,   110] loss: 1.538\n",
            "[104,   120] loss: 1.534\n",
            "[104,   130] loss: 1.541\n",
            "[104,   140] loss: 1.536\n",
            "[104,   150] loss: 1.544\n",
            "[104,   160] loss: 1.541\n",
            "[104,   170] loss: 1.554\n",
            "[104,   180] loss: 1.554\n",
            "[104,   190] loss: 1.544\n",
            "[104,   200] loss: 1.548\n",
            "[104,   210] loss: 1.551\n",
            "[104,   220] loss: 1.548\n",
            "[104,   230] loss: 1.555\n",
            "[104,   240] loss: 1.538\n",
            "[104,   250] loss: 1.549\n",
            "[104,   260] loss: 1.548\n",
            "[104,   270] loss: 1.566\n",
            "[104,   280] loss: 1.558\n",
            "[104,   290] loss: 1.546\n",
            "[104,   300] loss: 1.551\n",
            "[104,   310] loss: 1.547\n",
            "[104,   320] loss: 1.548\n",
            "[104,   330] loss: 1.556\n",
            "[104,   340] loss: 1.543\n",
            "[104,   350] loss: 1.537\n",
            "epoch 103 val_loss 64.23988258838654 val_steps 40 val_acc 0.8598\n",
            "[105,    10] loss: 1.539\n",
            "[105,    20] loss: 1.543\n",
            "[105,    30] loss: 1.548\n",
            "[105,    40] loss: 1.558\n",
            "[105,    50] loss: 1.542\n",
            "[105,    60] loss: 1.553\n",
            "[105,    70] loss: 1.560\n",
            "[105,    80] loss: 1.537\n",
            "[105,    90] loss: 1.554\n",
            "[105,   100] loss: 1.543\n",
            "[105,   110] loss: 1.547\n",
            "[105,   120] loss: 1.547\n",
            "[105,   130] loss: 1.542\n",
            "[105,   140] loss: 1.552\n",
            "[105,   150] loss: 1.532\n",
            "[105,   160] loss: 1.558\n",
            "[105,   170] loss: 1.550\n",
            "[105,   180] loss: 1.554\n",
            "[105,   190] loss: 1.545\n",
            "[105,   200] loss: 1.533\n",
            "[105,   210] loss: 1.549\n",
            "[105,   220] loss: 1.549\n",
            "[105,   230] loss: 1.543\n",
            "[105,   240] loss: 1.538\n",
            "[105,   250] loss: 1.551\n",
            "[105,   260] loss: 1.547\n",
            "[105,   270] loss: 1.539\n",
            "[105,   280] loss: 1.540\n",
            "[105,   290] loss: 1.552\n",
            "[105,   300] loss: 1.531\n",
            "[105,   310] loss: 1.548\n",
            "[105,   320] loss: 1.555\n",
            "[105,   330] loss: 1.538\n",
            "[105,   340] loss: 1.548\n",
            "[105,   350] loss: 1.544\n",
            "epoch 104 val_loss 63.95581293106079 val_steps 40 val_acc 0.861\n",
            "[106,    10] loss: 1.549\n",
            "[106,    20] loss: 1.538\n",
            "[106,    30] loss: 1.558\n",
            "[106,    40] loss: 1.544\n",
            "[106,    50] loss: 1.533\n",
            "[106,    60] loss: 1.540\n",
            "[106,    70] loss: 1.534\n",
            "[106,    80] loss: 1.552\n",
            "[106,    90] loss: 1.537\n",
            "[106,   100] loss: 1.543\n",
            "[106,   110] loss: 1.540\n",
            "[106,   120] loss: 1.564\n",
            "[106,   130] loss: 1.546\n",
            "[106,   140] loss: 1.538\n",
            "[106,   150] loss: 1.539\n",
            "[106,   160] loss: 1.538\n",
            "[106,   170] loss: 1.550\n",
            "[106,   180] loss: 1.538\n",
            "[106,   190] loss: 1.556\n",
            "[106,   200] loss: 1.546\n",
            "[106,   210] loss: 1.546\n",
            "[106,   220] loss: 1.554\n",
            "[106,   230] loss: 1.544\n",
            "[106,   240] loss: 1.545\n",
            "[106,   250] loss: 1.545\n",
            "[106,   260] loss: 1.540\n",
            "[106,   270] loss: 1.546\n",
            "[106,   280] loss: 1.539\n",
            "[106,   290] loss: 1.547\n",
            "[106,   300] loss: 1.567\n",
            "[106,   310] loss: 1.539\n",
            "[106,   320] loss: 1.539\n",
            "[106,   330] loss: 1.543\n",
            "[106,   340] loss: 1.541\n",
            "[106,   350] loss: 1.546\n",
            "epoch 105 val_loss 64.33329546451569 val_steps 40 val_acc 0.8588\n",
            "[107,    10] loss: 1.552\n",
            "[107,    20] loss: 1.536\n",
            "[107,    30] loss: 1.527\n",
            "[107,    40] loss: 1.549\n",
            "[107,    50] loss: 1.536\n",
            "[107,    60] loss: 1.543\n",
            "[107,    70] loss: 1.550\n",
            "[107,    80] loss: 1.538\n",
            "[107,    90] loss: 1.547\n",
            "[107,   100] loss: 1.543\n",
            "[107,   110] loss: 1.549\n",
            "[107,   120] loss: 1.546\n",
            "[107,   130] loss: 1.554\n",
            "[107,   140] loss: 1.550\n",
            "[107,   150] loss: 1.535\n",
            "[107,   160] loss: 1.535\n",
            "[107,   170] loss: 1.540\n",
            "[107,   180] loss: 1.530\n",
            "[107,   190] loss: 1.545\n",
            "[107,   200] loss: 1.552\n",
            "[107,   210] loss: 1.548\n",
            "[107,   220] loss: 1.557\n",
            "[107,   230] loss: 1.529\n",
            "[107,   240] loss: 1.544\n",
            "[107,   250] loss: 1.541\n",
            "[107,   260] loss: 1.550\n",
            "[107,   270] loss: 1.557\n",
            "[107,   280] loss: 1.540\n",
            "[107,   290] loss: 1.551\n",
            "[107,   300] loss: 1.540\n",
            "[107,   310] loss: 1.525\n",
            "[107,   320] loss: 1.543\n",
            "[107,   330] loss: 1.542\n",
            "[107,   340] loss: 1.551\n",
            "[107,   350] loss: 1.547\n",
            "epoch 106 val_loss 64.01508402824402 val_steps 40 val_acc 0.8574\n",
            "[108,    10] loss: 1.539\n",
            "[108,    20] loss: 1.541\n",
            "[108,    30] loss: 1.545\n",
            "[108,    40] loss: 1.540\n",
            "[108,    50] loss: 1.539\n",
            "[108,    60] loss: 1.532\n",
            "[108,    70] loss: 1.528\n",
            "[108,    80] loss: 1.535\n",
            "[108,    90] loss: 1.538\n",
            "[108,   100] loss: 1.534\n",
            "[108,   110] loss: 1.545\n",
            "[108,   120] loss: 1.547\n",
            "[108,   130] loss: 1.552\n",
            "[108,   140] loss: 1.543\n",
            "[108,   150] loss: 1.543\n",
            "[108,   160] loss: 1.538\n",
            "[108,   170] loss: 1.536\n",
            "[108,   180] loss: 1.542\n",
            "[108,   190] loss: 1.533\n",
            "[108,   200] loss: 1.541\n",
            "[108,   210] loss: 1.541\n",
            "[108,   220] loss: 1.545\n",
            "[108,   230] loss: 1.529\n",
            "[108,   240] loss: 1.549\n",
            "[108,   250] loss: 1.555\n",
            "[108,   260] loss: 1.540\n",
            "[108,   270] loss: 1.536\n",
            "[108,   280] loss: 1.540\n",
            "[108,   290] loss: 1.541\n",
            "[108,   300] loss: 1.544\n",
            "[108,   310] loss: 1.547\n",
            "[108,   320] loss: 1.539\n",
            "[108,   330] loss: 1.560\n",
            "[108,   340] loss: 1.549\n",
            "[108,   350] loss: 1.547\n",
            "epoch 107 val_loss 64.5750982761383 val_steps 40 val_acc 0.8572\n",
            "[109,    10] loss: 1.538\n",
            "[109,    20] loss: 1.547\n",
            "[109,    30] loss: 1.545\n",
            "[109,    40] loss: 1.555\n",
            "[109,    50] loss: 1.550\n",
            "[109,    60] loss: 1.541\n",
            "[109,    70] loss: 1.540\n",
            "[109,    80] loss: 1.546\n",
            "[109,    90] loss: 1.550\n",
            "[109,   100] loss: 1.545\n",
            "[109,   110] loss: 1.550\n",
            "[109,   120] loss: 1.556\n",
            "[109,   130] loss: 1.540\n",
            "[109,   140] loss: 1.533\n",
            "[109,   150] loss: 1.528\n",
            "[109,   160] loss: 1.540\n",
            "[109,   170] loss: 1.549\n",
            "[109,   180] loss: 1.540\n",
            "[109,   190] loss: 1.535\n",
            "[109,   200] loss: 1.535\n",
            "[109,   210] loss: 1.540\n",
            "[109,   220] loss: 1.547\n",
            "[109,   230] loss: 1.531\n",
            "[109,   240] loss: 1.548\n",
            "[109,   250] loss: 1.536\n",
            "[109,   260] loss: 1.546\n",
            "[109,   270] loss: 1.542\n",
            "[109,   280] loss: 1.544\n",
            "[109,   290] loss: 1.541\n",
            "[109,   300] loss: 1.542\n",
            "[109,   310] loss: 1.533\n",
            "[109,   320] loss: 1.538\n",
            "[109,   330] loss: 1.546\n",
            "[109,   340] loss: 1.546\n",
            "[109,   350] loss: 1.542\n",
            "epoch 108 val_loss 64.03916931152344 val_steps 40 val_acc 0.8646\n",
            "[110,    10] loss: 1.557\n",
            "[110,    20] loss: 1.548\n",
            "[110,    30] loss: 1.546\n",
            "[110,    40] loss: 1.532\n",
            "[110,    50] loss: 1.541\n",
            "[110,    60] loss: 1.530\n",
            "[110,    70] loss: 1.535\n",
            "[110,    80] loss: 1.535\n",
            "[110,    90] loss: 1.546\n",
            "[110,   100] loss: 1.532\n",
            "[110,   110] loss: 1.543\n",
            "[110,   120] loss: 1.560\n",
            "[110,   130] loss: 1.538\n",
            "[110,   140] loss: 1.540\n",
            "[110,   150] loss: 1.550\n",
            "[110,   160] loss: 1.520\n",
            "[110,   170] loss: 1.549\n",
            "[110,   180] loss: 1.543\n",
            "[110,   190] loss: 1.541\n",
            "[110,   200] loss: 1.550\n",
            "[110,   210] loss: 1.547\n",
            "[110,   220] loss: 1.544\n",
            "[110,   230] loss: 1.543\n",
            "[110,   240] loss: 1.534\n",
            "[110,   250] loss: 1.533\n",
            "[110,   260] loss: 1.529\n",
            "[110,   270] loss: 1.527\n",
            "[110,   280] loss: 1.548\n",
            "[110,   290] loss: 1.543\n",
            "[110,   300] loss: 1.537\n",
            "[110,   310] loss: 1.546\n",
            "[110,   320] loss: 1.534\n",
            "[110,   330] loss: 1.539\n",
            "[110,   340] loss: 1.544\n",
            "[110,   350] loss: 1.538\n",
            "epoch 109 val_loss 63.92395102977753 val_steps 40 val_acc 0.861\n",
            "[111,    10] loss: 1.542\n",
            "[111,    20] loss: 1.532\n",
            "[111,    30] loss: 1.534\n",
            "[111,    40] loss: 1.539\n",
            "[111,    50] loss: 1.530\n",
            "[111,    60] loss: 1.537\n",
            "[111,    70] loss: 1.533\n",
            "[111,    80] loss: 1.551\n",
            "[111,    90] loss: 1.541\n",
            "[111,   100] loss: 1.539\n",
            "[111,   110] loss: 1.543\n",
            "[111,   120] loss: 1.541\n",
            "[111,   130] loss: 1.544\n",
            "[111,   140] loss: 1.546\n",
            "[111,   150] loss: 1.548\n",
            "[111,   160] loss: 1.526\n",
            "[111,   170] loss: 1.536\n",
            "[111,   180] loss: 1.540\n",
            "[111,   190] loss: 1.533\n",
            "[111,   200] loss: 1.528\n",
            "[111,   210] loss: 1.555\n",
            "[111,   220] loss: 1.532\n",
            "[111,   230] loss: 1.551\n",
            "[111,   240] loss: 1.539\n",
            "[111,   250] loss: 1.556\n",
            "[111,   260] loss: 1.535\n",
            "[111,   270] loss: 1.551\n",
            "[111,   280] loss: 1.548\n",
            "[111,   290] loss: 1.538\n",
            "[111,   300] loss: 1.546\n",
            "[111,   310] loss: 1.537\n",
            "[111,   320] loss: 1.551\n",
            "[111,   330] loss: 1.539\n",
            "[111,   340] loss: 1.541\n",
            "[111,   350] loss: 1.544\n",
            "epoch 110 val_loss 64.23758316040039 val_steps 40 val_acc 0.8604\n",
            "[112,    10] loss: 1.533\n",
            "[112,    20] loss: 1.536\n",
            "[112,    30] loss: 1.535\n",
            "[112,    40] loss: 1.547\n",
            "[112,    50] loss: 1.546\n",
            "[112,    60] loss: 1.540\n",
            "[112,    70] loss: 1.557\n",
            "[112,    80] loss: 1.531\n",
            "[112,    90] loss: 1.539\n",
            "[112,   100] loss: 1.529\n",
            "[112,   110] loss: 1.538\n",
            "[112,   120] loss: 1.538\n",
            "[112,   130] loss: 1.527\n",
            "[112,   140] loss: 1.536\n",
            "[112,   150] loss: 1.539\n",
            "[112,   160] loss: 1.536\n",
            "[112,   170] loss: 1.541\n",
            "[112,   180] loss: 1.535\n",
            "[112,   190] loss: 1.549\n",
            "[112,   200] loss: 1.544\n",
            "[112,   210] loss: 1.530\n",
            "[112,   220] loss: 1.533\n",
            "[112,   230] loss: 1.542\n",
            "[112,   240] loss: 1.540\n",
            "[112,   250] loss: 1.535\n",
            "[112,   260] loss: 1.541\n",
            "[112,   270] loss: 1.539\n",
            "[112,   280] loss: 1.541\n",
            "[112,   290] loss: 1.538\n",
            "[112,   300] loss: 1.550\n",
            "[112,   310] loss: 1.538\n",
            "[112,   320] loss: 1.538\n",
            "[112,   330] loss: 1.554\n",
            "[112,   340] loss: 1.537\n",
            "[112,   350] loss: 1.535\n",
            "epoch 111 val_loss 64.0118545293808 val_steps 40 val_acc 0.8658\n",
            "[113,    10] loss: 1.542\n",
            "[113,    20] loss: 1.546\n",
            "[113,    30] loss: 1.537\n",
            "[113,    40] loss: 1.530\n",
            "[113,    50] loss: 1.541\n",
            "[113,    60] loss: 1.549\n",
            "[113,    70] loss: 1.540\n",
            "[113,    80] loss: 1.523\n",
            "[113,    90] loss: 1.545\n",
            "[113,   100] loss: 1.533\n",
            "[113,   110] loss: 1.538\n",
            "[113,   120] loss: 1.542\n",
            "[113,   130] loss: 1.536\n",
            "[113,   140] loss: 1.544\n",
            "[113,   150] loss: 1.540\n",
            "[113,   160] loss: 1.538\n",
            "[113,   170] loss: 1.542\n",
            "[113,   180] loss: 1.541\n",
            "[113,   190] loss: 1.538\n",
            "[113,   200] loss: 1.556\n",
            "[113,   210] loss: 1.538\n",
            "[113,   220] loss: 1.546\n",
            "[113,   230] loss: 1.552\n",
            "[113,   240] loss: 1.541\n",
            "[113,   250] loss: 1.528\n",
            "[113,   260] loss: 1.535\n",
            "[113,   270] loss: 1.532\n",
            "[113,   280] loss: 1.544\n",
            "[113,   290] loss: 1.540\n",
            "[113,   300] loss: 1.539\n",
            "[113,   310] loss: 1.544\n",
            "[113,   320] loss: 1.544\n",
            "[113,   330] loss: 1.536\n",
            "[113,   340] loss: 1.531\n",
            "[113,   350] loss: 1.546\n",
            "epoch 112 val_loss 64.34864592552185 val_steps 40 val_acc 0.86\n",
            "[114,    10] loss: 1.547\n",
            "[114,    20] loss: 1.542\n",
            "[114,    30] loss: 1.525\n",
            "[114,    40] loss: 1.536\n",
            "[114,    50] loss: 1.531\n",
            "[114,    60] loss: 1.543\n",
            "[114,    70] loss: 1.535\n",
            "[114,    80] loss: 1.525\n",
            "[114,    90] loss: 1.530\n",
            "[114,   100] loss: 1.543\n",
            "[114,   110] loss: 1.534\n",
            "[114,   120] loss: 1.536\n",
            "[114,   130] loss: 1.540\n",
            "[114,   140] loss: 1.531\n",
            "[114,   150] loss: 1.542\n",
            "[114,   160] loss: 1.531\n",
            "[114,   170] loss: 1.539\n",
            "[114,   180] loss: 1.543\n",
            "[114,   190] loss: 1.545\n",
            "[114,   200] loss: 1.535\n",
            "[114,   210] loss: 1.545\n",
            "[114,   220] loss: 1.549\n",
            "[114,   230] loss: 1.537\n",
            "[114,   240] loss: 1.536\n",
            "[114,   250] loss: 1.542\n",
            "[114,   260] loss: 1.536\n",
            "[114,   270] loss: 1.531\n",
            "[114,   280] loss: 1.542\n",
            "[114,   290] loss: 1.531\n",
            "[114,   300] loss: 1.525\n",
            "[114,   310] loss: 1.548\n",
            "[114,   320] loss: 1.537\n",
            "[114,   330] loss: 1.547\n",
            "[114,   340] loss: 1.536\n",
            "[114,   350] loss: 1.544\n",
            "epoch 113 val_loss 64.01258027553558 val_steps 40 val_acc 0.8606\n",
            "[115,    10] loss: 1.535\n",
            "[115,    20] loss: 1.545\n",
            "[115,    30] loss: 1.524\n",
            "[115,    40] loss: 1.565\n",
            "[115,    50] loss: 1.529\n",
            "[115,    60] loss: 1.536\n",
            "[115,    70] loss: 1.527\n",
            "[115,    80] loss: 1.535\n",
            "[115,    90] loss: 1.539\n",
            "[115,   100] loss: 1.540\n",
            "[115,   110] loss: 1.532\n",
            "[115,   120] loss: 1.543\n",
            "[115,   130] loss: 1.547\n",
            "[115,   140] loss: 1.531\n",
            "[115,   150] loss: 1.551\n",
            "[115,   160] loss: 1.535\n",
            "[115,   170] loss: 1.545\n",
            "[115,   180] loss: 1.530\n",
            "[115,   190] loss: 1.533\n",
            "[115,   200] loss: 1.537\n",
            "[115,   210] loss: 1.529\n",
            "[115,   220] loss: 1.541\n",
            "[115,   230] loss: 1.529\n",
            "[115,   240] loss: 1.539\n",
            "[115,   250] loss: 1.529\n",
            "[115,   260] loss: 1.536\n",
            "[115,   270] loss: 1.542\n",
            "[115,   280] loss: 1.542\n",
            "[115,   290] loss: 1.528\n",
            "[115,   300] loss: 1.537\n",
            "[115,   310] loss: 1.542\n",
            "[115,   320] loss: 1.531\n",
            "[115,   330] loss: 1.545\n",
            "[115,   340] loss: 1.536\n",
            "[115,   350] loss: 1.536\n",
            "epoch 114 val_loss 63.69090509414673 val_steps 40 val_acc 0.8682\n",
            "[116,    10] loss: 1.527\n",
            "[116,    20] loss: 1.542\n",
            "[116,    30] loss: 1.534\n",
            "[116,    40] loss: 1.540\n",
            "[116,    50] loss: 1.536\n",
            "[116,    60] loss: 1.548\n",
            "[116,    70] loss: 1.535\n",
            "[116,    80] loss: 1.539\n",
            "[116,    90] loss: 1.536\n",
            "[116,   100] loss: 1.536\n",
            "[116,   110] loss: 1.543\n",
            "[116,   120] loss: 1.542\n",
            "[116,   130] loss: 1.532\n",
            "[116,   140] loss: 1.537\n",
            "[116,   150] loss: 1.545\n",
            "[116,   160] loss: 1.535\n",
            "[116,   170] loss: 1.532\n",
            "[116,   180] loss: 1.525\n",
            "[116,   190] loss: 1.541\n",
            "[116,   200] loss: 1.556\n",
            "[116,   210] loss: 1.536\n",
            "[116,   220] loss: 1.538\n",
            "[116,   230] loss: 1.538\n",
            "[116,   240] loss: 1.540\n",
            "[116,   250] loss: 1.527\n",
            "[116,   260] loss: 1.538\n",
            "[116,   270] loss: 1.530\n",
            "[116,   280] loss: 1.529\n",
            "[116,   290] loss: 1.543\n",
            "[116,   300] loss: 1.527\n",
            "[116,   310] loss: 1.522\n",
            "[116,   320] loss: 1.537\n",
            "[116,   330] loss: 1.543\n",
            "[116,   340] loss: 1.535\n",
            "[116,   350] loss: 1.534\n",
            "epoch 115 val_loss 64.15154266357422 val_steps 40 val_acc 0.866\n",
            "[117,    10] loss: 1.537\n",
            "[117,    20] loss: 1.532\n",
            "[117,    30] loss: 1.533\n",
            "[117,    40] loss: 1.533\n",
            "[117,    50] loss: 1.536\n",
            "[117,    60] loss: 1.536\n",
            "[117,    70] loss: 1.544\n",
            "[117,    80] loss: 1.529\n",
            "[117,    90] loss: 1.545\n",
            "[117,   100] loss: 1.531\n",
            "[117,   110] loss: 1.542\n",
            "[117,   120] loss: 1.534\n",
            "[117,   130] loss: 1.534\n",
            "[117,   140] loss: 1.531\n",
            "[117,   150] loss: 1.530\n",
            "[117,   160] loss: 1.525\n",
            "[117,   170] loss: 1.542\n",
            "[117,   180] loss: 1.527\n",
            "[117,   190] loss: 1.535\n",
            "[117,   200] loss: 1.536\n",
            "[117,   210] loss: 1.536\n",
            "[117,   220] loss: 1.540\n",
            "[117,   230] loss: 1.534\n",
            "[117,   240] loss: 1.545\n",
            "[117,   250] loss: 1.543\n",
            "[117,   260] loss: 1.535\n",
            "[117,   270] loss: 1.535\n",
            "[117,   280] loss: 1.545\n",
            "[117,   290] loss: 1.538\n",
            "[117,   300] loss: 1.538\n",
            "[117,   310] loss: 1.535\n",
            "[117,   320] loss: 1.523\n",
            "[117,   330] loss: 1.538\n",
            "[117,   340] loss: 1.538\n",
            "[117,   350] loss: 1.545\n",
            "epoch 116 val_loss 63.941269636154175 val_steps 40 val_acc 0.8656\n",
            "[118,    10] loss: 1.535\n",
            "[118,    20] loss: 1.529\n",
            "[118,    30] loss: 1.531\n",
            "[118,    40] loss: 1.533\n",
            "[118,    50] loss: 1.529\n",
            "[118,    60] loss: 1.557\n",
            "[118,    70] loss: 1.536\n",
            "[118,    80] loss: 1.536\n",
            "[118,    90] loss: 1.530\n",
            "[118,   100] loss: 1.535\n",
            "[118,   110] loss: 1.533\n",
            "[118,   120] loss: 1.539\n",
            "[118,   130] loss: 1.534\n",
            "[118,   140] loss: 1.530\n",
            "[118,   150] loss: 1.531\n",
            "[118,   160] loss: 1.529\n",
            "[118,   170] loss: 1.523\n",
            "[118,   180] loss: 1.539\n",
            "[118,   190] loss: 1.533\n",
            "[118,   200] loss: 1.540\n",
            "[118,   210] loss: 1.532\n",
            "[118,   220] loss: 1.546\n",
            "[118,   230] loss: 1.528\n",
            "[118,   240] loss: 1.542\n",
            "[118,   250] loss: 1.548\n",
            "[118,   260] loss: 1.533\n",
            "[118,   270] loss: 1.547\n",
            "[118,   280] loss: 1.532\n",
            "[118,   290] loss: 1.529\n",
            "[118,   300] loss: 1.539\n",
            "[118,   310] loss: 1.541\n",
            "[118,   320] loss: 1.536\n",
            "[118,   330] loss: 1.542\n",
            "[118,   340] loss: 1.531\n",
            "[118,   350] loss: 1.537\n",
            "epoch 117 val_loss 63.96227407455444 val_steps 40 val_acc 0.868\n",
            "[119,    10] loss: 1.530\n",
            "[119,    20] loss: 1.536\n",
            "[119,    30] loss: 1.534\n",
            "[119,    40] loss: 1.531\n",
            "[119,    50] loss: 1.543\n",
            "[119,    60] loss: 1.527\n",
            "[119,    70] loss: 1.536\n",
            "[119,    80] loss: 1.537\n",
            "[119,    90] loss: 1.534\n",
            "[119,   100] loss: 1.532\n",
            "[119,   110] loss: 1.531\n",
            "[119,   120] loss: 1.540\n",
            "[119,   130] loss: 1.530\n",
            "[119,   140] loss: 1.532\n",
            "[119,   150] loss: 1.541\n",
            "[119,   160] loss: 1.545\n",
            "[119,   170] loss: 1.530\n",
            "[119,   180] loss: 1.534\n",
            "[119,   190] loss: 1.535\n",
            "[119,   200] loss: 1.532\n",
            "[119,   210] loss: 1.540\n",
            "[119,   220] loss: 1.541\n",
            "[119,   230] loss: 1.540\n",
            "[119,   240] loss: 1.541\n",
            "[119,   250] loss: 1.532\n",
            "[119,   260] loss: 1.544\n",
            "[119,   270] loss: 1.533\n",
            "[119,   280] loss: 1.538\n",
            "[119,   290] loss: 1.536\n",
            "[119,   300] loss: 1.540\n",
            "[119,   310] loss: 1.534\n",
            "[119,   320] loss: 1.533\n",
            "[119,   330] loss: 1.542\n",
            "[119,   340] loss: 1.542\n",
            "[119,   350] loss: 1.536\n",
            "epoch 118 val_loss 63.95794916152954 val_steps 40 val_acc 0.8614\n",
            "[120,    10] loss: 1.542\n",
            "[120,    20] loss: 1.524\n",
            "[120,    30] loss: 1.538\n",
            "[120,    40] loss: 1.532\n",
            "[120,    50] loss: 1.532\n",
            "[120,    60] loss: 1.523\n",
            "[120,    70] loss: 1.528\n",
            "[120,    80] loss: 1.536\n",
            "[120,    90] loss: 1.539\n",
            "[120,   100] loss: 1.543\n",
            "[120,   110] loss: 1.535\n",
            "[120,   120] loss: 1.532\n",
            "[120,   130] loss: 1.537\n",
            "[120,   140] loss: 1.540\n",
            "[120,   150] loss: 1.533\n",
            "[120,   160] loss: 1.523\n",
            "[120,   170] loss: 1.533\n",
            "[120,   180] loss: 1.548\n",
            "[120,   190] loss: 1.537\n",
            "[120,   200] loss: 1.532\n",
            "[120,   210] loss: 1.533\n",
            "[120,   220] loss: 1.539\n",
            "[120,   230] loss: 1.548\n",
            "[120,   240] loss: 1.538\n",
            "[120,   250] loss: 1.529\n",
            "[120,   260] loss: 1.536\n",
            "[120,   270] loss: 1.536\n",
            "[120,   280] loss: 1.528\n",
            "[120,   290] loss: 1.530\n",
            "[120,   300] loss: 1.527\n",
            "[120,   310] loss: 1.531\n",
            "[120,   320] loss: 1.543\n",
            "[120,   330] loss: 1.531\n",
            "[120,   340] loss: 1.536\n",
            "[120,   350] loss: 1.537\n",
            "epoch 119 val_loss 63.899574398994446 val_steps 40 val_acc 0.8632\n",
            "[121,    10] loss: 1.534\n",
            "[121,    20] loss: 1.533\n",
            "[121,    30] loss: 1.531\n",
            "[121,    40] loss: 1.538\n",
            "[121,    50] loss: 1.540\n",
            "[121,    60] loss: 1.523\n",
            "[121,    70] loss: 1.530\n",
            "[121,    80] loss: 1.522\n",
            "[121,    90] loss: 1.530\n",
            "[121,   100] loss: 1.532\n",
            "[121,   110] loss: 1.536\n",
            "[121,   120] loss: 1.532\n",
            "[121,   130] loss: 1.538\n",
            "[121,   140] loss: 1.541\n",
            "[121,   150] loss: 1.538\n",
            "[121,   160] loss: 1.527\n",
            "[121,   170] loss: 1.534\n",
            "[121,   180] loss: 1.533\n",
            "[121,   190] loss: 1.536\n",
            "[121,   200] loss: 1.538\n",
            "[121,   210] loss: 1.526\n",
            "[121,   220] loss: 1.536\n",
            "[121,   230] loss: 1.544\n",
            "[121,   240] loss: 1.544\n",
            "[121,   250] loss: 1.531\n",
            "[121,   260] loss: 1.529\n",
            "[121,   270] loss: 1.550\n",
            "[121,   280] loss: 1.524\n",
            "[121,   290] loss: 1.540\n",
            "[121,   300] loss: 1.524\n",
            "[121,   310] loss: 1.523\n",
            "[121,   320] loss: 1.536\n",
            "[121,   330] loss: 1.530\n",
            "[121,   340] loss: 1.538\n",
            "[121,   350] loss: 1.544\n",
            "epoch 120 val_loss 63.93258607387543 val_steps 40 val_acc 0.8682\n",
            "[122,    10] loss: 1.536\n",
            "[122,    20] loss: 1.529\n",
            "[122,    30] loss: 1.528\n",
            "[122,    40] loss: 1.538\n",
            "[122,    50] loss: 1.529\n",
            "[122,    60] loss: 1.530\n",
            "[122,    70] loss: 1.533\n",
            "[122,    80] loss: 1.543\n",
            "[122,    90] loss: 1.532\n",
            "[122,   100] loss: 1.531\n",
            "[122,   110] loss: 1.532\n",
            "[122,   120] loss: 1.525\n",
            "[122,   130] loss: 1.535\n",
            "[122,   140] loss: 1.526\n",
            "[122,   150] loss: 1.535\n",
            "[122,   160] loss: 1.525\n",
            "[122,   170] loss: 1.539\n",
            "[122,   180] loss: 1.533\n",
            "[122,   190] loss: 1.539\n",
            "[122,   200] loss: 1.533\n",
            "[122,   210] loss: 1.547\n",
            "[122,   220] loss: 1.527\n",
            "[122,   230] loss: 1.535\n",
            "[122,   240] loss: 1.543\n",
            "[122,   250] loss: 1.536\n",
            "[122,   260] loss: 1.534\n",
            "[122,   270] loss: 1.538\n",
            "[122,   280] loss: 1.535\n",
            "[122,   290] loss: 1.540\n",
            "[122,   300] loss: 1.536\n",
            "[122,   310] loss: 1.537\n",
            "[122,   320] loss: 1.523\n",
            "[122,   330] loss: 1.534\n",
            "[122,   340] loss: 1.529\n",
            "[122,   350] loss: 1.536\n",
            "epoch 121 val_loss 63.806878209114075 val_steps 40 val_acc 0.8672\n",
            "[123,    10] loss: 1.536\n",
            "[123,    20] loss: 1.531\n",
            "[123,    30] loss: 1.535\n",
            "[123,    40] loss: 1.533\n",
            "[123,    50] loss: 1.538\n",
            "[123,    60] loss: 1.525\n",
            "[123,    70] loss: 1.532\n",
            "[123,    80] loss: 1.538\n",
            "[123,    90] loss: 1.528\n",
            "[123,   100] loss: 1.536\n",
            "[123,   110] loss: 1.542\n",
            "[123,   120] loss: 1.541\n",
            "[123,   130] loss: 1.526\n",
            "[123,   140] loss: 1.539\n",
            "[123,   150] loss: 1.535\n",
            "[123,   160] loss: 1.532\n",
            "[123,   170] loss: 1.529\n",
            "[123,   180] loss: 1.532\n",
            "[123,   190] loss: 1.543\n",
            "[123,   200] loss: 1.540\n",
            "[123,   210] loss: 1.530\n",
            "[123,   220] loss: 1.522\n",
            "[123,   230] loss: 1.537\n",
            "[123,   240] loss: 1.520\n",
            "[123,   250] loss: 1.526\n",
            "[123,   260] loss: 1.537\n",
            "[123,   270] loss: 1.541\n",
            "[123,   280] loss: 1.533\n",
            "[123,   290] loss: 1.529\n",
            "[123,   300] loss: 1.531\n",
            "[123,   310] loss: 1.536\n",
            "[123,   320] loss: 1.522\n",
            "[123,   330] loss: 1.535\n",
            "[123,   340] loss: 1.535\n",
            "[123,   350] loss: 1.533\n",
            "epoch 122 val_loss 64.0358533859253 val_steps 40 val_acc 0.8638\n",
            "[124,    10] loss: 1.526\n",
            "[124,    20] loss: 1.530\n",
            "[124,    30] loss: 1.530\n",
            "[124,    40] loss: 1.535\n",
            "[124,    50] loss: 1.528\n",
            "[124,    60] loss: 1.536\n",
            "[124,    70] loss: 1.546\n",
            "[124,    80] loss: 1.545\n",
            "[124,    90] loss: 1.525\n",
            "[124,   100] loss: 1.531\n",
            "[124,   110] loss: 1.525\n",
            "[124,   120] loss: 1.539\n",
            "[124,   130] loss: 1.531\n",
            "[124,   140] loss: 1.540\n",
            "[124,   150] loss: 1.532\n",
            "[124,   160] loss: 1.536\n",
            "[124,   170] loss: 1.543\n",
            "[124,   180] loss: 1.536\n",
            "[124,   190] loss: 1.537\n",
            "[124,   200] loss: 1.525\n",
            "[124,   210] loss: 1.541\n",
            "[124,   220] loss: 1.526\n",
            "[124,   230] loss: 1.527\n",
            "[124,   240] loss: 1.533\n",
            "[124,   250] loss: 1.534\n",
            "[124,   260] loss: 1.535\n",
            "[124,   270] loss: 1.535\n",
            "[124,   280] loss: 1.527\n",
            "[124,   290] loss: 1.529\n",
            "[124,   300] loss: 1.525\n",
            "[124,   310] loss: 1.523\n",
            "[124,   320] loss: 1.526\n",
            "[124,   330] loss: 1.524\n",
            "[124,   340] loss: 1.535\n",
            "[124,   350] loss: 1.529\n",
            "epoch 123 val_loss 64.1006772518158 val_steps 40 val_acc 0.8654\n",
            "[125,    10] loss: 1.531\n",
            "[125,    20] loss: 1.538\n",
            "[125,    30] loss: 1.530\n",
            "[125,    40] loss: 1.538\n",
            "[125,    50] loss: 1.527\n",
            "[125,    60] loss: 1.529\n",
            "[125,    70] loss: 1.535\n",
            "[125,    80] loss: 1.529\n",
            "[125,    90] loss: 1.523\n",
            "[125,   100] loss: 1.529\n",
            "[125,   110] loss: 1.528\n",
            "[125,   120] loss: 1.523\n",
            "[125,   130] loss: 1.538\n",
            "[125,   140] loss: 1.532\n",
            "[125,   150] loss: 1.530\n",
            "[125,   160] loss: 1.538\n",
            "[125,   170] loss: 1.529\n",
            "[125,   180] loss: 1.538\n",
            "[125,   190] loss: 1.539\n",
            "[125,   200] loss: 1.529\n",
            "[125,   210] loss: 1.543\n",
            "[125,   220] loss: 1.541\n",
            "[125,   230] loss: 1.544\n",
            "[125,   240] loss: 1.535\n",
            "[125,   250] loss: 1.534\n",
            "[125,   260] loss: 1.530\n",
            "[125,   270] loss: 1.530\n",
            "[125,   280] loss: 1.540\n",
            "[125,   290] loss: 1.523\n",
            "[125,   300] loss: 1.520\n",
            "[125,   310] loss: 1.532\n",
            "[125,   320] loss: 1.538\n",
            "[125,   330] loss: 1.535\n",
            "[125,   340] loss: 1.534\n",
            "[125,   350] loss: 1.527\n",
            "epoch 124 val_loss 63.971256732940674 val_steps 40 val_acc 0.866\n",
            "[126,    10] loss: 1.529\n",
            "[126,    20] loss: 1.533\n",
            "[126,    30] loss: 1.527\n",
            "[126,    40] loss: 1.542\n",
            "[126,    50] loss: 1.527\n",
            "[126,    60] loss: 1.533\n",
            "[126,    70] loss: 1.528\n",
            "[126,    80] loss: 1.532\n",
            "[126,    90] loss: 1.536\n",
            "[126,   100] loss: 1.530\n",
            "[126,   110] loss: 1.526\n",
            "[126,   120] loss: 1.535\n",
            "[126,   130] loss: 1.535\n",
            "[126,   140] loss: 1.521\n",
            "[126,   150] loss: 1.517\n",
            "[126,   160] loss: 1.545\n",
            "[126,   170] loss: 1.534\n",
            "[126,   180] loss: 1.527\n",
            "[126,   190] loss: 1.526\n",
            "[126,   200] loss: 1.537\n",
            "[126,   210] loss: 1.533\n",
            "[126,   220] loss: 1.528\n",
            "[126,   230] loss: 1.524\n",
            "[126,   240] loss: 1.536\n",
            "[126,   250] loss: 1.534\n",
            "[126,   260] loss: 1.531\n",
            "[126,   270] loss: 1.531\n",
            "[126,   280] loss: 1.537\n",
            "[126,   290] loss: 1.535\n",
            "[126,   300] loss: 1.529\n",
            "[126,   310] loss: 1.527\n",
            "[126,   320] loss: 1.535\n",
            "[126,   330] loss: 1.526\n",
            "[126,   340] loss: 1.523\n",
            "[126,   350] loss: 1.525\n",
            "epoch 125 val_loss 64.1820718050003 val_steps 40 val_acc 0.8628\n",
            "[127,    10] loss: 1.531\n",
            "[127,    20] loss: 1.531\n",
            "[127,    30] loss: 1.528\n",
            "[127,    40] loss: 1.541\n",
            "[127,    50] loss: 1.538\n",
            "[127,    60] loss: 1.531\n",
            "[127,    70] loss: 1.551\n",
            "[127,    80] loss: 1.532\n",
            "[127,    90] loss: 1.521\n",
            "[127,   100] loss: 1.521\n",
            "[127,   110] loss: 1.530\n",
            "[127,   120] loss: 1.527\n",
            "[127,   130] loss: 1.530\n",
            "[127,   140] loss: 1.534\n",
            "[127,   150] loss: 1.526\n",
            "[127,   160] loss: 1.527\n",
            "[127,   170] loss: 1.533\n",
            "[127,   180] loss: 1.541\n",
            "[127,   190] loss: 1.532\n",
            "[127,   200] loss: 1.542\n",
            "[127,   210] loss: 1.521\n",
            "[127,   220] loss: 1.549\n",
            "[127,   230] loss: 1.523\n",
            "[127,   240] loss: 1.520\n",
            "[127,   250] loss: 1.534\n",
            "[127,   260] loss: 1.531\n",
            "[127,   270] loss: 1.534\n",
            "[127,   280] loss: 1.529\n",
            "[127,   290] loss: 1.540\n",
            "[127,   300] loss: 1.529\n",
            "[127,   310] loss: 1.531\n",
            "[127,   320] loss: 1.524\n",
            "[127,   330] loss: 1.531\n",
            "[127,   340] loss: 1.538\n",
            "[127,   350] loss: 1.543\n",
            "epoch 126 val_loss 63.85287845134735 val_steps 40 val_acc 0.868\n",
            "[128,    10] loss: 1.530\n",
            "[128,    20] loss: 1.529\n",
            "[128,    30] loss: 1.531\n",
            "[128,    40] loss: 1.539\n",
            "[128,    50] loss: 1.540\n",
            "[128,    60] loss: 1.531\n",
            "[128,    70] loss: 1.533\n",
            "[128,    80] loss: 1.530\n",
            "[128,    90] loss: 1.525\n",
            "[128,   100] loss: 1.517\n",
            "[128,   110] loss: 1.531\n",
            "[128,   120] loss: 1.529\n",
            "[128,   130] loss: 1.536\n",
            "[128,   140] loss: 1.519\n",
            "[128,   150] loss: 1.536\n",
            "[128,   160] loss: 1.518\n",
            "[128,   170] loss: 1.510\n",
            "[128,   180] loss: 1.534\n",
            "[128,   190] loss: 1.551\n",
            "[128,   200] loss: 1.526\n",
            "[128,   210] loss: 1.526\n",
            "[128,   220] loss: 1.532\n",
            "[128,   230] loss: 1.524\n",
            "[128,   240] loss: 1.529\n",
            "[128,   250] loss: 1.532\n",
            "[128,   260] loss: 1.532\n",
            "[128,   270] loss: 1.534\n",
            "[128,   280] loss: 1.536\n",
            "[128,   290] loss: 1.522\n",
            "[128,   300] loss: 1.539\n",
            "[128,   310] loss: 1.540\n",
            "[128,   320] loss: 1.536\n",
            "[128,   330] loss: 1.542\n",
            "[128,   340] loss: 1.529\n",
            "[128,   350] loss: 1.518\n",
            "epoch 127 val_loss 64.363276720047 val_steps 40 val_acc 0.8648\n",
            "[129,    10] loss: 1.535\n",
            "[129,    20] loss: 1.531\n",
            "[129,    30] loss: 1.526\n",
            "[129,    40] loss: 1.523\n",
            "[129,    50] loss: 1.531\n",
            "[129,    60] loss: 1.531\n",
            "[129,    70] loss: 1.532\n",
            "[129,    80] loss: 1.533\n",
            "[129,    90] loss: 1.531\n",
            "[129,   100] loss: 1.528\n",
            "[129,   110] loss: 1.539\n",
            "[129,   120] loss: 1.520\n",
            "[129,   130] loss: 1.534\n",
            "[129,   140] loss: 1.530\n",
            "[129,   150] loss: 1.519\n",
            "[129,   160] loss: 1.528\n",
            "[129,   170] loss: 1.522\n",
            "[129,   180] loss: 1.525\n",
            "[129,   190] loss: 1.530\n",
            "[129,   200] loss: 1.523\n",
            "[129,   210] loss: 1.536\n",
            "[129,   220] loss: 1.532\n",
            "[129,   230] loss: 1.521\n",
            "[129,   240] loss: 1.528\n",
            "[129,   250] loss: 1.527\n",
            "[129,   260] loss: 1.533\n",
            "[129,   270] loss: 1.528\n",
            "[129,   280] loss: 1.522\n",
            "[129,   290] loss: 1.537\n",
            "[129,   300] loss: 1.535\n",
            "[129,   310] loss: 1.527\n",
            "[129,   320] loss: 1.541\n",
            "[129,   330] loss: 1.526\n",
            "[129,   340] loss: 1.546\n",
            "[129,   350] loss: 1.536\n",
            "epoch 128 val_loss 63.89411401748657 val_steps 40 val_acc 0.8658\n",
            "[130,    10] loss: 1.536\n",
            "[130,    20] loss: 1.535\n",
            "[130,    30] loss: 1.522\n",
            "[130,    40] loss: 1.530\n",
            "[130,    50] loss: 1.526\n",
            "[130,    60] loss: 1.522\n",
            "[130,    70] loss: 1.533\n",
            "[130,    80] loss: 1.528\n",
            "[130,    90] loss: 1.518\n",
            "[130,   100] loss: 1.523\n",
            "[130,   110] loss: 1.532\n",
            "[130,   120] loss: 1.534\n",
            "[130,   130] loss: 1.523\n",
            "[130,   140] loss: 1.519\n",
            "[130,   150] loss: 1.531\n",
            "[130,   160] loss: 1.533\n",
            "[130,   170] loss: 1.529\n",
            "[130,   180] loss: 1.534\n",
            "[130,   190] loss: 1.528\n",
            "[130,   200] loss: 1.535\n",
            "[130,   210] loss: 1.529\n",
            "[130,   220] loss: 1.528\n",
            "[130,   230] loss: 1.534\n",
            "[130,   240] loss: 1.536\n",
            "[130,   250] loss: 1.531\n",
            "[130,   260] loss: 1.532\n",
            "[130,   270] loss: 1.533\n",
            "[130,   280] loss: 1.532\n",
            "[130,   290] loss: 1.534\n",
            "[130,   300] loss: 1.528\n",
            "[130,   310] loss: 1.539\n",
            "[130,   320] loss: 1.527\n",
            "[130,   330] loss: 1.533\n",
            "[130,   340] loss: 1.532\n",
            "[130,   350] loss: 1.529\n",
            "epoch 129 val_loss 63.98331820964813 val_steps 40 val_acc 0.867\n",
            "[131,    10] loss: 1.531\n",
            "[131,    20] loss: 1.525\n",
            "[131,    30] loss: 1.521\n",
            "[131,    40] loss: 1.541\n",
            "[131,    50] loss: 1.522\n",
            "[131,    60] loss: 1.530\n",
            "[131,    70] loss: 1.539\n",
            "[131,    80] loss: 1.540\n",
            "[131,    90] loss: 1.535\n",
            "[131,   100] loss: 1.529\n",
            "[131,   110] loss: 1.524\n",
            "[131,   120] loss: 1.527\n",
            "[131,   130] loss: 1.527\n",
            "[131,   140] loss: 1.526\n",
            "[131,   150] loss: 1.533\n",
            "[131,   160] loss: 1.538\n",
            "[131,   170] loss: 1.527\n",
            "[131,   180] loss: 1.539\n",
            "[131,   190] loss: 1.528\n",
            "[131,   200] loss: 1.530\n",
            "[131,   210] loss: 1.523\n",
            "[131,   220] loss: 1.533\n",
            "[131,   230] loss: 1.524\n",
            "[131,   240] loss: 1.526\n",
            "[131,   250] loss: 1.525\n",
            "[131,   260] loss: 1.534\n",
            "[131,   270] loss: 1.528\n",
            "[131,   280] loss: 1.533\n",
            "[131,   290] loss: 1.536\n",
            "[131,   300] loss: 1.528\n",
            "[131,   310] loss: 1.523\n",
            "[131,   320] loss: 1.523\n",
            "[131,   330] loss: 1.525\n",
            "[131,   340] loss: 1.528\n",
            "[131,   350] loss: 1.517\n",
            "epoch 130 val_loss 63.80004024505615 val_steps 40 val_acc 0.8638\n",
            "[132,    10] loss: 1.525\n",
            "[132,    20] loss: 1.532\n",
            "[132,    30] loss: 1.534\n",
            "[132,    40] loss: 1.528\n",
            "[132,    50] loss: 1.527\n",
            "[132,    60] loss: 1.522\n",
            "[132,    70] loss: 1.537\n",
            "[132,    80] loss: 1.525\n",
            "[132,    90] loss: 1.528\n",
            "[132,   100] loss: 1.526\n",
            "[132,   110] loss: 1.528\n",
            "[132,   120] loss: 1.520\n",
            "[132,   130] loss: 1.516\n",
            "[132,   140] loss: 1.524\n",
            "[132,   150] loss: 1.532\n",
            "[132,   160] loss: 1.531\n",
            "[132,   170] loss: 1.532\n",
            "[132,   180] loss: 1.530\n",
            "[132,   190] loss: 1.531\n",
            "[132,   200] loss: 1.526\n",
            "[132,   210] loss: 1.528\n",
            "[132,   220] loss: 1.526\n",
            "[132,   230] loss: 1.535\n",
            "[132,   240] loss: 1.539\n",
            "[132,   250] loss: 1.535\n",
            "[132,   260] loss: 1.536\n",
            "[132,   270] loss: 1.536\n",
            "[132,   280] loss: 1.529\n",
            "[132,   290] loss: 1.532\n",
            "[132,   300] loss: 1.519\n",
            "[132,   310] loss: 1.534\n",
            "[132,   320] loss: 1.532\n",
            "[132,   330] loss: 1.528\n",
            "[132,   340] loss: 1.524\n",
            "[132,   350] loss: 1.530\n",
            "epoch 131 val_loss 63.95709979534149 val_steps 40 val_acc 0.8638\n",
            "[133,    10] loss: 1.527\n",
            "[133,    20] loss: 1.518\n",
            "[133,    30] loss: 1.526\n",
            "[133,    40] loss: 1.529\n",
            "[133,    50] loss: 1.535\n",
            "[133,    60] loss: 1.518\n",
            "[133,    70] loss: 1.523\n",
            "[133,    80] loss: 1.520\n",
            "[133,    90] loss: 1.523\n",
            "[133,   100] loss: 1.528\n",
            "[133,   110] loss: 1.518\n",
            "[133,   120] loss: 1.520\n",
            "[133,   130] loss: 1.530\n",
            "[133,   140] loss: 1.518\n",
            "[133,   150] loss: 1.544\n",
            "[133,   160] loss: 1.524\n",
            "[133,   170] loss: 1.522\n",
            "[133,   180] loss: 1.532\n",
            "[133,   190] loss: 1.521\n",
            "[133,   200] loss: 1.526\n",
            "[133,   210] loss: 1.534\n",
            "[133,   220] loss: 1.534\n",
            "[133,   230] loss: 1.524\n",
            "[133,   240] loss: 1.539\n",
            "[133,   250] loss: 1.530\n",
            "[133,   260] loss: 1.534\n",
            "[133,   270] loss: 1.533\n",
            "[133,   280] loss: 1.515\n",
            "[133,   290] loss: 1.530\n",
            "[133,   300] loss: 1.522\n",
            "[133,   310] loss: 1.543\n",
            "[133,   320] loss: 1.533\n",
            "[133,   330] loss: 1.533\n",
            "[133,   340] loss: 1.532\n",
            "[133,   350] loss: 1.524\n",
            "epoch 132 val_loss 63.91846179962158 val_steps 40 val_acc 0.867\n",
            "[134,    10] loss: 1.526\n",
            "[134,    20] loss: 1.525\n",
            "[134,    30] loss: 1.521\n",
            "[134,    40] loss: 1.535\n",
            "[134,    50] loss: 1.516\n",
            "[134,    60] loss: 1.531\n",
            "[134,    70] loss: 1.537\n",
            "[134,    80] loss: 1.522\n",
            "[134,    90] loss: 1.524\n",
            "[134,   100] loss: 1.525\n",
            "[134,   110] loss: 1.535\n",
            "[134,   120] loss: 1.513\n",
            "[134,   130] loss: 1.524\n",
            "[134,   140] loss: 1.523\n",
            "[134,   150] loss: 1.526\n",
            "[134,   160] loss: 1.524\n",
            "[134,   170] loss: 1.524\n",
            "[134,   180] loss: 1.526\n",
            "[134,   190] loss: 1.528\n",
            "[134,   200] loss: 1.519\n",
            "[134,   210] loss: 1.530\n",
            "[134,   220] loss: 1.538\n",
            "[134,   230] loss: 1.538\n",
            "[134,   240] loss: 1.527\n",
            "[134,   250] loss: 1.526\n",
            "[134,   260] loss: 1.541\n",
            "[134,   270] loss: 1.536\n",
            "[134,   280] loss: 1.540\n",
            "[134,   290] loss: 1.527\n",
            "[134,   300] loss: 1.538\n",
            "[134,   310] loss: 1.530\n",
            "[134,   320] loss: 1.537\n",
            "[134,   330] loss: 1.521\n",
            "[134,   340] loss: 1.525\n",
            "[134,   350] loss: 1.528\n",
            "epoch 133 val_loss 64.01470494270325 val_steps 40 val_acc 0.861\n",
            "[135,    10] loss: 1.527\n",
            "[135,    20] loss: 1.525\n",
            "[135,    30] loss: 1.520\n",
            "[135,    40] loss: 1.530\n",
            "[135,    50] loss: 1.521\n",
            "[135,    60] loss: 1.523\n",
            "[135,    70] loss: 1.525\n",
            "[135,    80] loss: 1.525\n",
            "[135,    90] loss: 1.524\n",
            "[135,   100] loss: 1.525\n",
            "[135,   110] loss: 1.520\n",
            "[135,   120] loss: 1.545\n",
            "[135,   130] loss: 1.529\n",
            "[135,   140] loss: 1.521\n",
            "[135,   150] loss: 1.531\n",
            "[135,   160] loss: 1.525\n",
            "[135,   170] loss: 1.522\n",
            "[135,   180] loss: 1.525\n",
            "[135,   190] loss: 1.516\n",
            "[135,   200] loss: 1.524\n",
            "[135,   210] loss: 1.529\n",
            "[135,   220] loss: 1.525\n",
            "[135,   230] loss: 1.525\n",
            "[135,   240] loss: 1.527\n",
            "[135,   250] loss: 1.527\n",
            "[135,   260] loss: 1.529\n",
            "[135,   270] loss: 1.519\n",
            "[135,   280] loss: 1.518\n",
            "[135,   290] loss: 1.524\n",
            "[135,   300] loss: 1.542\n",
            "[135,   310] loss: 1.526\n",
            "[135,   320] loss: 1.528\n",
            "[135,   330] loss: 1.536\n",
            "[135,   340] loss: 1.536\n",
            "[135,   350] loss: 1.544\n",
            "epoch 134 val_loss 64.04715323448181 val_steps 40 val_acc 0.8656\n",
            "[136,    10] loss: 1.526\n",
            "[136,    20] loss: 1.526\n",
            "[136,    30] loss: 1.534\n",
            "[136,    40] loss: 1.519\n",
            "[136,    50] loss: 1.517\n",
            "[136,    60] loss: 1.529\n",
            "[136,    70] loss: 1.518\n",
            "[136,    80] loss: 1.525\n",
            "[136,    90] loss: 1.534\n",
            "[136,   100] loss: 1.535\n",
            "[136,   110] loss: 1.514\n",
            "[136,   120] loss: 1.523\n",
            "[136,   130] loss: 1.529\n",
            "[136,   140] loss: 1.532\n",
            "[136,   150] loss: 1.523\n",
            "[136,   160] loss: 1.525\n",
            "[136,   170] loss: 1.527\n",
            "[136,   180] loss: 1.526\n",
            "[136,   190] loss: 1.535\n",
            "[136,   200] loss: 1.535\n",
            "[136,   210] loss: 1.522\n",
            "[136,   220] loss: 1.531\n",
            "[136,   230] loss: 1.528\n",
            "[136,   240] loss: 1.527\n",
            "[136,   250] loss: 1.524\n",
            "[136,   260] loss: 1.519\n",
            "[136,   270] loss: 1.516\n",
            "[136,   280] loss: 1.526\n",
            "[136,   290] loss: 1.520\n",
            "[136,   300] loss: 1.527\n",
            "[136,   310] loss: 1.531\n",
            "[136,   320] loss: 1.528\n",
            "[136,   330] loss: 1.526\n",
            "[136,   340] loss: 1.531\n",
            "[136,   350] loss: 1.530\n",
            "epoch 135 val_loss 63.98388051986694 val_steps 40 val_acc 0.8618\n",
            "[137,    10] loss: 1.536\n",
            "[137,    20] loss: 1.532\n",
            "[137,    30] loss: 1.521\n",
            "[137,    40] loss: 1.526\n",
            "[137,    50] loss: 1.526\n",
            "[137,    60] loss: 1.522\n",
            "[137,    70] loss: 1.520\n",
            "[137,    80] loss: 1.523\n",
            "[137,    90] loss: 1.530\n",
            "[137,   100] loss: 1.527\n",
            "[137,   110] loss: 1.523\n",
            "[137,   120] loss: 1.537\n",
            "[137,   130] loss: 1.536\n",
            "[137,   140] loss: 1.521\n",
            "[137,   150] loss: 1.531\n",
            "[137,   160] loss: 1.528\n",
            "[137,   170] loss: 1.530\n",
            "[137,   180] loss: 1.531\n",
            "[137,   190] loss: 1.523\n",
            "[137,   200] loss: 1.518\n",
            "[137,   210] loss: 1.517\n",
            "[137,   220] loss: 1.514\n",
            "[137,   230] loss: 1.527\n",
            "[137,   240] loss: 1.530\n",
            "[137,   250] loss: 1.530\n",
            "[137,   260] loss: 1.522\n",
            "[137,   270] loss: 1.520\n",
            "[137,   280] loss: 1.527\n",
            "[137,   290] loss: 1.533\n",
            "[137,   300] loss: 1.520\n",
            "[137,   310] loss: 1.524\n",
            "[137,   320] loss: 1.519\n",
            "[137,   330] loss: 1.518\n",
            "[137,   340] loss: 1.532\n",
            "[137,   350] loss: 1.515\n",
            "epoch 136 val_loss 64.13784325122833 val_steps 40 val_acc 0.8662\n",
            "[138,    10] loss: 1.518\n",
            "[138,    20] loss: 1.524\n",
            "[138,    30] loss: 1.535\n",
            "[138,    40] loss: 1.521\n",
            "[138,    50] loss: 1.514\n",
            "[138,    60] loss: 1.532\n",
            "[138,    70] loss: 1.518\n",
            "[138,    80] loss: 1.520\n",
            "[138,    90] loss: 1.529\n",
            "[138,   100] loss: 1.525\n",
            "[138,   110] loss: 1.525\n",
            "[138,   120] loss: 1.517\n",
            "[138,   130] loss: 1.530\n",
            "[138,   140] loss: 1.518\n",
            "[138,   150] loss: 1.526\n",
            "[138,   160] loss: 1.509\n",
            "[138,   170] loss: 1.531\n",
            "[138,   180] loss: 1.521\n",
            "[138,   190] loss: 1.518\n",
            "[138,   200] loss: 1.533\n",
            "[138,   210] loss: 1.520\n",
            "[138,   220] loss: 1.517\n",
            "[138,   230] loss: 1.531\n",
            "[138,   240] loss: 1.520\n",
            "[138,   250] loss: 1.526\n",
            "[138,   260] loss: 1.521\n",
            "[138,   270] loss: 1.523\n",
            "[138,   280] loss: 1.516\n",
            "[138,   290] loss: 1.518\n",
            "[138,   300] loss: 1.519\n",
            "[138,   310] loss: 1.535\n",
            "[138,   320] loss: 1.521\n",
            "[138,   330] loss: 1.519\n",
            "[138,   340] loss: 1.512\n",
            "[138,   350] loss: 1.528\n",
            "epoch 137 val_loss 63.99815011024475 val_steps 40 val_acc 0.8662\n",
            "[139,    10] loss: 1.516\n",
            "[139,    20] loss: 1.527\n",
            "[139,    30] loss: 1.523\n",
            "[139,    40] loss: 1.527\n",
            "[139,    50] loss: 1.521\n",
            "[139,    60] loss: 1.517\n",
            "[139,    70] loss: 1.519\n",
            "[139,    80] loss: 1.515\n",
            "[139,    90] loss: 1.520\n",
            "[139,   100] loss: 1.527\n",
            "[139,   110] loss: 1.518\n",
            "[139,   120] loss: 1.522\n",
            "[139,   130] loss: 1.514\n",
            "[139,   140] loss: 1.527\n",
            "[139,   150] loss: 1.514\n",
            "[139,   160] loss: 1.513\n",
            "[139,   170] loss: 1.522\n",
            "[139,   180] loss: 1.509\n",
            "[139,   190] loss: 1.528\n",
            "[139,   200] loss: 1.516\n",
            "[139,   210] loss: 1.514\n",
            "[139,   220] loss: 1.531\n",
            "[139,   230] loss: 1.521\n",
            "[139,   240] loss: 1.514\n",
            "[139,   250] loss: 1.528\n",
            "[139,   260] loss: 1.514\n",
            "[139,   270] loss: 1.518\n",
            "[139,   280] loss: 1.526\n",
            "[139,   290] loss: 1.511\n",
            "[139,   300] loss: 1.529\n",
            "[139,   310] loss: 1.524\n",
            "[139,   320] loss: 1.522\n",
            "[139,   330] loss: 1.519\n",
            "[139,   340] loss: 1.517\n",
            "[139,   350] loss: 1.518\n",
            "epoch 138 val_loss 63.93384516239166 val_steps 40 val_acc 0.8654\n",
            "[140,    10] loss: 1.529\n",
            "[140,    20] loss: 1.515\n",
            "[140,    30] loss: 1.522\n",
            "[140,    40] loss: 1.510\n",
            "[140,    50] loss: 1.515\n",
            "[140,    60] loss: 1.527\n",
            "[140,    70] loss: 1.516\n",
            "[140,    80] loss: 1.534\n",
            "[140,    90] loss: 1.517\n",
            "[140,   100] loss: 1.521\n",
            "[140,   110] loss: 1.530\n",
            "[140,   120] loss: 1.514\n",
            "[140,   130] loss: 1.519\n",
            "[140,   140] loss: 1.529\n",
            "[140,   150] loss: 1.531\n",
            "[140,   160] loss: 1.518\n",
            "[140,   170] loss: 1.523\n",
            "[140,   180] loss: 1.534\n",
            "[140,   190] loss: 1.510\n",
            "[140,   200] loss: 1.513\n",
            "[140,   210] loss: 1.509\n",
            "[140,   220] loss: 1.528\n",
            "[140,   230] loss: 1.516\n",
            "[140,   240] loss: 1.505\n",
            "[140,   250] loss: 1.518\n",
            "[140,   260] loss: 1.511\n",
            "[140,   270] loss: 1.529\n",
            "[140,   280] loss: 1.526\n",
            "[140,   290] loss: 1.523\n",
            "[140,   300] loss: 1.525\n",
            "[140,   310] loss: 1.528\n",
            "[140,   320] loss: 1.515\n",
            "[140,   330] loss: 1.514\n",
            "[140,   340] loss: 1.521\n",
            "[140,   350] loss: 1.516\n",
            "epoch 139 val_loss 64.00548958778381 val_steps 40 val_acc 0.868\n",
            "[141,    10] loss: 1.516\n",
            "[141,    20] loss: 1.523\n",
            "[141,    30] loss: 1.514\n",
            "[141,    40] loss: 1.529\n",
            "[141,    50] loss: 1.513\n",
            "[141,    60] loss: 1.516\n",
            "[141,    70] loss: 1.529\n",
            "[141,    80] loss: 1.520\n",
            "[141,    90] loss: 1.518\n",
            "[141,   100] loss: 1.515\n",
            "[141,   110] loss: 1.525\n",
            "[141,   120] loss: 1.518\n",
            "[141,   130] loss: 1.510\n",
            "[141,   140] loss: 1.522\n",
            "[141,   150] loss: 1.505\n",
            "[141,   160] loss: 1.520\n",
            "[141,   170] loss: 1.515\n",
            "[141,   180] loss: 1.519\n",
            "[141,   190] loss: 1.518\n",
            "[141,   200] loss: 1.528\n",
            "[141,   210] loss: 1.530\n",
            "[141,   220] loss: 1.522\n",
            "[141,   230] loss: 1.522\n",
            "[141,   240] loss: 1.535\n",
            "[141,   250] loss: 1.527\n",
            "[141,   260] loss: 1.521\n",
            "[141,   270] loss: 1.522\n",
            "[141,   280] loss: 1.521\n",
            "[141,   290] loss: 1.518\n",
            "[141,   300] loss: 1.520\n",
            "[141,   310] loss: 1.520\n",
            "[141,   320] loss: 1.521\n",
            "[141,   330] loss: 1.509\n",
            "[141,   340] loss: 1.517\n",
            "[141,   350] loss: 1.517\n",
            "epoch 140 val_loss 64.19639194011688 val_steps 40 val_acc 0.865\n",
            "[142,    10] loss: 1.529\n",
            "[142,    20] loss: 1.514\n",
            "[142,    30] loss: 1.535\n",
            "[142,    40] loss: 1.512\n",
            "[142,    50] loss: 1.516\n",
            "[142,    60] loss: 1.516\n",
            "[142,    70] loss: 1.529\n",
            "[142,    80] loss: 1.516\n",
            "[142,    90] loss: 1.518\n",
            "[142,   100] loss: 1.521\n",
            "[142,   110] loss: 1.515\n",
            "[142,   120] loss: 1.517\n",
            "[142,   130] loss: 1.518\n",
            "[142,   140] loss: 1.520\n",
            "[142,   150] loss: 1.520\n",
            "[142,   160] loss: 1.520\n",
            "[142,   170] loss: 1.523\n",
            "[142,   180] loss: 1.521\n",
            "[142,   190] loss: 1.515\n",
            "[142,   200] loss: 1.518\n",
            "[142,   210] loss: 1.519\n",
            "[142,   220] loss: 1.520\n",
            "[142,   230] loss: 1.517\n",
            "[142,   240] loss: 1.515\n",
            "[142,   250] loss: 1.515\n",
            "[142,   260] loss: 1.521\n",
            "[142,   270] loss: 1.516\n",
            "[142,   280] loss: 1.523\n",
            "[142,   290] loss: 1.521\n",
            "[142,   300] loss: 1.519\n",
            "[142,   310] loss: 1.514\n",
            "[142,   320] loss: 1.525\n",
            "[142,   330] loss: 1.520\n",
            "[142,   340] loss: 1.516\n",
            "[142,   350] loss: 1.519\n",
            "epoch 141 val_loss 63.8824200630188 val_steps 40 val_acc 0.8674\n",
            "[143,    10] loss: 1.536\n",
            "[143,    20] loss: 1.519\n",
            "[143,    30] loss: 1.522\n",
            "[143,    40] loss: 1.529\n",
            "[143,    50] loss: 1.534\n",
            "[143,    60] loss: 1.527\n",
            "[143,    70] loss: 1.511\n",
            "[143,    80] loss: 1.499\n",
            "[143,    90] loss: 1.515\n",
            "[143,   100] loss: 1.518\n",
            "[143,   110] loss: 1.516\n",
            "[143,   120] loss: 1.511\n",
            "[143,   130] loss: 1.519\n",
            "[143,   140] loss: 1.513\n",
            "[143,   150] loss: 1.521\n",
            "[143,   160] loss: 1.518\n",
            "[143,   170] loss: 1.513\n",
            "[143,   180] loss: 1.517\n",
            "[143,   190] loss: 1.519\n",
            "[143,   200] loss: 1.523\n",
            "[143,   210] loss: 1.518\n",
            "[143,   220] loss: 1.521\n",
            "[143,   230] loss: 1.512\n",
            "[143,   240] loss: 1.521\n",
            "[143,   250] loss: 1.529\n",
            "[143,   260] loss: 1.514\n",
            "[143,   270] loss: 1.515\n",
            "[143,   280] loss: 1.520\n",
            "[143,   290] loss: 1.515\n",
            "[143,   300] loss: 1.517\n",
            "[143,   310] loss: 1.519\n",
            "[143,   320] loss: 1.522\n",
            "[143,   330] loss: 1.524\n",
            "[143,   340] loss: 1.517\n",
            "[143,   350] loss: 1.518\n",
            "epoch 142 val_loss 64.11471927165985 val_steps 40 val_acc 0.8664\n",
            "[144,    10] loss: 1.526\n",
            "[144,    20] loss: 1.512\n",
            "[144,    30] loss: 1.528\n",
            "[144,    40] loss: 1.530\n",
            "[144,    50] loss: 1.516\n",
            "[144,    60] loss: 1.509\n",
            "[144,    70] loss: 1.524\n",
            "[144,    80] loss: 1.522\n",
            "[144,    90] loss: 1.519\n",
            "[144,   100] loss: 1.524\n",
            "[144,   110] loss: 1.510\n",
            "[144,   120] loss: 1.524\n",
            "[144,   130] loss: 1.522\n",
            "[144,   140] loss: 1.519\n",
            "[144,   150] loss: 1.530\n",
            "[144,   160] loss: 1.526\n",
            "[144,   170] loss: 1.520\n",
            "[144,   180] loss: 1.518\n",
            "[144,   190] loss: 1.515\n",
            "[144,   200] loss: 1.514\n",
            "[144,   210] loss: 1.513\n",
            "[144,   220] loss: 1.523\n",
            "[144,   230] loss: 1.518\n",
            "[144,   240] loss: 1.517\n",
            "[144,   250] loss: 1.523\n",
            "[144,   260] loss: 1.523\n",
            "[144,   270] loss: 1.525\n",
            "[144,   280] loss: 1.512\n",
            "[144,   290] loss: 1.514\n",
            "[144,   300] loss: 1.517\n",
            "[144,   310] loss: 1.521\n",
            "[144,   320] loss: 1.523\n",
            "[144,   330] loss: 1.510\n",
            "[144,   340] loss: 1.520\n",
            "[144,   350] loss: 1.516\n",
            "epoch 143 val_loss 63.98522889614105 val_steps 40 val_acc 0.8682\n",
            "[145,    10] loss: 1.518\n",
            "[145,    20] loss: 1.529\n",
            "[145,    30] loss: 1.518\n",
            "[145,    40] loss: 1.514\n",
            "[145,    50] loss: 1.527\n",
            "[145,    60] loss: 1.519\n",
            "[145,    70] loss: 1.521\n",
            "[145,    80] loss: 1.524\n",
            "[145,    90] loss: 1.511\n",
            "[145,   100] loss: 1.529\n",
            "[145,   110] loss: 1.521\n",
            "[145,   120] loss: 1.523\n",
            "[145,   130] loss: 1.526\n",
            "[145,   140] loss: 1.518\n",
            "[145,   150] loss: 1.512\n",
            "[145,   160] loss: 1.523\n",
            "[145,   170] loss: 1.517\n",
            "[145,   180] loss: 1.509\n",
            "[145,   190] loss: 1.518\n",
            "[145,   200] loss: 1.516\n",
            "[145,   210] loss: 1.520\n",
            "[145,   220] loss: 1.511\n",
            "[145,   230] loss: 1.516\n",
            "[145,   240] loss: 1.522\n",
            "[145,   250] loss: 1.514\n",
            "[145,   260] loss: 1.531\n",
            "[145,   270] loss: 1.511\n",
            "[145,   280] loss: 1.521\n",
            "[145,   290] loss: 1.511\n",
            "[145,   300] loss: 1.519\n",
            "[145,   310] loss: 1.524\n",
            "[145,   320] loss: 1.515\n",
            "[145,   330] loss: 1.532\n",
            "[145,   340] loss: 1.528\n",
            "[145,   350] loss: 1.509\n",
            "epoch 144 val_loss 63.862610936164856 val_steps 40 val_acc 0.8696\n",
            "[146,    10] loss: 1.523\n",
            "[146,    20] loss: 1.515\n",
            "[146,    30] loss: 1.523\n",
            "[146,    40] loss: 1.523\n",
            "[146,    50] loss: 1.524\n",
            "[146,    60] loss: 1.519\n",
            "[146,    70] loss: 1.523\n",
            "[146,    80] loss: 1.522\n",
            "[146,    90] loss: 1.516\n",
            "[146,   100] loss: 1.521\n",
            "[146,   110] loss: 1.513\n",
            "[146,   120] loss: 1.515\n",
            "[146,   130] loss: 1.516\n",
            "[146,   140] loss: 1.525\n",
            "[146,   150] loss: 1.518\n",
            "[146,   160] loss: 1.521\n",
            "[146,   170] loss: 1.521\n",
            "[146,   180] loss: 1.518\n",
            "[146,   190] loss: 1.519\n",
            "[146,   200] loss: 1.515\n",
            "[146,   210] loss: 1.512\n",
            "[146,   220] loss: 1.513\n",
            "[146,   230] loss: 1.519\n",
            "[146,   240] loss: 1.518\n",
            "[146,   250] loss: 1.523\n",
            "[146,   260] loss: 1.513\n",
            "[146,   270] loss: 1.520\n",
            "[146,   280] loss: 1.511\n",
            "[146,   290] loss: 1.519\n",
            "[146,   300] loss: 1.515\n",
            "[146,   310] loss: 1.514\n",
            "[146,   320] loss: 1.511\n",
            "[146,   330] loss: 1.525\n",
            "[146,   340] loss: 1.514\n",
            "[146,   350] loss: 1.519\n",
            "epoch 145 val_loss 64.11613011360168 val_steps 40 val_acc 0.8642\n",
            "[147,    10] loss: 1.526\n",
            "[147,    20] loss: 1.507\n",
            "[147,    30] loss: 1.526\n",
            "[147,    40] loss: 1.523\n",
            "[147,    50] loss: 1.514\n",
            "[147,    60] loss: 1.515\n",
            "[147,    70] loss: 1.515\n",
            "[147,    80] loss: 1.513\n",
            "[147,    90] loss: 1.515\n",
            "[147,   100] loss: 1.517\n",
            "[147,   110] loss: 1.531\n",
            "[147,   120] loss: 1.515\n",
            "[147,   130] loss: 1.510\n",
            "[147,   140] loss: 1.521\n",
            "[147,   150] loss: 1.517\n",
            "[147,   160] loss: 1.514\n",
            "[147,   170] loss: 1.516\n",
            "[147,   180] loss: 1.524\n",
            "[147,   190] loss: 1.519\n",
            "[147,   200] loss: 1.515\n",
            "[147,   210] loss: 1.528\n",
            "[147,   220] loss: 1.519\n",
            "[147,   230] loss: 1.526\n",
            "[147,   240] loss: 1.513\n",
            "[147,   250] loss: 1.516\n",
            "[147,   260] loss: 1.504\n",
            "[147,   270] loss: 1.514\n",
            "[147,   280] loss: 1.511\n",
            "[147,   290] loss: 1.521\n",
            "[147,   300] loss: 1.521\n",
            "[147,   310] loss: 1.522\n",
            "[147,   320] loss: 1.521\n",
            "[147,   330] loss: 1.521\n",
            "[147,   340] loss: 1.528\n",
            "[147,   350] loss: 1.515\n",
            "epoch 146 val_loss 64.02717626094818 val_steps 40 val_acc 0.8656\n",
            "[148,    10] loss: 1.520\n",
            "[148,    20] loss: 1.515\n",
            "[148,    30] loss: 1.512\n",
            "[148,    40] loss: 1.514\n",
            "[148,    50] loss: 1.519\n",
            "[148,    60] loss: 1.521\n",
            "[148,    70] loss: 1.522\n",
            "[148,    80] loss: 1.522\n",
            "[148,    90] loss: 1.513\n",
            "[148,   100] loss: 1.521\n",
            "[148,   110] loss: 1.518\n",
            "[148,   120] loss: 1.523\n",
            "[148,   130] loss: 1.528\n",
            "[148,   140] loss: 1.512\n",
            "[148,   150] loss: 1.513\n",
            "[148,   160] loss: 1.527\n",
            "[148,   170] loss: 1.520\n",
            "[148,   180] loss: 1.509\n",
            "[148,   190] loss: 1.518\n",
            "[148,   200] loss: 1.513\n",
            "[148,   210] loss: 1.516\n",
            "[148,   220] loss: 1.527\n",
            "[148,   230] loss: 1.512\n",
            "[148,   240] loss: 1.513\n",
            "[148,   250] loss: 1.517\n",
            "[148,   260] loss: 1.516\n",
            "[148,   270] loss: 1.523\n",
            "[148,   280] loss: 1.517\n",
            "[148,   290] loss: 1.509\n",
            "[148,   300] loss: 1.512\n",
            "[148,   310] loss: 1.520\n",
            "[148,   320] loss: 1.523\n",
            "[148,   330] loss: 1.523\n",
            "[148,   340] loss: 1.520\n",
            "[148,   350] loss: 1.507\n",
            "epoch 147 val_loss 63.86820638179779 val_steps 40 val_acc 0.8674\n",
            "[149,    10] loss: 1.518\n",
            "[149,    20] loss: 1.531\n",
            "[149,    30] loss: 1.518\n",
            "[149,    40] loss: 1.519\n",
            "[149,    50] loss: 1.514\n",
            "[149,    60] loss: 1.508\n",
            "[149,    70] loss: 1.519\n",
            "[149,    80] loss: 1.525\n",
            "[149,    90] loss: 1.523\n",
            "[149,   100] loss: 1.528\n",
            "[149,   110] loss: 1.507\n",
            "[149,   120] loss: 1.513\n",
            "[149,   130] loss: 1.523\n",
            "[149,   140] loss: 1.512\n",
            "[149,   150] loss: 1.513\n",
            "[149,   160] loss: 1.510\n",
            "[149,   170] loss: 1.522\n",
            "[149,   180] loss: 1.519\n",
            "[149,   190] loss: 1.520\n",
            "[149,   200] loss: 1.520\n",
            "[149,   210] loss: 1.515\n",
            "[149,   220] loss: 1.523\n",
            "[149,   230] loss: 1.514\n",
            "[149,   240] loss: 1.522\n",
            "[149,   250] loss: 1.524\n",
            "[149,   260] loss: 1.524\n",
            "[149,   270] loss: 1.518\n",
            "[149,   280] loss: 1.527\n",
            "[149,   290] loss: 1.516\n",
            "[149,   300] loss: 1.522\n",
            "[149,   310] loss: 1.518\n",
            "[149,   320] loss: 1.510\n",
            "[149,   330] loss: 1.512\n",
            "[149,   340] loss: 1.522\n",
            "[149,   350] loss: 1.512\n",
            "epoch 148 val_loss 63.74357831478119 val_steps 40 val_acc 0.865\n",
            "[150,    10] loss: 1.527\n",
            "[150,    20] loss: 1.520\n",
            "[150,    30] loss: 1.519\n",
            "[150,    40] loss: 1.523\n",
            "[150,    50] loss: 1.534\n",
            "[150,    60] loss: 1.516\n",
            "[150,    70] loss: 1.507\n",
            "[150,    80] loss: 1.528\n",
            "[150,    90] loss: 1.521\n",
            "[150,   100] loss: 1.510\n",
            "[150,   110] loss: 1.511\n",
            "[150,   120] loss: 1.521\n",
            "[150,   130] loss: 1.511\n",
            "[150,   140] loss: 1.517\n",
            "[150,   150] loss: 1.521\n",
            "[150,   160] loss: 1.514\n",
            "[150,   170] loss: 1.518\n",
            "[150,   180] loss: 1.513\n",
            "[150,   190] loss: 1.516\n",
            "[150,   200] loss: 1.520\n",
            "[150,   210] loss: 1.526\n",
            "[150,   220] loss: 1.525\n",
            "[150,   230] loss: 1.516\n",
            "[150,   240] loss: 1.529\n",
            "[150,   250] loss: 1.515\n",
            "[150,   260] loss: 1.519\n",
            "[150,   270] loss: 1.507\n",
            "[150,   280] loss: 1.517\n",
            "[150,   290] loss: 1.517\n",
            "[150,   300] loss: 1.517\n",
            "[150,   310] loss: 1.507\n",
            "[150,   320] loss: 1.515\n",
            "[150,   330] loss: 1.513\n",
            "[150,   340] loss: 1.522\n",
            "[150,   350] loss: 1.511\n",
            "epoch 149 val_loss 63.909940242767334 val_steps 40 val_acc 0.8682\n",
            "[151,    10] loss: 1.509\n",
            "[151,    20] loss: 1.521\n",
            "[151,    30] loss: 1.517\n",
            "[151,    40] loss: 1.515\n",
            "[151,    50] loss: 1.527\n",
            "[151,    60] loss: 1.509\n",
            "[151,    70] loss: 1.522\n",
            "[151,    80] loss: 1.526\n",
            "[151,    90] loss: 1.527\n",
            "[151,   100] loss: 1.514\n",
            "[151,   110] loss: 1.513\n",
            "[151,   120] loss: 1.530\n",
            "[151,   130] loss: 1.517\n",
            "[151,   140] loss: 1.517\n",
            "[151,   150] loss: 1.526\n",
            "[151,   160] loss: 1.521\n",
            "[151,   170] loss: 1.514\n",
            "[151,   180] loss: 1.524\n",
            "[151,   190] loss: 1.515\n",
            "[151,   200] loss: 1.524\n",
            "[151,   210] loss: 1.513\n",
            "[151,   220] loss: 1.517\n",
            "[151,   230] loss: 1.525\n",
            "[151,   240] loss: 1.522\n",
            "[151,   250] loss: 1.522\n",
            "[151,   260] loss: 1.528\n",
            "[151,   270] loss: 1.518\n",
            "[151,   280] loss: 1.512\n",
            "[151,   290] loss: 1.519\n",
            "[151,   300] loss: 1.509\n",
            "[151,   310] loss: 1.518\n",
            "[151,   320] loss: 1.518\n",
            "[151,   330] loss: 1.505\n",
            "[151,   340] loss: 1.526\n",
            "[151,   350] loss: 1.510\n",
            "epoch 150 val_loss 63.94091796875 val_steps 40 val_acc 0.8646\n",
            "[152,    10] loss: 1.521\n",
            "[152,    20] loss: 1.528\n",
            "[152,    30] loss: 1.512\n",
            "[152,    40] loss: 1.521\n",
            "[152,    50] loss: 1.513\n",
            "[152,    60] loss: 1.520\n",
            "[152,    70] loss: 1.514\n",
            "[152,    80] loss: 1.520\n",
            "[152,    90] loss: 1.509\n",
            "[152,   100] loss: 1.521\n",
            "[152,   110] loss: 1.511\n",
            "[152,   120] loss: 1.515\n",
            "[152,   130] loss: 1.508\n",
            "[152,   140] loss: 1.520\n",
            "[152,   150] loss: 1.512\n",
            "[152,   160] loss: 1.516\n",
            "[152,   170] loss: 1.518\n",
            "[152,   180] loss: 1.510\n",
            "[152,   190] loss: 1.515\n",
            "[152,   200] loss: 1.523\n",
            "[152,   210] loss: 1.516\n",
            "[152,   220] loss: 1.519\n",
            "[152,   230] loss: 1.518\n",
            "[152,   240] loss: 1.516\n",
            "[152,   250] loss: 1.515\n",
            "[152,   260] loss: 1.524\n",
            "[152,   270] loss: 1.515\n",
            "[152,   280] loss: 1.518\n",
            "[152,   290] loss: 1.516\n",
            "[152,   300] loss: 1.514\n",
            "[152,   310] loss: 1.518\n",
            "[152,   320] loss: 1.506\n",
            "[152,   330] loss: 1.520\n",
            "[152,   340] loss: 1.520\n",
            "[152,   350] loss: 1.513\n",
            "epoch 151 val_loss 63.93498969078064 val_steps 40 val_acc 0.8668\n",
            "[153,    10] loss: 1.512\n",
            "[153,    20] loss: 1.519\n",
            "[153,    30] loss: 1.518\n",
            "[153,    40] loss: 1.524\n",
            "[153,    50] loss: 1.513\n",
            "[153,    60] loss: 1.508\n",
            "[153,    70] loss: 1.518\n",
            "[153,    80] loss: 1.518\n",
            "[153,    90] loss: 1.519\n",
            "[153,   100] loss: 1.515\n",
            "[153,   110] loss: 1.513\n",
            "[153,   120] loss: 1.503\n",
            "[153,   130] loss: 1.520\n",
            "[153,   140] loss: 1.526\n",
            "[153,   150] loss: 1.515\n",
            "[153,   160] loss: 1.513\n",
            "[153,   170] loss: 1.524\n",
            "[153,   180] loss: 1.520\n",
            "[153,   190] loss: 1.520\n",
            "[153,   200] loss: 1.517\n",
            "[153,   210] loss: 1.518\n",
            "[153,   220] loss: 1.521\n",
            "[153,   230] loss: 1.526\n",
            "[153,   240] loss: 1.526\n",
            "[153,   250] loss: 1.514\n",
            "[153,   260] loss: 1.514\n",
            "[153,   270] loss: 1.528\n",
            "[153,   280] loss: 1.510\n",
            "[153,   290] loss: 1.512\n",
            "[153,   300] loss: 1.523\n",
            "[153,   310] loss: 1.519\n",
            "[153,   320] loss: 1.518\n",
            "[153,   330] loss: 1.518\n",
            "[153,   340] loss: 1.516\n",
            "[153,   350] loss: 1.508\n",
            "epoch 152 val_loss 63.71215224266052 val_steps 40 val_acc 0.868\n",
            "[154,    10] loss: 1.517\n",
            "[154,    20] loss: 1.527\n",
            "[154,    30] loss: 1.513\n",
            "[154,    40] loss: 1.516\n",
            "[154,    50] loss: 1.520\n",
            "[154,    60] loss: 1.515\n",
            "[154,    70] loss: 1.519\n",
            "[154,    80] loss: 1.513\n",
            "[154,    90] loss: 1.517\n",
            "[154,   100] loss: 1.521\n",
            "[154,   110] loss: 1.515\n",
            "[154,   120] loss: 1.514\n",
            "[154,   130] loss: 1.521\n",
            "[154,   140] loss: 1.512\n",
            "[154,   150] loss: 1.508\n",
            "[154,   160] loss: 1.519\n",
            "[154,   170] loss: 1.516\n",
            "[154,   180] loss: 1.521\n",
            "[154,   190] loss: 1.515\n",
            "[154,   200] loss: 1.512\n",
            "[154,   210] loss: 1.520\n",
            "[154,   220] loss: 1.529\n",
            "[154,   230] loss: 1.520\n",
            "[154,   240] loss: 1.514\n",
            "[154,   250] loss: 1.518\n",
            "[154,   260] loss: 1.519\n",
            "[154,   270] loss: 1.521\n",
            "[154,   280] loss: 1.510\n",
            "[154,   290] loss: 1.530\n",
            "[154,   300] loss: 1.514\n",
            "[154,   310] loss: 1.520\n",
            "[154,   320] loss: 1.522\n",
            "[154,   330] loss: 1.516\n",
            "[154,   340] loss: 1.516\n",
            "[154,   350] loss: 1.516\n",
            "epoch 153 val_loss 64.03871726989746 val_steps 40 val_acc 0.8684\n",
            "[155,    10] loss: 1.518\n",
            "[155,    20] loss: 1.520\n",
            "[155,    30] loss: 1.511\n",
            "[155,    40] loss: 1.517\n",
            "[155,    50] loss: 1.514\n",
            "[155,    60] loss: 1.514\n",
            "[155,    70] loss: 1.521\n",
            "[155,    80] loss: 1.518\n",
            "[155,    90] loss: 1.519\n",
            "[155,   100] loss: 1.510\n",
            "[155,   110] loss: 1.518\n",
            "[155,   120] loss: 1.516\n",
            "[155,   130] loss: 1.510\n",
            "[155,   140] loss: 1.526\n",
            "[155,   150] loss: 1.510\n",
            "[155,   160] loss: 1.514\n",
            "[155,   170] loss: 1.529\n",
            "[155,   180] loss: 1.518\n",
            "[155,   190] loss: 1.515\n",
            "[155,   200] loss: 1.521\n",
            "[155,   210] loss: 1.510\n",
            "[155,   220] loss: 1.520\n",
            "[155,   230] loss: 1.515\n",
            "[155,   240] loss: 1.518\n",
            "[155,   250] loss: 1.509\n",
            "[155,   260] loss: 1.514\n",
            "[155,   270] loss: 1.522\n",
            "[155,   280] loss: 1.532\n",
            "[155,   290] loss: 1.516\n",
            "[155,   300] loss: 1.523\n",
            "[155,   310] loss: 1.518\n",
            "[155,   320] loss: 1.521\n",
            "[155,   330] loss: 1.517\n",
            "[155,   340] loss: 1.515\n",
            "[155,   350] loss: 1.525\n",
            "epoch 154 val_loss 63.974753737449646 val_steps 40 val_acc 0.8658\n",
            "[156,    10] loss: 1.520\n",
            "[156,    20] loss: 1.522\n",
            "[156,    30] loss: 1.514\n",
            "[156,    40] loss: 1.511\n",
            "[156,    50] loss: 1.508\n",
            "[156,    60] loss: 1.526\n",
            "[156,    70] loss: 1.525\n",
            "[156,    80] loss: 1.514\n",
            "[156,    90] loss: 1.517\n",
            "[156,   100] loss: 1.522\n",
            "[156,   110] loss: 1.510\n",
            "[156,   120] loss: 1.521\n",
            "[156,   130] loss: 1.515\n",
            "[156,   140] loss: 1.514\n",
            "[156,   150] loss: 1.510\n",
            "[156,   160] loss: 1.527\n",
            "[156,   170] loss: 1.520\n",
            "[156,   180] loss: 1.506\n",
            "[156,   190] loss: 1.521\n",
            "[156,   200] loss: 1.512\n",
            "[156,   210] loss: 1.520\n",
            "[156,   220] loss: 1.522\n",
            "[156,   230] loss: 1.520\n",
            "[156,   240] loss: 1.516\n",
            "[156,   250] loss: 1.518\n",
            "[156,   260] loss: 1.510\n",
            "[156,   270] loss: 1.515\n",
            "[156,   280] loss: 1.514\n",
            "[156,   290] loss: 1.514\n",
            "[156,   300] loss: 1.508\n",
            "[156,   310] loss: 1.516\n",
            "[156,   320] loss: 1.514\n",
            "[156,   330] loss: 1.509\n",
            "[156,   340] loss: 1.544\n",
            "[156,   350] loss: 1.528\n",
            "epoch 155 val_loss 64.0612291097641 val_steps 40 val_acc 0.8658\n",
            "[157,    10] loss: 1.520\n",
            "[157,    20] loss: 1.510\n",
            "[157,    30] loss: 1.523\n",
            "[157,    40] loss: 1.512\n",
            "[157,    50] loss: 1.521\n",
            "[157,    60] loss: 1.512\n",
            "[157,    70] loss: 1.517\n",
            "[157,    80] loss: 1.517\n",
            "[157,    90] loss: 1.524\n",
            "[157,   100] loss: 1.514\n",
            "[157,   110] loss: 1.518\n",
            "[157,   120] loss: 1.513\n",
            "[157,   130] loss: 1.509\n",
            "[157,   140] loss: 1.515\n",
            "[157,   150] loss: 1.520\n",
            "[157,   160] loss: 1.526\n",
            "[157,   170] loss: 1.515\n",
            "[157,   180] loss: 1.520\n",
            "[157,   190] loss: 1.518\n",
            "[157,   200] loss: 1.511\n",
            "[157,   210] loss: 1.508\n",
            "[157,   220] loss: 1.521\n",
            "[157,   230] loss: 1.522\n",
            "[157,   240] loss: 1.513\n",
            "[157,   250] loss: 1.515\n",
            "[157,   260] loss: 1.516\n",
            "[157,   270] loss: 1.520\n",
            "[157,   280] loss: 1.520\n",
            "[157,   290] loss: 1.516\n",
            "[157,   300] loss: 1.522\n",
            "[157,   310] loss: 1.514\n",
            "[157,   320] loss: 1.511\n",
            "[157,   330] loss: 1.514\n",
            "[157,   340] loss: 1.514\n",
            "[157,   350] loss: 1.534\n",
            "epoch 156 val_loss 63.9840362071991 val_steps 40 val_acc 0.8628\n",
            "[158,    10] loss: 1.510\n",
            "[158,    20] loss: 1.526\n",
            "[158,    30] loss: 1.524\n",
            "[158,    40] loss: 1.515\n",
            "[158,    50] loss: 1.524\n",
            "[158,    60] loss: 1.510\n",
            "[158,    70] loss: 1.516\n",
            "[158,    80] loss: 1.514\n",
            "[158,    90] loss: 1.512\n",
            "[158,   100] loss: 1.521\n",
            "[158,   110] loss: 1.511\n",
            "[158,   120] loss: 1.526\n",
            "[158,   130] loss: 1.515\n",
            "[158,   140] loss: 1.512\n",
            "[158,   150] loss: 1.515\n",
            "[158,   160] loss: 1.520\n",
            "[158,   170] loss: 1.511\n",
            "[158,   180] loss: 1.520\n",
            "[158,   190] loss: 1.515\n",
            "[158,   200] loss: 1.513\n",
            "[158,   210] loss: 1.514\n",
            "[158,   220] loss: 1.510\n",
            "[158,   230] loss: 1.527\n",
            "[158,   240] loss: 1.522\n",
            "[158,   250] loss: 1.516\n",
            "[158,   260] loss: 1.521\n",
            "[158,   270] loss: 1.520\n",
            "[158,   280] loss: 1.519\n",
            "[158,   290] loss: 1.519\n",
            "[158,   300] loss: 1.514\n",
            "[158,   310] loss: 1.516\n",
            "[158,   320] loss: 1.509\n",
            "[158,   330] loss: 1.510\n",
            "[158,   340] loss: 1.517\n",
            "[158,   350] loss: 1.515\n",
            "epoch 157 val_loss 64.11426901817322 val_steps 40 val_acc 0.8682\n",
            "[159,    10] loss: 1.518\n",
            "[159,    20] loss: 1.505\n",
            "[159,    30] loss: 1.509\n",
            "[159,    40] loss: 1.515\n",
            "[159,    50] loss: 1.518\n",
            "[159,    60] loss: 1.519\n",
            "[159,    70] loss: 1.530\n",
            "[159,    80] loss: 1.523\n",
            "[159,    90] loss: 1.524\n",
            "[159,   100] loss: 1.506\n",
            "[159,   110] loss: 1.514\n",
            "[159,   120] loss: 1.521\n",
            "[159,   130] loss: 1.519\n",
            "[159,   140] loss: 1.500\n",
            "[159,   150] loss: 1.519\n",
            "[159,   160] loss: 1.517\n",
            "[159,   170] loss: 1.523\n",
            "[159,   180] loss: 1.527\n",
            "[159,   190] loss: 1.520\n",
            "[159,   200] loss: 1.521\n",
            "[159,   210] loss: 1.516\n",
            "[159,   220] loss: 1.513\n",
            "[159,   230] loss: 1.504\n",
            "[159,   240] loss: 1.526\n",
            "[159,   250] loss: 1.518\n",
            "[159,   260] loss: 1.513\n",
            "[159,   270] loss: 1.518\n",
            "[159,   280] loss: 1.517\n",
            "[159,   290] loss: 1.514\n",
            "[159,   300] loss: 1.520\n",
            "[159,   310] loss: 1.519\n",
            "[159,   320] loss: 1.512\n",
            "[159,   330] loss: 1.521\n",
            "[159,   340] loss: 1.515\n",
            "[159,   350] loss: 1.509\n",
            "epoch 158 val_loss 63.77470850944519 val_steps 40 val_acc 0.87\n",
            "[160,    10] loss: 1.512\n",
            "[160,    20] loss: 1.511\n",
            "[160,    30] loss: 1.511\n",
            "[160,    40] loss: 1.521\n",
            "[160,    50] loss: 1.519\n",
            "[160,    60] loss: 1.508\n",
            "[160,    70] loss: 1.530\n",
            "[160,    80] loss: 1.520\n",
            "[160,    90] loss: 1.515\n",
            "[160,   100] loss: 1.523\n",
            "[160,   110] loss: 1.517\n",
            "[160,   120] loss: 1.518\n",
            "[160,   130] loss: 1.517\n",
            "[160,   140] loss: 1.527\n",
            "[160,   150] loss: 1.520\n",
            "[160,   160] loss: 1.515\n",
            "[160,   170] loss: 1.517\n",
            "[160,   180] loss: 1.505\n",
            "[160,   190] loss: 1.518\n",
            "[160,   200] loss: 1.518\n",
            "[160,   210] loss: 1.518\n",
            "[160,   220] loss: 1.520\n",
            "[160,   230] loss: 1.518\n",
            "[160,   240] loss: 1.517\n",
            "[160,   250] loss: 1.513\n",
            "[160,   260] loss: 1.510\n",
            "[160,   270] loss: 1.510\n",
            "[160,   280] loss: 1.522\n",
            "[160,   290] loss: 1.517\n",
            "[160,   300] loss: 1.516\n",
            "[160,   310] loss: 1.505\n",
            "[160,   320] loss: 1.505\n",
            "[160,   330] loss: 1.514\n",
            "[160,   340] loss: 1.520\n",
            "[160,   350] loss: 1.514\n",
            "epoch 159 val_loss 63.93621349334717 val_steps 40 val_acc 0.8672\n",
            "[161,    10] loss: 1.524\n",
            "[161,    20] loss: 1.521\n",
            "[161,    30] loss: 1.525\n",
            "[161,    40] loss: 1.516\n",
            "[161,    50] loss: 1.523\n",
            "[161,    60] loss: 1.514\n",
            "[161,    70] loss: 1.501\n",
            "[161,    80] loss: 1.511\n",
            "[161,    90] loss: 1.525\n",
            "[161,   100] loss: 1.520\n",
            "[161,   110] loss: 1.515\n",
            "[161,   120] loss: 1.523\n",
            "[161,   130] loss: 1.510\n",
            "[161,   140] loss: 1.523\n",
            "[161,   150] loss: 1.517\n",
            "[161,   160] loss: 1.520\n",
            "[161,   170] loss: 1.512\n",
            "[161,   180] loss: 1.522\n",
            "[161,   190] loss: 1.515\n",
            "[161,   200] loss: 1.527\n",
            "[161,   210] loss: 1.509\n",
            "[161,   220] loss: 1.519\n",
            "[161,   230] loss: 1.520\n",
            "[161,   240] loss: 1.510\n",
            "[161,   250] loss: 1.523\n",
            "[161,   260] loss: 1.511\n",
            "[161,   270] loss: 1.514\n",
            "[161,   280] loss: 1.514\n",
            "[161,   290] loss: 1.521\n",
            "[161,   300] loss: 1.524\n",
            "[161,   310] loss: 1.513\n",
            "[161,   320] loss: 1.509\n",
            "[161,   330] loss: 1.512\n",
            "[161,   340] loss: 1.506\n",
            "[161,   350] loss: 1.521\n",
            "epoch 160 val_loss 64.14573872089386 val_steps 40 val_acc 0.8648\n",
            "[162,    10] loss: 1.512\n",
            "[162,    20] loss: 1.516\n",
            "[162,    30] loss: 1.511\n",
            "[162,    40] loss: 1.513\n",
            "[162,    50] loss: 1.528\n",
            "[162,    60] loss: 1.505\n",
            "[162,    70] loss: 1.514\n",
            "[162,    80] loss: 1.514\n",
            "[162,    90] loss: 1.523\n",
            "[162,   100] loss: 1.506\n",
            "[162,   110] loss: 1.513\n",
            "[162,   120] loss: 1.516\n",
            "[162,   130] loss: 1.516\n",
            "[162,   140] loss: 1.503\n",
            "[162,   150] loss: 1.523\n",
            "[162,   160] loss: 1.515\n",
            "[162,   170] loss: 1.503\n",
            "[162,   180] loss: 1.521\n",
            "[162,   190] loss: 1.517\n",
            "[162,   200] loss: 1.513\n",
            "[162,   210] loss: 1.513\n",
            "[162,   220] loss: 1.528\n",
            "[162,   230] loss: 1.515\n",
            "[162,   240] loss: 1.516\n",
            "[162,   250] loss: 1.521\n",
            "[162,   260] loss: 1.514\n",
            "[162,   270] loss: 1.526\n",
            "[162,   280] loss: 1.511\n",
            "[162,   290] loss: 1.526\n",
            "[162,   300] loss: 1.522\n",
            "[162,   310] loss: 1.512\n",
            "[162,   320] loss: 1.511\n",
            "[162,   330] loss: 1.526\n",
            "[162,   340] loss: 1.524\n",
            "[162,   350] loss: 1.509\n",
            "epoch 161 val_loss 63.93772542476654 val_steps 40 val_acc 0.8688\n",
            "[163,    10] loss: 1.520\n",
            "[163,    20] loss: 1.509\n",
            "[163,    30] loss: 1.533\n",
            "[163,    40] loss: 1.517\n",
            "[163,    50] loss: 1.518\n",
            "[163,    60] loss: 1.518\n",
            "[163,    70] loss: 1.515\n",
            "[163,    80] loss: 1.521\n",
            "[163,    90] loss: 1.520\n",
            "[163,   100] loss: 1.516\n",
            "[163,   110] loss: 1.514\n",
            "[163,   120] loss: 1.510\n",
            "[163,   130] loss: 1.512\n",
            "[163,   140] loss: 1.514\n",
            "[163,   150] loss: 1.515\n",
            "[163,   160] loss: 1.518\n",
            "[163,   170] loss: 1.521\n",
            "[163,   180] loss: 1.512\n",
            "[163,   190] loss: 1.514\n",
            "[163,   200] loss: 1.515\n",
            "[163,   210] loss: 1.519\n",
            "[163,   220] loss: 1.525\n",
            "[163,   230] loss: 1.515\n",
            "[163,   240] loss: 1.514\n",
            "[163,   250] loss: 1.514\n",
            "[163,   260] loss: 1.516\n",
            "[163,   270] loss: 1.519\n",
            "[163,   280] loss: 1.512\n",
            "[163,   290] loss: 1.520\n",
            "[163,   300] loss: 1.532\n",
            "[163,   310] loss: 1.513\n",
            "[163,   320] loss: 1.510\n",
            "[163,   330] loss: 1.516\n",
            "[163,   340] loss: 1.506\n",
            "[163,   350] loss: 1.528\n",
            "epoch 162 val_loss 63.77938997745514 val_steps 40 val_acc 0.866\n",
            "[164,    10] loss: 1.513\n",
            "[164,    20] loss: 1.515\n",
            "[164,    30] loss: 1.514\n",
            "[164,    40] loss: 1.512\n",
            "[164,    50] loss: 1.521\n",
            "[164,    60] loss: 1.516\n",
            "[164,    70] loss: 1.514\n",
            "[164,    80] loss: 1.521\n",
            "[164,    90] loss: 1.521\n",
            "[164,   100] loss: 1.511\n",
            "[164,   110] loss: 1.523\n",
            "[164,   120] loss: 1.527\n",
            "[164,   130] loss: 1.515\n",
            "[164,   140] loss: 1.518\n",
            "[164,   150] loss: 1.513\n",
            "[164,   160] loss: 1.526\n",
            "[164,   170] loss: 1.526\n",
            "[164,   180] loss: 1.514\n",
            "[164,   190] loss: 1.516\n",
            "[164,   200] loss: 1.519\n",
            "[164,   210] loss: 1.516\n",
            "[164,   220] loss: 1.524\n",
            "[164,   230] loss: 1.513\n",
            "[164,   240] loss: 1.513\n",
            "[164,   250] loss: 1.511\n",
            "[164,   260] loss: 1.511\n",
            "[164,   270] loss: 1.513\n",
            "[164,   280] loss: 1.519\n",
            "[164,   290] loss: 1.508\n",
            "[164,   300] loss: 1.520\n",
            "[164,   310] loss: 1.526\n",
            "[164,   320] loss: 1.520\n",
            "[164,   330] loss: 1.513\n",
            "[164,   340] loss: 1.514\n",
            "[164,   350] loss: 1.520\n",
            "epoch 163 val_loss 63.83899211883545 val_steps 40 val_acc 0.8688\n",
            "[165,    10] loss: 1.506\n",
            "[165,    20] loss: 1.512\n",
            "[165,    30] loss: 1.518\n",
            "[165,    40] loss: 1.516\n",
            "[165,    50] loss: 1.509\n",
            "[165,    60] loss: 1.525\n",
            "[165,    70] loss: 1.509\n",
            "[165,    80] loss: 1.513\n",
            "[165,    90] loss: 1.519\n",
            "[165,   100] loss: 1.526\n",
            "[165,   110] loss: 1.515\n",
            "[165,   120] loss: 1.508\n",
            "[165,   130] loss: 1.532\n",
            "[165,   140] loss: 1.517\n",
            "[165,   150] loss: 1.521\n",
            "[165,   160] loss: 1.523\n",
            "[165,   170] loss: 1.511\n",
            "[165,   180] loss: 1.521\n",
            "[165,   190] loss: 1.514\n",
            "[165,   200] loss: 1.512\n",
            "[165,   210] loss: 1.523\n",
            "[165,   220] loss: 1.539\n",
            "[165,   230] loss: 1.513\n",
            "[165,   240] loss: 1.519\n",
            "[165,   250] loss: 1.510\n",
            "[165,   260] loss: 1.507\n",
            "[165,   270] loss: 1.520\n",
            "[165,   280] loss: 1.510\n",
            "[165,   290] loss: 1.505\n",
            "[165,   300] loss: 1.515\n",
            "[165,   310] loss: 1.513\n",
            "[165,   320] loss: 1.509\n",
            "[165,   330] loss: 1.506\n",
            "[165,   340] loss: 1.508\n",
            "[165,   350] loss: 1.513\n",
            "epoch 164 val_loss 63.80912208557129 val_steps 40 val_acc 0.867\n",
            "[166,    10] loss: 1.512\n",
            "[166,    20] loss: 1.512\n",
            "[166,    30] loss: 1.513\n",
            "[166,    40] loss: 1.516\n",
            "[166,    50] loss: 1.520\n",
            "[166,    60] loss: 1.517\n",
            "[166,    70] loss: 1.512\n",
            "[166,    80] loss: 1.511\n",
            "[166,    90] loss: 1.519\n",
            "[166,   100] loss: 1.511\n",
            "[166,   110] loss: 1.523\n",
            "[166,   120] loss: 1.504\n",
            "[166,   130] loss: 1.528\n",
            "[166,   140] loss: 1.521\n",
            "[166,   150] loss: 1.520\n",
            "[166,   160] loss: 1.517\n",
            "[166,   170] loss: 1.516\n",
            "[166,   180] loss: 1.507\n",
            "[166,   190] loss: 1.532\n",
            "[166,   200] loss: 1.521\n",
            "[166,   210] loss: 1.519\n",
            "[166,   220] loss: 1.517\n",
            "[166,   230] loss: 1.517\n",
            "[166,   240] loss: 1.512\n",
            "[166,   250] loss: 1.504\n",
            "[166,   260] loss: 1.515\n",
            "[166,   270] loss: 1.513\n",
            "[166,   280] loss: 1.515\n",
            "[166,   290] loss: 1.505\n",
            "[166,   300] loss: 1.528\n",
            "[166,   310] loss: 1.514\n",
            "[166,   320] loss: 1.526\n",
            "[166,   330] loss: 1.519\n",
            "[166,   340] loss: 1.523\n",
            "[166,   350] loss: 1.517\n",
            "epoch 165 val_loss 64.00853323936462 val_steps 40 val_acc 0.8622\n",
            "[167,    10] loss: 1.519\n",
            "[167,    20] loss: 1.519\n",
            "[167,    30] loss: 1.517\n",
            "[167,    40] loss: 1.504\n",
            "[167,    50] loss: 1.519\n",
            "[167,    60] loss: 1.506\n",
            "[167,    70] loss: 1.509\n",
            "[167,    80] loss: 1.516\n",
            "[167,    90] loss: 1.510\n",
            "[167,   100] loss: 1.510\n",
            "[167,   110] loss: 1.520\n",
            "[167,   120] loss: 1.508\n",
            "[167,   130] loss: 1.523\n",
            "[167,   140] loss: 1.518\n",
            "[167,   150] loss: 1.514\n",
            "[167,   160] loss: 1.516\n",
            "[167,   170] loss: 1.516\n",
            "[167,   180] loss: 1.523\n",
            "[167,   190] loss: 1.509\n",
            "[167,   200] loss: 1.511\n",
            "[167,   210] loss: 1.523\n",
            "[167,   220] loss: 1.514\n",
            "[167,   230] loss: 1.519\n",
            "[167,   240] loss: 1.526\n",
            "[167,   250] loss: 1.521\n",
            "[167,   260] loss: 1.509\n",
            "[167,   270] loss: 1.519\n",
            "[167,   280] loss: 1.509\n",
            "[167,   290] loss: 1.511\n",
            "[167,   300] loss: 1.516\n",
            "[167,   310] loss: 1.522\n",
            "[167,   320] loss: 1.513\n",
            "[167,   330] loss: 1.525\n",
            "[167,   340] loss: 1.518\n",
            "[167,   350] loss: 1.526\n",
            "epoch 166 val_loss 63.9801025390625 val_steps 40 val_acc 0.867\n",
            "[168,    10] loss: 1.521\n",
            "[168,    20] loss: 1.513\n",
            "[168,    30] loss: 1.513\n",
            "[168,    40] loss: 1.521\n",
            "[168,    50] loss: 1.531\n",
            "[168,    60] loss: 1.515\n",
            "[168,    70] loss: 1.513\n",
            "[168,    80] loss: 1.519\n",
            "[168,    90] loss: 1.512\n",
            "[168,   100] loss: 1.514\n",
            "[168,   110] loss: 1.521\n",
            "[168,   120] loss: 1.521\n",
            "[168,   130] loss: 1.510\n",
            "[168,   140] loss: 1.512\n",
            "[168,   150] loss: 1.508\n",
            "[168,   160] loss: 1.515\n",
            "[168,   170] loss: 1.517\n",
            "[168,   180] loss: 1.511\n",
            "[168,   190] loss: 1.517\n",
            "[168,   200] loss: 1.518\n",
            "[168,   210] loss: 1.509\n",
            "[168,   220] loss: 1.512\n",
            "[168,   230] loss: 1.513\n",
            "[168,   240] loss: 1.521\n",
            "[168,   250] loss: 1.518\n",
            "[168,   260] loss: 1.511\n",
            "[168,   270] loss: 1.520\n",
            "[168,   280] loss: 1.522\n",
            "[168,   290] loss: 1.524\n",
            "[168,   300] loss: 1.522\n",
            "[168,   310] loss: 1.520\n",
            "[168,   320] loss: 1.518\n",
            "[168,   330] loss: 1.515\n",
            "[168,   340] loss: 1.525\n",
            "[168,   350] loss: 1.520\n",
            "epoch 167 val_loss 63.83158600330353 val_steps 40 val_acc 0.8682\n",
            "[169,    10] loss: 1.513\n",
            "[169,    20] loss: 1.512\n",
            "[169,    30] loss: 1.523\n",
            "[169,    40] loss: 1.511\n",
            "[169,    50] loss: 1.526\n",
            "[169,    60] loss: 1.502\n",
            "[169,    70] loss: 1.507\n",
            "[169,    80] loss: 1.528\n",
            "[169,    90] loss: 1.515\n",
            "[169,   100] loss: 1.508\n",
            "[169,   110] loss: 1.528\n",
            "[169,   120] loss: 1.509\n",
            "[169,   130] loss: 1.520\n",
            "[169,   140] loss: 1.519\n",
            "[169,   150] loss: 1.517\n",
            "[169,   160] loss: 1.514\n",
            "[169,   170] loss: 1.521\n",
            "[169,   180] loss: 1.523\n",
            "[169,   190] loss: 1.513\n",
            "[169,   200] loss: 1.514\n",
            "[169,   210] loss: 1.517\n",
            "[169,   220] loss: 1.507\n",
            "[169,   230] loss: 1.506\n",
            "[169,   240] loss: 1.514\n",
            "[169,   250] loss: 1.507\n",
            "[169,   260] loss: 1.522\n",
            "[169,   270] loss: 1.513\n",
            "[169,   280] loss: 1.522\n",
            "[169,   290] loss: 1.520\n",
            "[169,   300] loss: 1.518\n",
            "[169,   310] loss: 1.507\n",
            "[169,   320] loss: 1.520\n",
            "[169,   330] loss: 1.522\n",
            "[169,   340] loss: 1.516\n",
            "[169,   350] loss: 1.515\n",
            "epoch 168 val_loss 63.758487820625305 val_steps 40 val_acc 0.8656\n",
            "[170,    10] loss: 1.516\n",
            "[170,    20] loss: 1.511\n",
            "[170,    30] loss: 1.511\n",
            "[170,    40] loss: 1.516\n",
            "[170,    50] loss: 1.513\n",
            "[170,    60] loss: 1.512\n",
            "[170,    70] loss: 1.520\n",
            "[170,    80] loss: 1.523\n",
            "[170,    90] loss: 1.516\n",
            "[170,   100] loss: 1.526\n",
            "[170,   110] loss: 1.520\n",
            "[170,   120] loss: 1.513\n",
            "[170,   130] loss: 1.505\n",
            "[170,   140] loss: 1.510\n",
            "[170,   150] loss: 1.514\n",
            "[170,   160] loss: 1.506\n",
            "[170,   170] loss: 1.521\n",
            "[170,   180] loss: 1.523\n",
            "[170,   190] loss: 1.521\n",
            "[170,   200] loss: 1.507\n",
            "[170,   210] loss: 1.510\n",
            "[170,   220] loss: 1.513\n",
            "[170,   230] loss: 1.515\n",
            "[170,   240] loss: 1.515\n",
            "[170,   250] loss: 1.527\n",
            "[170,   260] loss: 1.522\n",
            "[170,   270] loss: 1.515\n",
            "[170,   280] loss: 1.514\n",
            "[170,   290] loss: 1.506\n",
            "[170,   300] loss: 1.527\n",
            "[170,   310] loss: 1.509\n",
            "[170,   320] loss: 1.521\n",
            "[170,   330] loss: 1.515\n",
            "[170,   340] loss: 1.515\n",
            "[170,   350] loss: 1.513\n",
            "epoch 169 val_loss 63.83999729156494 val_steps 40 val_acc 0.8668\n",
            "[171,    10] loss: 1.517\n",
            "[171,    20] loss: 1.513\n",
            "[171,    30] loss: 1.516\n",
            "[171,    40] loss: 1.509\n",
            "[171,    50] loss: 1.516\n",
            "[171,    60] loss: 1.529\n",
            "[171,    70] loss: 1.507\n",
            "[171,    80] loss: 1.516\n",
            "[171,    90] loss: 1.509\n",
            "[171,   100] loss: 1.512\n",
            "[171,   110] loss: 1.522\n",
            "[171,   120] loss: 1.519\n",
            "[171,   130] loss: 1.510\n",
            "[171,   140] loss: 1.514\n",
            "[171,   150] loss: 1.520\n",
            "[171,   160] loss: 1.517\n",
            "[171,   170] loss: 1.516\n",
            "[171,   180] loss: 1.514\n",
            "[171,   190] loss: 1.517\n",
            "[171,   200] loss: 1.518\n",
            "[171,   210] loss: 1.510\n",
            "[171,   220] loss: 1.513\n",
            "[171,   230] loss: 1.519\n",
            "[171,   240] loss: 1.515\n",
            "[171,   250] loss: 1.518\n",
            "[171,   260] loss: 1.520\n",
            "[171,   270] loss: 1.514\n",
            "[171,   280] loss: 1.518\n",
            "[171,   290] loss: 1.512\n",
            "[171,   300] loss: 1.519\n",
            "[171,   310] loss: 1.516\n",
            "[171,   320] loss: 1.513\n",
            "[171,   330] loss: 1.513\n",
            "[171,   340] loss: 1.515\n",
            "[171,   350] loss: 1.514\n",
            "epoch 170 val_loss 64.02617406845093 val_steps 40 val_acc 0.8694\n",
            "[172,    10] loss: 1.526\n",
            "[172,    20] loss: 1.510\n",
            "[172,    30] loss: 1.512\n",
            "[172,    40] loss: 1.497\n",
            "[172,    50] loss: 1.519\n",
            "[172,    60] loss: 1.524\n",
            "[172,    70] loss: 1.513\n",
            "[172,    80] loss: 1.516\n",
            "[172,    90] loss: 1.510\n",
            "[172,   100] loss: 1.520\n",
            "[172,   110] loss: 1.514\n",
            "[172,   120] loss: 1.511\n",
            "[172,   130] loss: 1.515\n",
            "[172,   140] loss: 1.517\n",
            "[172,   150] loss: 1.519\n",
            "[172,   160] loss: 1.517\n",
            "[172,   170] loss: 1.516\n",
            "[172,   180] loss: 1.522\n",
            "[172,   190] loss: 1.515\n",
            "[172,   200] loss: 1.518\n",
            "[172,   210] loss: 1.514\n",
            "[172,   220] loss: 1.504\n",
            "[172,   230] loss: 1.515\n",
            "[172,   240] loss: 1.514\n",
            "[172,   250] loss: 1.517\n",
            "[172,   260] loss: 1.511\n",
            "[172,   270] loss: 1.520\n",
            "[172,   280] loss: 1.517\n",
            "[172,   290] loss: 1.511\n",
            "[172,   300] loss: 1.512\n",
            "[172,   310] loss: 1.504\n",
            "[172,   320] loss: 1.511\n",
            "[172,   330] loss: 1.516\n",
            "[172,   340] loss: 1.515\n",
            "[172,   350] loss: 1.511\n",
            "epoch 171 val_loss 63.69399058818817 val_steps 40 val_acc 0.8686\n",
            "[173,    10] loss: 1.517\n",
            "[173,    20] loss: 1.513\n",
            "[173,    30] loss: 1.516\n",
            "[173,    40] loss: 1.514\n",
            "[173,    50] loss: 1.511\n",
            "[173,    60] loss: 1.510\n",
            "[173,    70] loss: 1.506\n",
            "[173,    80] loss: 1.518\n",
            "[173,    90] loss: 1.518\n",
            "[173,   100] loss: 1.521\n",
            "[173,   110] loss: 1.520\n",
            "[173,   120] loss: 1.512\n",
            "[173,   130] loss: 1.505\n",
            "[173,   140] loss: 1.526\n",
            "[173,   150] loss: 1.523\n",
            "[173,   160] loss: 1.510\n",
            "[173,   170] loss: 1.519\n",
            "[173,   180] loss: 1.528\n",
            "[173,   190] loss: 1.514\n",
            "[173,   200] loss: 1.506\n",
            "[173,   210] loss: 1.525\n",
            "[173,   220] loss: 1.502\n",
            "[173,   230] loss: 1.521\n",
            "[173,   240] loss: 1.511\n",
            "[173,   250] loss: 1.532\n",
            "[173,   260] loss: 1.510\n",
            "[173,   270] loss: 1.511\n",
            "[173,   280] loss: 1.532\n",
            "[173,   290] loss: 1.518\n",
            "[173,   300] loss: 1.511\n",
            "[173,   310] loss: 1.517\n",
            "[173,   320] loss: 1.513\n",
            "[173,   330] loss: 1.516\n",
            "[173,   340] loss: 1.516\n",
            "[173,   350] loss: 1.517\n",
            "epoch 172 val_loss 63.9054411649704 val_steps 40 val_acc 0.8666\n",
            "[174,    10] loss: 1.515\n",
            "[174,    20] loss: 1.515\n",
            "[174,    30] loss: 1.532\n",
            "[174,    40] loss: 1.508\n",
            "[174,    50] loss: 1.517\n",
            "[174,    60] loss: 1.514\n",
            "[174,    70] loss: 1.521\n",
            "[174,    80] loss: 1.505\n",
            "[174,    90] loss: 1.521\n",
            "[174,   100] loss: 1.519\n",
            "[174,   110] loss: 1.518\n",
            "[174,   120] loss: 1.525\n",
            "[174,   130] loss: 1.513\n",
            "[174,   140] loss: 1.510\n",
            "[174,   150] loss: 1.518\n",
            "[174,   160] loss: 1.512\n",
            "[174,   170] loss: 1.526\n",
            "[174,   180] loss: 1.513\n",
            "[174,   190] loss: 1.502\n",
            "[174,   200] loss: 1.511\n",
            "[174,   210] loss: 1.513\n",
            "[174,   220] loss: 1.515\n",
            "[174,   230] loss: 1.523\n",
            "[174,   240] loss: 1.506\n",
            "[174,   250] loss: 1.519\n",
            "[174,   260] loss: 1.522\n",
            "[174,   270] loss: 1.512\n",
            "[174,   280] loss: 1.510\n",
            "[174,   290] loss: 1.519\n",
            "[174,   300] loss: 1.517\n",
            "[174,   310] loss: 1.507\n",
            "[174,   320] loss: 1.520\n",
            "[174,   330] loss: 1.519\n",
            "[174,   340] loss: 1.523\n",
            "[174,   350] loss: 1.513\n",
            "epoch 173 val_loss 63.751962423324585 val_steps 40 val_acc 0.8662\n",
            "[175,    10] loss: 1.514\n",
            "[175,    20] loss: 1.519\n",
            "[175,    30] loss: 1.514\n",
            "[175,    40] loss: 1.525\n",
            "[175,    50] loss: 1.514\n",
            "[175,    60] loss: 1.509\n",
            "[175,    70] loss: 1.512\n",
            "[175,    80] loss: 1.515\n",
            "[175,    90] loss: 1.508\n",
            "[175,   100] loss: 1.518\n",
            "[175,   110] loss: 1.514\n",
            "[175,   120] loss: 1.506\n",
            "[175,   130] loss: 1.514\n",
            "[175,   140] loss: 1.521\n",
            "[175,   150] loss: 1.515\n",
            "[175,   160] loss: 1.515\n",
            "[175,   170] loss: 1.515\n",
            "[175,   180] loss: 1.510\n",
            "[175,   190] loss: 1.517\n",
            "[175,   200] loss: 1.518\n",
            "[175,   210] loss: 1.511\n",
            "[175,   220] loss: 1.517\n",
            "[175,   230] loss: 1.509\n",
            "[175,   240] loss: 1.525\n",
            "[175,   250] loss: 1.521\n",
            "[175,   260] loss: 1.512\n",
            "[175,   270] loss: 1.523\n",
            "[175,   280] loss: 1.505\n",
            "[175,   290] loss: 1.517\n",
            "[175,   300] loss: 1.522\n",
            "[175,   310] loss: 1.509\n",
            "[175,   320] loss: 1.518\n",
            "[175,   330] loss: 1.521\n",
            "[175,   340] loss: 1.514\n",
            "[175,   350] loss: 1.511\n",
            "epoch 174 val_loss 64.06095612049103 val_steps 40 val_acc 0.8654\n",
            "[176,    10] loss: 1.514\n",
            "[176,    20] loss: 1.519\n",
            "[176,    30] loss: 1.521\n",
            "[176,    40] loss: 1.525\n",
            "[176,    50] loss: 1.511\n",
            "[176,    60] loss: 1.515\n",
            "[176,    70] loss: 1.510\n",
            "[176,    80] loss: 1.515\n",
            "[176,    90] loss: 1.510\n",
            "[176,   100] loss: 1.520\n",
            "[176,   110] loss: 1.519\n",
            "[176,   120] loss: 1.509\n",
            "[176,   130] loss: 1.521\n",
            "[176,   140] loss: 1.508\n",
            "[176,   150] loss: 1.515\n",
            "[176,   160] loss: 1.521\n",
            "[176,   170] loss: 1.510\n",
            "[176,   180] loss: 1.519\n",
            "[176,   190] loss: 1.525\n",
            "[176,   200] loss: 1.512\n",
            "[176,   210] loss: 1.508\n",
            "[176,   220] loss: 1.524\n",
            "[176,   230] loss: 1.511\n",
            "[176,   240] loss: 1.528\n",
            "[176,   250] loss: 1.514\n",
            "[176,   260] loss: 1.521\n",
            "[176,   270] loss: 1.517\n",
            "[176,   280] loss: 1.517\n",
            "[176,   290] loss: 1.516\n",
            "[176,   300] loss: 1.510\n",
            "[176,   310] loss: 1.502\n",
            "[176,   320] loss: 1.517\n",
            "[176,   330] loss: 1.509\n",
            "[176,   340] loss: 1.521\n",
            "[176,   350] loss: 1.515\n",
            "epoch 175 val_loss 63.97083818912506 val_steps 40 val_acc 0.865\n",
            "[177,    10] loss: 1.522\n",
            "[177,    20] loss: 1.517\n",
            "[177,    30] loss: 1.508\n",
            "[177,    40] loss: 1.513\n",
            "[177,    50] loss: 1.509\n",
            "[177,    60] loss: 1.509\n",
            "[177,    70] loss: 1.503\n",
            "[177,    80] loss: 1.521\n",
            "[177,    90] loss: 1.509\n",
            "[177,   100] loss: 1.513\n",
            "[177,   110] loss: 1.505\n",
            "[177,   120] loss: 1.527\n",
            "[177,   130] loss: 1.516\n",
            "[177,   140] loss: 1.518\n",
            "[177,   150] loss: 1.514\n",
            "[177,   160] loss: 1.525\n",
            "[177,   170] loss: 1.522\n",
            "[177,   180] loss: 1.516\n",
            "[177,   190] loss: 1.517\n",
            "[177,   200] loss: 1.513\n",
            "[177,   210] loss: 1.519\n",
            "[177,   220] loss: 1.512\n",
            "[177,   230] loss: 1.513\n",
            "[177,   240] loss: 1.514\n",
            "[177,   250] loss: 1.510\n",
            "[177,   260] loss: 1.507\n",
            "[177,   270] loss: 1.516\n",
            "[177,   280] loss: 1.519\n",
            "[177,   290] loss: 1.509\n",
            "[177,   300] loss: 1.517\n",
            "[177,   310] loss: 1.510\n",
            "[177,   320] loss: 1.512\n",
            "[177,   330] loss: 1.515\n",
            "[177,   340] loss: 1.514\n",
            "[177,   350] loss: 1.511\n",
            "epoch 176 val_loss 63.83946764469147 val_steps 40 val_acc 0.8688\n",
            "[178,    10] loss: 1.522\n",
            "[178,    20] loss: 1.516\n",
            "[178,    30] loss: 1.523\n",
            "[178,    40] loss: 1.516\n",
            "[178,    50] loss: 1.520\n",
            "[178,    60] loss: 1.520\n",
            "[178,    70] loss: 1.522\n",
            "[178,    80] loss: 1.514\n",
            "[178,    90] loss: 1.518\n",
            "[178,   100] loss: 1.507\n",
            "[178,   110] loss: 1.515\n",
            "[178,   120] loss: 1.512\n",
            "[178,   130] loss: 1.519\n",
            "[178,   140] loss: 1.512\n",
            "[178,   150] loss: 1.513\n",
            "[178,   160] loss: 1.513\n",
            "[178,   170] loss: 1.511\n",
            "[178,   180] loss: 1.512\n",
            "[178,   190] loss: 1.523\n",
            "[178,   200] loss: 1.516\n",
            "[178,   210] loss: 1.514\n",
            "[178,   220] loss: 1.516\n",
            "[178,   230] loss: 1.515\n",
            "[178,   240] loss: 1.522\n",
            "[178,   250] loss: 1.510\n",
            "[178,   260] loss: 1.518\n",
            "[178,   270] loss: 1.519\n",
            "[178,   280] loss: 1.507\n",
            "[178,   290] loss: 1.517\n",
            "[178,   300] loss: 1.511\n",
            "[178,   310] loss: 1.517\n",
            "[178,   320] loss: 1.515\n",
            "[178,   330] loss: 1.515\n",
            "[178,   340] loss: 1.514\n",
            "[178,   350] loss: 1.523\n",
            "epoch 177 val_loss 64.02334702014923 val_steps 40 val_acc 0.867\n",
            "[179,    10] loss: 1.515\n",
            "[179,    20] loss: 1.521\n",
            "[179,    30] loss: 1.517\n",
            "[179,    40] loss: 1.519\n",
            "[179,    50] loss: 1.518\n",
            "[179,    60] loss: 1.518\n",
            "[179,    70] loss: 1.508\n",
            "[179,    80] loss: 1.515\n",
            "[179,    90] loss: 1.505\n",
            "[179,   100] loss: 1.504\n",
            "[179,   110] loss: 1.515\n",
            "[179,   120] loss: 1.526\n",
            "[179,   130] loss: 1.519\n",
            "[179,   140] loss: 1.519\n",
            "[179,   150] loss: 1.512\n",
            "[179,   160] loss: 1.522\n",
            "[179,   170] loss: 1.509\n",
            "[179,   180] loss: 1.515\n",
            "[179,   190] loss: 1.513\n",
            "[179,   200] loss: 1.511\n",
            "[179,   210] loss: 1.517\n",
            "[179,   220] loss: 1.505\n",
            "[179,   230] loss: 1.520\n",
            "[179,   240] loss: 1.511\n",
            "[179,   250] loss: 1.522\n",
            "[179,   260] loss: 1.513\n",
            "[179,   270] loss: 1.504\n",
            "[179,   280] loss: 1.515\n",
            "[179,   290] loss: 1.517\n",
            "[179,   300] loss: 1.522\n",
            "[179,   310] loss: 1.515\n",
            "[179,   320] loss: 1.513\n",
            "[179,   330] loss: 1.521\n",
            "[179,   340] loss: 1.520\n",
            "[179,   350] loss: 1.514\n",
            "epoch 178 val_loss 64.30702757835388 val_steps 40 val_acc 0.866\n",
            "[180,    10] loss: 1.513\n",
            "[180,    20] loss: 1.512\n",
            "[180,    30] loss: 1.513\n",
            "[180,    40] loss: 1.516\n",
            "[180,    50] loss: 1.519\n",
            "[180,    60] loss: 1.510\n",
            "[180,    70] loss: 1.510\n",
            "[180,    80] loss: 1.511\n",
            "[180,    90] loss: 1.516\n",
            "[180,   100] loss: 1.516\n",
            "[180,   110] loss: 1.514\n",
            "[180,   120] loss: 1.515\n",
            "[180,   130] loss: 1.512\n",
            "[180,   140] loss: 1.519\n",
            "[180,   150] loss: 1.524\n",
            "[180,   160] loss: 1.514\n",
            "[180,   170] loss: 1.519\n",
            "[180,   180] loss: 1.516\n",
            "[180,   190] loss: 1.511\n",
            "[180,   200] loss: 1.510\n",
            "[180,   210] loss: 1.505\n",
            "[180,   220] loss: 1.521\n",
            "[180,   230] loss: 1.511\n",
            "[180,   240] loss: 1.521\n",
            "[180,   250] loss: 1.513\n",
            "[180,   260] loss: 1.518\n",
            "[180,   270] loss: 1.512\n",
            "[180,   280] loss: 1.504\n",
            "[180,   290] loss: 1.509\n",
            "[180,   300] loss: 1.510\n",
            "[180,   310] loss: 1.511\n",
            "[180,   320] loss: 1.511\n",
            "[180,   330] loss: 1.516\n",
            "[180,   340] loss: 1.518\n",
            "[180,   350] loss: 1.512\n",
            "epoch 179 val_loss 63.862698554992676 val_steps 40 val_acc 0.8706\n",
            "[181,    10] loss: 1.523\n",
            "[181,    20] loss: 1.520\n",
            "[181,    30] loss: 1.532\n",
            "[181,    40] loss: 1.510\n",
            "[181,    50] loss: 1.517\n",
            "[181,    60] loss: 1.513\n",
            "[181,    70] loss: 1.515\n",
            "[181,    80] loss: 1.518\n",
            "[181,    90] loss: 1.508\n",
            "[181,   100] loss: 1.515\n",
            "[181,   110] loss: 1.518\n",
            "[181,   120] loss: 1.506\n",
            "[181,   130] loss: 1.516\n",
            "[181,   140] loss: 1.522\n",
            "[181,   150] loss: 1.516\n",
            "[181,   160] loss: 1.508\n",
            "[181,   170] loss: 1.518\n",
            "[181,   180] loss: 1.527\n",
            "[181,   190] loss: 1.511\n",
            "[181,   200] loss: 1.527\n",
            "[181,   210] loss: 1.513\n",
            "[181,   220] loss: 1.510\n",
            "[181,   230] loss: 1.508\n",
            "[181,   240] loss: 1.517\n",
            "[181,   250] loss: 1.504\n",
            "[181,   260] loss: 1.520\n",
            "[181,   270] loss: 1.505\n",
            "[181,   280] loss: 1.514\n",
            "[181,   290] loss: 1.517\n",
            "[181,   300] loss: 1.512\n",
            "[181,   310] loss: 1.522\n",
            "[181,   320] loss: 1.514\n",
            "[181,   330] loss: 1.514\n",
            "[181,   340] loss: 1.516\n",
            "[181,   350] loss: 1.514\n",
            "epoch 180 val_loss 63.81465172767639 val_steps 40 val_acc 0.8654\n",
            "[182,    10] loss: 1.510\n",
            "[182,    20] loss: 1.519\n",
            "[182,    30] loss: 1.512\n",
            "[182,    40] loss: 1.513\n",
            "[182,    50] loss: 1.505\n",
            "[182,    60] loss: 1.511\n",
            "[182,    70] loss: 1.514\n",
            "[182,    80] loss: 1.520\n",
            "[182,    90] loss: 1.533\n",
            "[182,   100] loss: 1.516\n",
            "[182,   110] loss: 1.521\n",
            "[182,   120] loss: 1.510\n",
            "[182,   130] loss: 1.511\n",
            "[182,   140] loss: 1.511\n",
            "[182,   150] loss: 1.518\n",
            "[182,   160] loss: 1.520\n",
            "[182,   170] loss: 1.511\n",
            "[182,   180] loss: 1.516\n",
            "[182,   190] loss: 1.513\n",
            "[182,   200] loss: 1.519\n",
            "[182,   210] loss: 1.501\n",
            "[182,   220] loss: 1.515\n",
            "[182,   230] loss: 1.512\n",
            "[182,   240] loss: 1.510\n",
            "[182,   250] loss: 1.521\n",
            "[182,   260] loss: 1.517\n",
            "[182,   270] loss: 1.514\n",
            "[182,   280] loss: 1.521\n",
            "[182,   290] loss: 1.509\n",
            "[182,   300] loss: 1.510\n",
            "[182,   310] loss: 1.512\n",
            "[182,   320] loss: 1.513\n",
            "[182,   330] loss: 1.517\n",
            "[182,   340] loss: 1.507\n",
            "[182,   350] loss: 1.522\n",
            "epoch 181 val_loss 63.795618653297424 val_steps 40 val_acc 0.8708\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test accuracy\n"
      ],
      "metadata": {
        "id": "RElAIN2qh3oJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(net, testloader, device, epoch):\n",
        "    global best_acc\n",
        "    net.eval()\n",
        "    test_loss=0\n",
        "    correct=0\n",
        "    total=0\n",
        "    criterion=nn.CrossEntropyLoss()\n",
        "    test_steps=0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx,(inputs,targets)in enumerate(testloader):\n",
        "            inputs,targets=inputs.to(device),targets.to(device)\n",
        "            outputs=net(inputs)\n",
        "            loss=criterion(outputs,targets)\n",
        "\n",
        "            test_loss+=loss.item()\n",
        "            _,predicted=outputs.max(1)\n",
        "            total+=targets.size(0)\n",
        "            correct+=predicted.eq(targets).sum().item()\n",
        "            test_steps+=1\n",
        "    print(\"epoch {} test_loss {} test_steps {} test_acc {}\".format(epoch,test_loss,test_steps,correct/total))\n",
        "\n",
        "batch_size = 128\n",
        "_, _, testloader = data_loader(batch_size)\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "  device = \"cuda:0\"\n",
        "test(trained_net, testloader, device, 1)\n",
        "#print(\"test acc {}\".format(test_accuracy(trained_net)))"
      ],
      "metadata": {
        "id": "z2T0sHBWh7E9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60ff6d9a-34a0-4420-c1d6-7a292c00c08c"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "epoch 1 test_loss 125.8524432182312 test_steps 79 test_acc 0.8683\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ...\n"
      ],
      "metadata": {
        "id": "qhaxY8hFXwcY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DenseNet"
      ],
      "metadata": {
        "id": "4u-oCJdf3evv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model"
      ],
      "metadata": {
        "id": "1K2rIgSc4HdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, in_planes, growth_rate):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "        self.conv1 = nn.Conv2d(in_planes, 4*growth_rate, kernel_size=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(4*growth_rate)\n",
        "        self.conv2 = nn.Conv2d(4*growth_rate, growth_rate, kernel_size=3, padding=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(F.relu(self.bn1(x)))\n",
        "        out = self.conv2(F.relu(self.bn2(out)))\n",
        "        out = torch.cat([out,x], 1)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Transition(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes):\n",
        "        super(Transition, self).__init__()\n",
        "        self.bn = nn.BatchNorm2d(in_planes)\n",
        "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(F.relu(self.bn(x)))\n",
        "        out = F.avg_pool2d(out, 2)\n",
        "        return out\n",
        "\n",
        "\n",
        "class DenseNet(nn.Module):\n",
        "    def __init__(self, block, nblocks, growth_rate=12, reduction=0.5, num_classes=10):\n",
        "        super(DenseNet, self).__init__()\n",
        "        self.growth_rate = growth_rate\n",
        "\n",
        "        num_planes = 2*growth_rate\n",
        "        self.conv1 = nn.Conv2d(3, num_planes, kernel_size=3, padding=1, bias=False)\n",
        "\n",
        "        self.dense1 = self._make_dense_layers(block, num_planes, nblocks[0])\n",
        "        num_planes += nblocks[0]*growth_rate\n",
        "        out_planes = int(math.floor(num_planes*reduction))\n",
        "        self.trans1 = Transition(num_planes, out_planes)\n",
        "        num_planes = out_planes\n",
        "\n",
        "        self.dense2 = self._make_dense_layers(block, num_planes, nblocks[1])\n",
        "        num_planes += nblocks[1]*growth_rate\n",
        "        out_planes = int(math.floor(num_planes*reduction))\n",
        "        self.trans2 = Transition(num_planes, out_planes)\n",
        "        num_planes = out_planes\n",
        "\n",
        "        self.dense3 = self._make_dense_layers(block, num_planes, nblocks[2])\n",
        "        num_planes += nblocks[2]*growth_rate\n",
        "        out_planes = int(math.floor(num_planes*reduction))\n",
        "        self.trans3 = Transition(num_planes, out_planes)\n",
        "        num_planes = out_planes\n",
        "\n",
        "        self.dense4 = self._make_dense_layers(block, num_planes, nblocks[3])\n",
        "        num_planes += nblocks[3]*growth_rate\n",
        "\n",
        "        self.bn = nn.BatchNorm2d(num_planes)\n",
        "        self.linear = nn.Linear(num_planes, num_classes)\n",
        "\n",
        "    def _make_dense_layers(self, block, in_planes, nblock):\n",
        "        layers = []\n",
        "        for i in range(nblock):\n",
        "            layers.append(block(in_planes, self.growth_rate))\n",
        "            in_planes += self.growth_rate\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.trans1(self.dense1(out))\n",
        "        out = self.trans2(self.dense2(out))\n",
        "        out = self.trans3(self.dense3(out))\n",
        "        out = self.dense4(out)\n",
        "        out = F.avg_pool2d(F.relu(self.bn(out)), 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "def DenseNet121():\n",
        "    return DenseNet(Bottleneck, [6,12,24,16], growth_rate=12)"
      ],
      "metadata": {
        "id": "aGsV4myB3gtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train\n"
      ],
      "metadata": {
        "id": "azb3E3-x4Joi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "batch size 64 for 300 and 40 epochs\n",
        "init lr 0.1 divided by 10 at 50% and 75%\n",
        "weight decay 10^-4\n",
        "momentum 0.9\n",
        "dropout 0.2"
      ],
      "metadata": {
        "id": "OhgXD33CMxHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_dense(weight_decay=0.0001, momentum=0.9, batch_size=128):\n",
        "  net = torch.hub.load('pytorch/vision:v0.10.0', 'densenet121', pretrained=False)\n",
        "  \n",
        "  trainloader, valloader, testloader = data_loader(batch_size)\n",
        "  classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "  \n",
        "  device = \"cpu\"\n",
        "  if torch.cuda.is_available():\n",
        "      device = \"cuda:0\"\n",
        "      if torch.cuda.device_count() > 1:\n",
        "          net = nn.DataParallel(net)\n",
        "  net.to(device)\n",
        "\n",
        "  def _lr_lambda(current_step):\n",
        "      \"\"\"\n",
        "      _lr_lambda returns a multiplicative factor given an interger parameter epochs.\n",
        "      \"\"\"\n",
        "      if current_step < 25000:\n",
        "          _lr = 1\n",
        "      elif current_step < 37500:\n",
        "          _lr = .1\n",
        "      else:\n",
        "          _lr = .01\n",
        "      return _lr\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(net.parameters(), lr=0.1,\n",
        "                        momentum, weight_decay)\n",
        "  scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, _lr_lambda, last_epoch=-1, verbose=True)\n",
        "\n",
        "  for epoch in range(10):  # loop over the dataset multiple times\n",
        "        running_loss = 0.0\n",
        "        epoch_steps = 0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            epoch_steps += 1\n",
        "            if i % 2000 == 1999:  # print every 2000 mini-batches\n",
        "                print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1,\n",
        "                                                running_loss / epoch_steps))\n",
        "                running_loss = 0.0\n",
        "        \n",
        "\n",
        "        # Validation loss\n",
        "        val_loss = 0.0\n",
        "        val_steps = 0\n",
        "        total = 0\n",
        "        correct = 0\n",
        "        for i, data in enumerate(valloader, 0):\n",
        "            with torch.no_grad():\n",
        "                inputs, labels = data\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                outputs = net(inputs)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.cpu().numpy()\n",
        "                val_steps += 1\n",
        "        print(\"epoch {} val_loss {} val_steps {} val_acc {}\".format(epoch, val_loss, val_steps, correct / total))\n",
        "  print(\"Finished Training\")\n"
      ],
      "metadata": {
        "id": "cU8zRrz-4KlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dense()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "OU6-pYu9IFut",
        "outputId": "4cb8c454-eef9-4045-81f1-ff71ddb1c08c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "epoch 0 val_loss 35.10935813188553 val_steps 40 val_acc 0.7032\n",
            "epoch 1 val_loss 29.122162997722626 val_steps 40 val_acc 0.7608\n",
            "epoch 2 val_loss 25.626320630311966 val_steps 40 val_acc 0.7912\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-f0e61ace9a41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-28-dde171ec4a8a>\u001b[0m in \u001b[0;36mtrain_dense\u001b[0;34m(batch_size)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Config the Search space for Ray Tune"
      ],
      "metadata": {
        "id": "RetvMq4ZevAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch.optim as optim\n",
        "# from ray import tune\n",
        "# from ray.tune.examples.mnist_pytorch import get_data_loaders, ConvNet, train, test\n",
        "\n",
        "\n",
        "# def train_mnist(config):\n",
        "#     train_loader, test_loader = get_data_loaders()\n",
        "#     model = ConvNet()\n",
        "#     optimizer = optim.SGD(model.parameters(), lr=config[\"lr\"])\n",
        "\n",
        "#     device = \"cpu\"\n",
        "#     if torch.cuda.is_available():\n",
        "#         device = \"cuda:0\"\n",
        "#         if torch.cuda.device_count() > 1:\n",
        "#             net = nn.DataParallel(net)\n",
        "#     net.to(device)\n",
        "\n",
        "#     for i in range(10):\n",
        "#         train(model, optimizer, train_loader)\n",
        "#         acc = test(model, test_loader)\n",
        "#         tune.report(mean_accuracy=acc)\n",
        "\n",
        "\n",
        "# analysis = tune.run(\n",
        "#     train_mnist, config={\"lr\": tune.grid_search([0.001, 0.01, 0.1])})\n",
        "\n",
        "# print(\"Best config: \", analysis.get_best_config(metric=\"mean_accuracy\"))\n",
        "\n",
        "# # Get a dataframe for analyzing trial results.\n",
        "# df = analysis.dataframe()"
      ],
      "metadata": {
        "id": "F2QnqP0MB5dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from functools import partial\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from filelock import FileLock\n",
        "from torch.utils.data import random_split\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import psutil\n",
        "import ray\n",
        "ray._private.utils.get_system_memory = lambda: psutil.virtual_memory().total\n",
        "from ray import tune\n",
        "from ray.tune import CLIReporter\n",
        "from ray.tune.schedulers import ASHAScheduler"
      ],
      "metadata": {
        "id": "B0oYc63tmQtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data1(data_dir=\"./data\"):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "    trainset = torchvision.datasets.CIFAR10(\n",
        "        root=data_dir, train=True, download=True, transform=transform)\n",
        "\n",
        "    testset = torchvision.datasets.CIFAR10(\n",
        "        root=data_dir, train=False, download=True, transform=transform)\n",
        "\n",
        "    return trainset, testset"
      ],
      "metadata": {
        "id": "FRr6hVpAmM8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class Net(nn.Module):\n",
        "#     def __init__(self, l1=120, l2=84):\n",
        "#         super(Net, self).__init__()\n",
        "#         self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "#         self.pool = nn.MaxPool2d(2, 2)\n",
        "#         self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "#         self.fc1 = nn.Linear(16 * 5 * 5, 32)\n",
        "#         self.fc2 = nn.Linear(32, 16)\n",
        "#         self.fc3 = nn.Linear(16, 10)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.pool(F.relu(self.conv1(x)))\n",
        "#         x = self.pool(F.relu(self.conv2(x)))\n",
        "#         x = x.view(-1, 16 * 5 * 5)\n",
        "#         x = F.relu(self.fc1(x))\n",
        "#         x = F.relu(self.fc2(x))\n",
        "#         x = self.fc3(x)\n",
        "#         return x"
      ],
      "metadata": {
        "id": "HvYUyqpImLiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_cifar(config, checkpoint_dir=None, data_dir=None):\n",
        "    net = ResNet(3, enable_skip_connections=True) # resnet20\n",
        "    if torch.cuda.is_available():\n",
        "      net.cuda()\n",
        "\n",
        "    device = \"cpu\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda:0\"\n",
        "        if torch.cuda.device_count() > 1:\n",
        "            net = nn.DataParallel(net)\n",
        "    net.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9)\n",
        "\n",
        "    if checkpoint_dir:\n",
        "        model_state, optimizer_state = torch.load(\n",
        "            os.path.join(checkpoint_dir, \"checkpoint\"))\n",
        "        net.load_state_dict(model_state)\n",
        "        optimizer.load_state_dict(optimizer_state)\n",
        "\n",
        "    trainset, testset = load_data1(data_dir)\n",
        "\n",
        "    test_abs = int(len(trainset) * 0.8)\n",
        "    train_subset, val_subset = random_split(\n",
        "        trainset, [test_abs, len(trainset) - test_abs])\n",
        "    \n",
        "    trainloader, valloader, testloader = data_loader()\n",
        "\n",
        "    trainloader = torch.utils.data.DataLoader(\n",
        "        train_subset,\n",
        "        batch_size=int(config[\"batch_size\"]),\n",
        "        shuffle=True,\n",
        "        num_workers=8)\n",
        "    valloader = torch.utils.data.DataLoader(\n",
        "        val_subset,\n",
        "        batch_size=int(config[\"batch_size\"]),\n",
        "        shuffle=True,\n",
        "        num_workers=8)\n",
        "\n",
        "    for epoch in range(10):  # loop over the dataset multiple times\n",
        "        running_loss = 0.0\n",
        "        epoch_steps = 0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            print(\"device:\", device)\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            epoch_steps += 1\n",
        "            if i % 2000 == 1999:  # print every 2000 mini-batches\n",
        "                print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1,\n",
        "                                                running_loss / epoch_steps))\n",
        "                running_loss = 0.0\n",
        "\n",
        "        # Validation loss\n",
        "        val_loss = 0.0\n",
        "        val_steps = 0\n",
        "        total = 0\n",
        "        correct = 0\n",
        "        for i, data in enumerate(valloader, 0):\n",
        "            with torch.no_grad():\n",
        "                inputs, labels = data\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                outputs = net(inputs)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.cpu().numpy()\n",
        "                val_steps += 1\n",
        "\n",
        "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
        "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
        "            torch.save((net.state_dict(), optimizer.state_dict()), path)\n",
        "\n",
        "        tune.report(loss=(val_loss / val_steps), accuracy=correct / total)\n",
        "    print(\"Finished Training\")"
      ],
      "metadata": {
        "id": "j2e4YGx_mIe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_accuracy(net, device=\"cpu\"):\n",
        "    trainset, testset = load_data1()\n",
        "\n",
        "    device = \"cpu\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda:0\"\n",
        "        net.cuda()\n",
        "\n",
        "    testloader = torch.utils.data.DataLoader(\n",
        "        testset, batch_size=4, shuffle=False, num_workers=2)\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return correct / total"
      ],
      "metadata": {
        "id": "nNFxU9jKqyyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Original paper\n",
        "(Hyper) SGD wiht mini-batch size 128  \n",
        "(Hyper) learning rate starts from 0.1, divide it by 10 at 32K and 48K iterations,   \n",
        "terminate at 64k iterations  \n",
        "45k/5k train/val split\n",
        "(Done) Image agumentation: 4 pixels are padded on each side, and a 32x32 crop is randomly sampled from the padded image or its horizontal flip. for testing, only evaluate the single view of the original 32x32 image.  \n",
        "(Hyper) weight decay : 0.0001  \n",
        "(Hyper) momentum : 0.9  \n"
      ],
      "metadata": {
        "id": "VMTwwmdrbp8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(num_samples=10, max_num_epochs=10, gpus_per_trial=2):\n",
        "    data_dir = os.path.abspath(\"./data\")\n",
        "    load_data1(data_dir)\n",
        "    config = {\n",
        "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
        "        \"batch_size\": tune.choice([2, 4, 8, 16])\n",
        "    }\n",
        "    scheduler = ASHAScheduler(\n",
        "        metric=\"loss\",\n",
        "        mode=\"min\",\n",
        "        max_t=max_num_epochs,\n",
        "        grace_period=1,\n",
        "        reduction_factor=2)\n",
        "    reporter = CLIReporter(\n",
        "        # parameter_columns=[\"l1\", \"l2\", \"lr\", \"batch_size\"],\n",
        "        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
        "    result = tune.run(\n",
        "        partial(train_cifar, data_dir=data_dir),\n",
        "        resources_per_trial={\"cpu\": 2, \"gpu\": gpus_per_trial},\n",
        "        config=config,\n",
        "        num_samples=num_samples,\n",
        "        scheduler=scheduler,\n",
        "        progress_reporter=reporter)\n",
        "\n",
        "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
        "    print(\"Best trial config: {}\".format(best_trial.config))\n",
        "    print(\"Best trial final validation loss: {}\".format(\n",
        "        best_trial.last_result[\"loss\"]))\n",
        "    print(\"Best trial final validation accuracy: {}\".format(\n",
        "        best_trial.last_result[\"accuracy\"]))\n",
        "\n",
        "    best_trained_model = ResNet(3, enable_skip_connections=True)\n",
        "    # best_trained_model = Net(best_trial.config[\"l1\"], best_trial.config[\"l2\"])\n",
        "    device = \"cpu\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda:0\"\n",
        "        if gpus_per_trial > 1:\n",
        "            best_trained_model = nn.DataParallel(best_trained_model)\n",
        "    best_trained_model.to(device)\n",
        "    best_trained_model.cuda()\n",
        "\n",
        "    best_checkpoint_dir = best_trial.checkpoint.value\n",
        "    model_state, optimizer_state = torch.load(os.path.join(\n",
        "        best_checkpoint_dir, \"checkpoint\"))\n",
        "    best_trained_model.load_state_dict(model_state)\n",
        "\n",
        "    test_acc = test_accuracy(best_trained_model, device)\n",
        "    print(\"Best trial test set accuracy: {}\".format(test_acc))\n",
        "\n",
        "main(num_samples=5, max_num_epochs=10, gpus_per_trial=1)"
      ],
      "metadata": {
        "id": "PHajivQyDtYY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b685d729-5f3e-4286-c763-3a8f86894480"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-04-20 02:36:56,420\tWARNING callback.py:126 -- The TensorboardX logger cannot be instantiated because either TensorboardX or one of it's dependencies is not installed. Please make sure you have the latest version of TensorboardX installed: `pip install -U tensorboardx`\n",
            "2022-04-20 02:36:56,574\tINFO trial_runner.py:803 -- starting train_cifar_bee56_00000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2022-04-20 02:36:56 (running for 00:00:00.25)\n",
            "Memory usage on this node: 2.1/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.35 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/train_cifar_2022-04-20_02-36-56\n",
            "Number of trials: 5/5 (4 PENDING, 1 RUNNING)\n",
            "+-------------------------+----------+-----------------+--------------+-------------+\n",
            "| Trial name              | status   | loc             |   batch_size |          lr |\n",
            "|-------------------------+----------+-----------------+--------------+-------------|\n",
            "| train_cifar_bee56_00000 | RUNNING  | 172.28.0.2:2625 |            4 | 0.0285835   |\n",
            "| train_cifar_bee56_00001 | PENDING  |                 |            4 | 0.00398609  |\n",
            "| train_cifar_bee56_00002 | PENDING  |                 |            2 | 0.000279323 |\n",
            "| train_cifar_bee56_00003 | PENDING  |                 |            4 | 0.0333555   |\n",
            "| train_cifar_bee56_00004 | PENDING  |                 |            4 | 0.00959189  |\n",
            "+-------------------------+----------+-----------------+--------------+-------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m Files already downloaded and verified\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m device: cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m 2022-04-20 02:37:04,439\tERROR function_runner.py:281 -- Runner Thread raised error.\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m Traceback (most recent call last):\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 272, in run\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m     self._entrypoint()\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 351, in entrypoint\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m     self._status_reporter.get_checkpoint(),\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/util/tracing/tracing_helper.py\", line 462, in _resume_span\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m     return method(self, *_args, **_kwargs)\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 640, in _trainable_func\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m     output = fn()\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   File \"<ipython-input-35-fdcfbf8dff67>\", line 51, in train_cifar\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m     return forward_call(*input, **kwargs)\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   File \"<ipython-input-34-16e98d6e3158>\", line 75, in forward\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m     return forward_call(*input, **kwargs)\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   File \"<ipython-input-34-16e98d6e3158>\", line 33, in forward\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m     return forward_call(*input, **kwargs)\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\", line 446, in forward\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\", line 443, in _conv_forward\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m     self.padding, self.dilation, self.groups)\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m Exception in thread Thread-3:\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m Traceback (most recent call last):\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m     self.run()\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 298, in run\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m     raise e\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 272, in run\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m     self._entrypoint()\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 351, in entrypoint\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m     self._status_reporter.get_checkpoint(),\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/util/tracing/tracing_helper.py\", line 462, in _resume_span\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m     return method(self, *_args, **_kwargs)\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 640, in _trainable_func\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m     output = fn()\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   File \"<ipython-input-35-fdcfbf8dff67>\", line 51, in train_cifar\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m     return forward_call(*input, **kwargs)\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   File \"<ipython-input-34-16e98d6e3158>\", line 75, in forward\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m     return forward_call(*input, **kwargs)\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   File \"<ipython-input-34-16e98d6e3158>\", line 33, in forward\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m     return forward_call(*input, **kwargs)\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\", line 446, in forward\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\", line 443, in _conv_forward\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m     self.padding, self.dilation, self.groups)\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same\n",
            "\u001b[2m\u001b[36m(func pid=2625)\u001b[0m \n",
            "2022-04-20 02:37:04,598\tERROR trial_runner.py:876 -- Trial train_cifar_bee56_00000: Error processing event.\n",
            "NoneType: None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2022-04-20 02:37:04 (running for 00:00:08.16)\n",
            "Memory usage on this node: 3.0/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.35 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/train_cifar_2022-04-20_02-36-56\n",
            "Number of trials: 5/5 (4 PENDING, 1 RUNNING)\n",
            "+-------------------------+----------+-----------------+--------------+-------------+\n",
            "| Trial name              | status   | loc             |   batch_size |          lr |\n",
            "|-------------------------+----------+-----------------+--------------+-------------|\n",
            "| train_cifar_bee56_00000 | RUNNING  | 172.28.0.2:2625 |            4 | 0.0285835   |\n",
            "| train_cifar_bee56_00001 | PENDING  |                 |            4 | 0.00398609  |\n",
            "| train_cifar_bee56_00002 | PENDING  |                 |            2 | 0.000279323 |\n",
            "| train_cifar_bee56_00003 | PENDING  |                 |            4 | 0.0333555   |\n",
            "| train_cifar_bee56_00004 | PENDING  |                 |            4 | 0.00959189  |\n",
            "+-------------------------+----------+-----------------+--------------+-------------+\n",
            "\n",
            "\n",
            "Result for train_cifar_bee56_00000:\n",
            "  date: 2022-04-20_02-36-59\n",
            "  experiment_id: 0c6c42295bb44010874510dd0f5f149e\n",
            "  hostname: 753d77ac8148\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2625\n",
            "  timestamp: 1650422219\n",
            "  trial_id: bee56_00000\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-04-20 02:37:05,395\tINFO trial_runner.py:803 -- starting train_cifar_bee56_00001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2022-04-20 02:37:10 (running for 00:00:13.99)\n",
            "Memory usage on this node: 2.9/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.35 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/train_cifar_2022-04-20_02-36-56\n",
            "Number of trials: 5/5 (1 ERROR, 3 PENDING, 1 RUNNING)\n",
            "+-------------------------+----------+-----------------+--------------+-------------+\n",
            "| Trial name              | status   | loc             |   batch_size |          lr |\n",
            "|-------------------------+----------+-----------------+--------------+-------------|\n",
            "| train_cifar_bee56_00001 | RUNNING  | 172.28.0.2:2710 |            4 | 0.00398609  |\n",
            "| train_cifar_bee56_00002 | PENDING  |                 |            2 | 0.000279323 |\n",
            "| train_cifar_bee56_00003 | PENDING  |                 |            4 | 0.0333555   |\n",
            "| train_cifar_bee56_00004 | PENDING  |                 |            4 | 0.00959189  |\n",
            "| train_cifar_bee56_00000 | ERROR    | 172.28.0.2:2625 |            4 | 0.0285835   |\n",
            "+-------------------------+----------+-----------------+--------------+-------------+\n",
            "Number of errored trials: 1\n",
            "+-------------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              |   # failures | error file                                                                                                                         |\n",
            "|-------------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------|\n",
            "| train_cifar_bee56_00000 |            1 | /root/ray_results/train_cifar_2022-04-20_02-36-56/train_cifar_bee56_00000_0_batch_size=4,lr=0.028584_2022-04-20_02-36-56/error.txt |\n",
            "+-------------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m Files already downloaded and verified\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m device: cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m 2022-04-20 02:37:13,125\tERROR function_runner.py:281 -- Runner Thread raised error.\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m Traceback (most recent call last):\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 272, in run\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m     self._entrypoint()\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 351, in entrypoint\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m     self._status_reporter.get_checkpoint(),\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/util/tracing/tracing_helper.py\", line 462, in _resume_span\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m     return method(self, *_args, **_kwargs)\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 640, in _trainable_func\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m     output = fn()\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   File \"<ipython-input-35-fdcfbf8dff67>\", line 51, in train_cifar\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m     return forward_call(*input, **kwargs)\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   File \"<ipython-input-34-16e98d6e3158>\", line 75, in forward\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m     return forward_call(*input, **kwargs)\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   File \"<ipython-input-34-16e98d6e3158>\", line 33, in forward\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m     return forward_call(*input, **kwargs)\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\", line 446, in forward\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\", line 443, in _conv_forward\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m     self.padding, self.dilation, self.groups)\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m Exception in thread Thread-3:\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m Traceback (most recent call last):\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m     self.run()\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 298, in run\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m     raise e\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 272, in run\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m     self._entrypoint()\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 351, in entrypoint\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m     self._status_reporter.get_checkpoint(),\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/util/tracing/tracing_helper.py\", line 462, in _resume_span\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m     return method(self, *_args, **_kwargs)\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 640, in _trainable_func\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m     output = fn()\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   File \"<ipython-input-35-fdcfbf8dff67>\", line 51, in train_cifar\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m     return forward_call(*input, **kwargs)\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   File \"<ipython-input-34-16e98d6e3158>\", line 75, in forward\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m     return forward_call(*input, **kwargs)\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   File \"<ipython-input-34-16e98d6e3158>\", line 33, in forward\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m     return forward_call(*input, **kwargs)\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\", line 446, in forward\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\", line 443, in _conv_forward\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m     self.padding, self.dilation, self.groups)\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same\n",
            "\u001b[2m\u001b[36m(func pid=2710)\u001b[0m \n",
            "2022-04-20 02:37:13,287\tERROR trial_runner.py:876 -- Trial train_cifar_bee56_00001: Error processing event.\n",
            "NoneType: None\n",
            "2022-04-20 02:37:13,396\tINFO trial_runner.py:803 -- starting train_cifar_bee56_00002\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for train_cifar_bee56_00001:\n",
            "  date: 2022-04-20_02-37-08\n",
            "  experiment_id: a84939f2765d44518c53d0e5c839a937\n",
            "  hostname: 753d77ac8148\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2710\n",
            "  timestamp: 1650422228\n",
            "  trial_id: bee56_00001\n",
            "  \n",
            "== Status ==\n",
            "Current time: 2022-04-20 02:37:18 (running for 00:00:22.01)\n",
            "Memory usage on this node: 2.9/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.35 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/train_cifar_2022-04-20_02-36-56\n",
            "Number of trials: 5/5 (2 ERROR, 2 PENDING, 1 RUNNING)\n",
            "+-------------------------+----------+-----------------+--------------+-------------+\n",
            "| Trial name              | status   | loc             |   batch_size |          lr |\n",
            "|-------------------------+----------+-----------------+--------------+-------------|\n",
            "| train_cifar_bee56_00002 | RUNNING  | 172.28.0.2:2794 |            2 | 0.000279323 |\n",
            "| train_cifar_bee56_00003 | PENDING  |                 |            4 | 0.0333555   |\n",
            "| train_cifar_bee56_00004 | PENDING  |                 |            4 | 0.00959189  |\n",
            "| train_cifar_bee56_00000 | ERROR    | 172.28.0.2:2625 |            4 | 0.0285835   |\n",
            "| train_cifar_bee56_00001 | ERROR    | 172.28.0.2:2710 |            4 | 0.00398609  |\n",
            "+-------------------------+----------+-----------------+--------------+-------------+\n",
            "Number of errored trials: 2\n",
            "+-------------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              |   # failures | error file                                                                                                                          |\n",
            "|-------------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------|\n",
            "| train_cifar_bee56_00000 |            1 | /root/ray_results/train_cifar_2022-04-20_02-36-56/train_cifar_bee56_00000_0_batch_size=4,lr=0.028584_2022-04-20_02-36-56/error.txt  |\n",
            "| train_cifar_bee56_00001 |            1 | /root/ray_results/train_cifar_2022-04-20_02-36-56/train_cifar_bee56_00001_1_batch_size=4,lr=0.0039861_2022-04-20_02-37-05/error.txt |\n",
            "+-------------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m Files already downloaded and verified\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m device: cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m 2022-04-20 02:37:21,145\tERROR function_runner.py:281 -- Runner Thread raised error.\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m Traceback (most recent call last):\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 272, in run\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m     self._entrypoint()\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 351, in entrypoint\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m     self._status_reporter.get_checkpoint(),\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/util/tracing/tracing_helper.py\", line 462, in _resume_span\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m     return method(self, *_args, **_kwargs)\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 640, in _trainable_func\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m     output = fn()\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   File \"<ipython-input-35-fdcfbf8dff67>\", line 51, in train_cifar\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m     return forward_call(*input, **kwargs)\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   File \"<ipython-input-34-16e98d6e3158>\", line 75, in forward\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m     return forward_call(*input, **kwargs)\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   File \"<ipython-input-34-16e98d6e3158>\", line 33, in forward\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m     return forward_call(*input, **kwargs)\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\", line 446, in forward\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\", line 443, in _conv_forward\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m     self.padding, self.dilation, self.groups)\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m Exception in thread Thread-3:\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m Traceback (most recent call last):\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m     self.run()\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 298, in run\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m     raise e\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 272, in run\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m     self._entrypoint()\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 351, in entrypoint\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m     self._status_reporter.get_checkpoint(),\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/util/tracing/tracing_helper.py\", line 462, in _resume_span\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m     return method(self, *_args, **_kwargs)\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 640, in _trainable_func\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m     output = fn()\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   File \"<ipython-input-35-fdcfbf8dff67>\", line 51, in train_cifar\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m     return forward_call(*input, **kwargs)\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   File \"<ipython-input-34-16e98d6e3158>\", line 75, in forward\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m     return forward_call(*input, **kwargs)\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   File \"<ipython-input-34-16e98d6e3158>\", line 33, in forward\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m     return forward_call(*input, **kwargs)\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\", line 446, in forward\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\", line 443, in _conv_forward\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m     self.padding, self.dilation, self.groups)\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same\n",
            "\u001b[2m\u001b[36m(func pid=2794)\u001b[0m \n",
            "2022-04-20 02:37:21,348\tERROR trial_runner.py:876 -- Trial train_cifar_bee56_00002: Error processing event.\n",
            "NoneType: None\n",
            "2022-04-20 02:37:21,397\tINFO trial_runner.py:803 -- starting train_cifar_bee56_00003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for train_cifar_bee56_00002:\n",
            "  date: 2022-04-20_02-37-16\n",
            "  experiment_id: b624c444353d4fa29dde8771f42b3652\n",
            "  hostname: 753d77ac8148\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2794\n",
            "  timestamp: 1650422236\n",
            "  trial_id: bee56_00002\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-04-20 02:37:22,414\tWARNING tune.py:651 -- SIGINT received (e.g. via Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C one more time (or send SIGINT/SIGKILL/SIGTERM) to skip. \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-51acda9eabda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best trial test set accuracy: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_num_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpus_per_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-36-51acda9eabda>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(num_samples, max_num_epochs, gpus_per_trial)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         progress_reporter=reporter)\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mbest_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"min\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"last\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, max_concurrent_trials, _experiment_checkpoint_dir, queue_trials, loggers, _remote)\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[0mprogress_reporter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_start_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtune_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_finished\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIGINT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m         \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhas_verbosity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVerbosity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV1_EXPERIMENT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m             \u001b[0m_report_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_reporter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/tune/trial_runner.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0mnext_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_trial_queue_and_get_next_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_and_handle_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop_experiment_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/tune/trial_runner.py\u001b[0m in \u001b[0;36m_wait_and_handle_event\u001b[0;34m(self, next_trial)\u001b[0m\n\u001b[1;32m    714\u001b[0m             \u001b[0;31m# Single wait of entire tune loop.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             future_result = self.trial_executor.get_next_executor_event(\n\u001b[0;32m--> 716\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_live_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_trial\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             )\n\u001b[1;32m    718\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfuture_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mExecutorEventType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPG_READY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/tune/ray_trial_executor.py\u001b[0m in \u001b[0;36mget_next_executor_event\u001b[0;34m(self, live_trials, next_trial_exists)\u001b[0m\n\u001b[1;32m    855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m             ready_futures, _ = ray.wait(\n\u001b[0;32m--> 857\u001b[0;31m                 \u001b[0mfutures_to_wait\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_returns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_next_event_wait\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    858\u001b[0m             )\n\u001b[1;32m    859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/_private/client_mode_hook.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"init\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_client_mode_enabled_by_default\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/worker.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_refs, num_returns, timeout, fetch_local)\u001b[0m\n\u001b[1;32m   1996\u001b[0m             \u001b[0mtimeout_milliseconds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m             \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_task_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1998\u001b[0;31m             \u001b[0mfetch_local\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1999\u001b[0m         )\n\u001b[1;32m   2000\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mready_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.CoreWorker.wait\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "gQ4JgbCP1pru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change these values if you want the training to run quicker or slower.\n",
        "EPOCH_SIZE = 512\n",
        "TEST_SIZE = 256\n",
        "\n",
        "def train(model, optimizer, train_loader):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        # We set this just for the example to run quickly.\n",
        "        if batch_idx * len(data) > EPOCH_SIZE:\n",
        "            return\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "def test(model, data_loader):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(data_loader):\n",
        "            # We set this just for the example to run quickly.\n",
        "            if batch_idx * len(data) > TEST_SIZE:\n",
        "                break\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            outputs = model(data)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "    return correct / total"
      ],
      "metadata": {
        "id": "zC_doY3o1s2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_cifar1(config):\n",
        "    # Data Setup\n",
        "    mnist_transforms = transforms.Compose(\n",
        "        [transforms.ToTensor(),\n",
        "         transforms.Normalize((0.1307, ), (0.3081, ))])\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        torchvision.datasets.CIFAR10(\"~/data\", train=True, download=True, transform=mnist_transforms),\n",
        "        batch_size=64,\n",
        "        shuffle=True)\n",
        "    test_loader = DataLoader(\n",
        "        torchvision.datasets.CIFAR10(\"~/data\", train=False, transform=mnist_transforms),\n",
        "        batch_size=64,\n",
        "        shuffle=True)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = ResNet(3, enable_skip_connections=True)\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = optim.SGD(\n",
        "        model.parameters(), lr=config[\"lr\"], momentum=config[\"momentum\"])\n",
        "    for i in range(10):\n",
        "        train(model, optimizer, train_loader)\n",
        "        acc = test(model, test_loader)\n",
        "\n",
        "        # Send the current training result back to Tune\n",
        "        tune.report(mean_accuracy=acc)\n",
        "\n",
        "        if i % 5 == 0:\n",
        "            # This saves the model to the trial directory\n",
        "            torch.save(model.state_dict(), \"./model.pth\")"
      ],
      "metadata": {
        "id": "JswhX1tQ16xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_space = {\n",
        "    \"lr\": tune.sample_from(lambda spec: 10 ** (-10 * np.random.rand())),\n",
        "    \"momentum\": tune.uniform(0.1, 0.9),\n",
        "}\n",
        "\n",
        "# Uncomment this to enable distributed execution\n",
        "# `ray.init(address=\"auto\")`\n",
        "\n",
        "# Download the dataset first\n",
        "torchvision.datasets.CIFAR10(\"~/data\", train=True, download=True)\n",
        "\n",
        "analysis = tune.run(train_cifar1, config=search_space)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aMezJl1K2jM1",
        "outputId": "3f6318de-8493-4612-c29f-d7a54998663b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-04-20 02:45:42,799\tWARNING callback.py:126 -- The TensorboardX logger cannot be instantiated because either TensorboardX or one of it's dependencies is not installed. Please make sure you have the latest version of TensorboardX installed: `pip install -U tensorboardx`\n",
            "2022-04-20 02:45:42,808\tWARNING tune.py:637 -- Tune detects GPUs, but no trials are using GPUs. To enable trials to use GPUs, set tune.run(resources_per_trial={'gpu': 1}...) which allows Tune to expose 1 GPU to each trial. You can also override `Trainable.default_resource_request` if using the Trainable API.\n",
            "2022-04-20 02:45:42,927\tINFO trial_runner.py:803 -- starting train_cifar1_f8a47_00000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-20 02:45:45 (running for 00:00:02.98)<br>Memory usage on this node: 2.2/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/2 CPUs, 0/1 GPUs, 0.0/7.35 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/train_cifar1_2022-04-20_02-45-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name              </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  momentum</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>train_cifar1_f8a47_00000</td><td>RUNNING </td><td>172.28.0.2:3168</td><td style=\"text-align: right;\">1.06222e-07</td><td style=\"text-align: right;\">  0.628584</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(train_cifar1 pid=3168)\u001b[0m Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-20 02:45:50 (running for 00:00:07.98)<br>Memory usage on this node: 2.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/2 CPUs, 0/1 GPUs, 0.0/7.35 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/train_cifar1_2022-04-20_02-45-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name              </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  momentum</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>train_cifar1_f8a47_00000</td><td>RUNNING </td><td>172.28.0.2:3168</td><td style=\"text-align: right;\">1.06222e-07</td><td style=\"text-align: right;\">  0.628584</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for train_cifar1_f8a47_00000:\n",
            "  date: 2022-04-20_02-45-52\n",
            "  done: false\n",
            "  experiment_id: c79eb786ed0e4c59be1a3212475a4b14\n",
            "  hostname: 753d77ac8148\n",
            "  iterations_since_restore: 1\n",
            "  mean_accuracy: 0.109375\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 3168\n",
            "  time_since_restore: 6.664595127105713\n",
            "  time_this_iter_s: 6.664595127105713\n",
            "  time_total_s: 6.664595127105713\n",
            "  timestamp: 1650422752\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: f8a47_00000\n",
            "  warmup_time: 0.003980159759521484\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-20 02:45:57 (running for 00:00:14.66)<br>Memory usage on this node: 2.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/2 CPUs, 0/1 GPUs, 0.0/7.35 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/train_cifar1_2022-04-20_02-45-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name              </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>train_cifar1_f8a47_00000</td><td>RUNNING </td><td>172.28.0.2:3168</td><td style=\"text-align: right;\">1.06222e-07</td><td style=\"text-align: right;\">  0.628584</td><td style=\"text-align: right;\">0.109375</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">          6.6646</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for train_cifar1_f8a47_00000:\n",
            "  date: 2022-04-20_02-45-57\n",
            "  done: false\n",
            "  experiment_id: c79eb786ed0e4c59be1a3212475a4b14\n",
            "  hostname: 753d77ac8148\n",
            "  iterations_since_restore: 2\n",
            "  mean_accuracy: 0.109375\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 3168\n",
            "  time_since_restore: 11.78224802017212\n",
            "  time_this_iter_s: 5.117652893066406\n",
            "  time_total_s: 11.78224802017212\n",
            "  timestamp: 1650422757\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 2\n",
            "  trial_id: f8a47_00000\n",
            "  warmup_time: 0.003980159759521484\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-20 02:46:02 (running for 00:00:19.77)<br>Memory usage on this node: 2.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/2 CPUs, 0/1 GPUs, 0.0/7.35 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/train_cifar1_2022-04-20_02-45-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name              </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>train_cifar1_f8a47_00000</td><td>RUNNING </td><td>172.28.0.2:3168</td><td style=\"text-align: right;\">1.06222e-07</td><td style=\"text-align: right;\">  0.628584</td><td style=\"text-align: right;\">0.109375</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         11.7822</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for train_cifar1_f8a47_00000:\n",
            "  date: 2022-04-20_02-46-02\n",
            "  done: false\n",
            "  experiment_id: c79eb786ed0e4c59be1a3212475a4b14\n",
            "  hostname: 753d77ac8148\n",
            "  iterations_since_restore: 3\n",
            "  mean_accuracy: 0.10625\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 3168\n",
            "  time_since_restore: 16.856648206710815\n",
            "  time_this_iter_s: 5.074400186538696\n",
            "  time_total_s: 16.856648206710815\n",
            "  timestamp: 1650422762\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 3\n",
            "  trial_id: f8a47_00000\n",
            "  warmup_time: 0.003980159759521484\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-20 02:46:07 (running for 00:00:24.85)<br>Memory usage on this node: 2.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/2 CPUs, 0/1 GPUs, 0.0/7.35 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/train_cifar1_2022-04-20_02-45-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name              </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">    acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>train_cifar1_f8a47_00000</td><td>RUNNING </td><td>172.28.0.2:3168</td><td style=\"text-align: right;\">1.06222e-07</td><td style=\"text-align: right;\">  0.628584</td><td style=\"text-align: right;\">0.10625</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         16.8566</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for train_cifar1_f8a47_00000:\n",
            "  date: 2022-04-20_02-46-07\n",
            "  done: false\n",
            "  experiment_id: c79eb786ed0e4c59be1a3212475a4b14\n",
            "  hostname: 753d77ac8148\n",
            "  iterations_since_restore: 4\n",
            "  mean_accuracy: 0.10625\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 3168\n",
            "  time_since_restore: 21.949237823486328\n",
            "  time_this_iter_s: 5.092589616775513\n",
            "  time_total_s: 21.949237823486328\n",
            "  timestamp: 1650422767\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 4\n",
            "  trial_id: f8a47_00000\n",
            "  warmup_time: 0.003980159759521484\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-20 02:46:12 (running for 00:00:29.93)<br>Memory usage on this node: 2.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/2 CPUs, 0/1 GPUs, 0.0/7.35 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/train_cifar1_2022-04-20_02-45-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name              </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">    acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>train_cifar1_f8a47_00000</td><td>RUNNING </td><td>172.28.0.2:3168</td><td style=\"text-align: right;\">1.06222e-07</td><td style=\"text-align: right;\">  0.628584</td><td style=\"text-align: right;\">0.10625</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         21.9492</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for train_cifar1_f8a47_00000:\n",
            "  date: 2022-04-20_02-46-12\n",
            "  done: false\n",
            "  experiment_id: c79eb786ed0e4c59be1a3212475a4b14\n",
            "  hostname: 753d77ac8148\n",
            "  iterations_since_restore: 5\n",
            "  mean_accuracy: 0.090625\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 3168\n",
            "  time_since_restore: 27.060617208480835\n",
            "  time_this_iter_s: 5.111379384994507\n",
            "  time_total_s: 27.060617208480835\n",
            "  timestamp: 1650422772\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 5\n",
            "  trial_id: f8a47_00000\n",
            "  warmup_time: 0.003980159759521484\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-20 02:46:17 (running for 00:00:35.05)<br>Memory usage on this node: 2.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/2 CPUs, 0/1 GPUs, 0.0/7.35 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/train_cifar1_2022-04-20_02-45-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name              </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>train_cifar1_f8a47_00000</td><td>RUNNING </td><td>172.28.0.2:3168</td><td style=\"text-align: right;\">1.06222e-07</td><td style=\"text-align: right;\">  0.628584</td><td style=\"text-align: right;\">0.090625</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         27.0606</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for train_cifar1_f8a47_00000:\n",
            "  date: 2022-04-20_02-46-17\n",
            "  done: false\n",
            "  experiment_id: c79eb786ed0e4c59be1a3212475a4b14\n",
            "  hostname: 753d77ac8148\n",
            "  iterations_since_restore: 6\n",
            "  mean_accuracy: 0.10625\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 3168\n",
            "  time_since_restore: 32.14216995239258\n",
            "  time_this_iter_s: 5.081552743911743\n",
            "  time_total_s: 32.14216995239258\n",
            "  timestamp: 1650422777\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 6\n",
            "  trial_id: f8a47_00000\n",
            "  warmup_time: 0.003980159759521484\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-20 02:46:22 (running for 00:00:40.13)<br>Memory usage on this node: 2.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/2 CPUs, 0/1 GPUs, 0.0/7.35 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/train_cifar1_2022-04-20_02-45-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name              </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">    acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>train_cifar1_f8a47_00000</td><td>RUNNING </td><td>172.28.0.2:3168</td><td style=\"text-align: right;\">1.06222e-07</td><td style=\"text-align: right;\">  0.628584</td><td style=\"text-align: right;\">0.10625</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         32.1422</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for train_cifar1_f8a47_00000:\n",
            "  date: 2022-04-20_02-46-23\n",
            "  done: false\n",
            "  experiment_id: c79eb786ed0e4c59be1a3212475a4b14\n",
            "  hostname: 753d77ac8148\n",
            "  iterations_since_restore: 7\n",
            "  mean_accuracy: 0.103125\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 3168\n",
            "  time_since_restore: 37.28174901008606\n",
            "  time_this_iter_s: 5.1395790576934814\n",
            "  time_total_s: 37.28174901008606\n",
            "  timestamp: 1650422783\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 7\n",
            "  trial_id: f8a47_00000\n",
            "  warmup_time: 0.003980159759521484\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-20 02:46:28 (running for 00:00:45.28)<br>Memory usage on this node: 2.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/2 CPUs, 0/1 GPUs, 0.0/7.35 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/train_cifar1_2022-04-20_02-45-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name              </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>train_cifar1_f8a47_00000</td><td>RUNNING </td><td>172.28.0.2:3168</td><td style=\"text-align: right;\">1.06222e-07</td><td style=\"text-align: right;\">  0.628584</td><td style=\"text-align: right;\">0.103125</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         37.2817</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for train_cifar1_f8a47_00000:\n",
            "  date: 2022-04-20_02-46-28\n",
            "  done: false\n",
            "  experiment_id: c79eb786ed0e4c59be1a3212475a4b14\n",
            "  hostname: 753d77ac8148\n",
            "  iterations_since_restore: 8\n",
            "  mean_accuracy: 0.10625\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 3168\n",
            "  time_since_restore: 42.40127110481262\n",
            "  time_this_iter_s: 5.1195220947265625\n",
            "  time_total_s: 42.40127110481262\n",
            "  timestamp: 1650422788\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 8\n",
            "  trial_id: f8a47_00000\n",
            "  warmup_time: 0.003980159759521484\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-20 02:46:33 (running for 00:00:50.39)<br>Memory usage on this node: 2.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/2 CPUs, 0/1 GPUs, 0.0/7.35 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/train_cifar1_2022-04-20_02-45-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name              </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">    acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>train_cifar1_f8a47_00000</td><td>RUNNING </td><td>172.28.0.2:3168</td><td style=\"text-align: right;\">1.06222e-07</td><td style=\"text-align: right;\">  0.628584</td><td style=\"text-align: right;\">0.10625</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         42.4013</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for train_cifar1_f8a47_00000:\n",
            "  date: 2022-04-20_02-46-33\n",
            "  done: false\n",
            "  experiment_id: c79eb786ed0e4c59be1a3212475a4b14\n",
            "  hostname: 753d77ac8148\n",
            "  iterations_since_restore: 9\n",
            "  mean_accuracy: 0.084375\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 3168\n",
            "  time_since_restore: 47.52450728416443\n",
            "  time_this_iter_s: 5.123236179351807\n",
            "  time_total_s: 47.52450728416443\n",
            "  timestamp: 1650422793\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 9\n",
            "  trial_id: f8a47_00000\n",
            "  warmup_time: 0.003980159759521484\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfs = analysis.trial_dataframes\n",
        "[d.mean_accuracy.plot() for d in dfs.values()]"
      ],
      "metadata": {
        "id": "vTKEK6ty2oj-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}